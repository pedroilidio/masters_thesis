%% USPSC-Introducao.tex

% ----------------------------------------------------------
% Introdução (exemplo de capítulo sem numeração, mas presente no Sumário)
% ----------------------------------------------------------
\chapter[Introduction]{Introduction}
\label{sec:introduction}

As the starting point, we provide a brief overview of the concepts explored along this thesis. We begin by describing the learning paradigm of interest (\autoref{sec:bipartite learning}), and proceed by outlining its main challenges and current approaches (\autoref{sec:challenges} and \autoref{sec:approaches}).
A brief discussion on the advantages of decision trees is then presented (\autoref{sec:why dt}), followed by a summary of previous literature on bipartite forests (\autoref{sec:related_work}).
We conclude this introduction by presenting the guiding research questions of our work (\autoref{sec:research questions}).
%, and describe how the remainder of this document will be organized.

%Thus, bipartite interactions also naturally encompasses the data format targeted by the broadly-known recommender systems \cite{}.

% DTI prediction review

%%The interaction attributes may be of any dimensionality, and may as well be
%%unknown for some (often many) instance pairs. When binary interactions are
%%considered (pairs either do or do not interact in any specified way) we
%%frequently find ourselves in a Positive-Unlabeled (PU) scenario \cite{}, where
%%we can only confidently measure the presence of a given phenomena, not its
%%absence, and hence, the instance pairs' interactions can only be said to be
%%positive (actually happening) or unknown.
%%
%%% assumptions must be made what is bipartite, graph stuff, networks
%%
%%Furthermore, as the number of interactions grows with the product of the numbers
%%of interacting instances in each bipartite group, taking all possible
%%interactions into consideration may become unfeasible for larger datasets using
%%standard machine learning algorithms. As a result, many workaround techniques
%%are usually employed to generate negative interaction data, such as considering
%%a random subset of unlabeled data as negative\cite{Zhang_2018, Zitnik_2018,
%%Huang_2021}, selecting the most reliably-non interacting pairs (which depends on
%%estimating the interaction likelihood with semi-supervised methods such as
%%self-learning)\cite{} or even artificially creating new dataset instances when
%%very specific factors are known to be needed for an interaction to occur (namely
%%the chemical-structural characteristics of an enzyme's active site) \cite{}.
%%
%%Despite even using sophisticated deep learning algorithms, these approaches thus
%%fail to take all possible drug-target pairs into consideration.
%%
%%Predictive Bi-Clustering Trees (PBCT) were proposed in 2018 by
%%\cite{Pliakos_2018} to address some of these issues, introducing a new method
%%for growing decision tree-based models from bipartite interaction data. With
%%this method and further optimizations, millions of interactions can be
%%considered in reasonable computation time.
%%
%%% explain decision trees TODO ID3, C4.5, CART?
%%Decision Trees work by recursively partitioning the dataset in chunks with
%%progressively similar labels\cite{Breiman_1984, Quinlan_1996}. They do so by
%%consecutively searching for decision rules in each partition that would split
%%the partition in two. For example, a specific numeric characteristic of our
%%instances being less or greater than a threshold value, or if an instance has
%%one of a specific set of values of a categorical variable. For this study, we
%%mainly focus on numerical instance features, so that each tree node represents a
%%binary split designated by an instance attribute an a threshold value.
%%
%%The main idea behind biclustering trees was to separately search for a split
%%attribute and value on each of the two instance groups, considering all possible
%%thresholds among row instances first (e.g. proteins), and only then processing
%%the column instances attributes (e.g. target drugs features).  \par In 2020, the
%%authors expanded on this concept, building ExtraTrees ensembles of PBCTs
%%\cite{Pliakos_2020} and reporting solid boosts on prediction performance. The
%%authors, however, did not explore other forms of tree ensembles, including the
%%so popular Random Forests proposed by \cite{Breiman_2001}, despite the latter
%%being oftentimes regarded one of the best tree ensemble
%%techniques\cite{Huang_2021, Amasyali_2011, Hall_2003, Banfield_2007}. Hence, in
%%this study we demonstrate how DTI prediction improvements can be achieved with
%%the use of Random Forests of Predictive Bi-Clustering Trees, that we name
%%Biclustering Random Forests, and provide an optimized implementation based on
%%scikit-learn \cite{scikit-learn}, one the most standard libraries for machine
%%learning applications using the Python\cite{python} programming language.
%In 2020, the authors expanded on this concept, building ExtraTrees ensembles of
%PBCTs and utilizing unsupervised neighborhood information to further improve
%the previous results \ref{pliakos, vens, 2020}. The use of unsupervised
%information is based upon continuity assumptions \cite{}, that is, the
%supposition that that similar instances are more likely to interact with
%similar targets. In other words, that interaction probability monotonically
%increases with some similarity measure \ref{pu}.

%matrix factorization approaches, previous results with unsupervised data and
%reasoning behind it (PU paper, two steps approach).

%trees can directly use unsupervised info

% However, it is already known that decision tree models can directly deal with
% unsupervised data as well.

% previous results unsupervised trees

% In the present work, we propose making use of both supervised and unsupervised
% data in conjunction when growing a decision tree model, employing thus
% semi-supervised impurity metrics to be followed when splitting data at each
% tree node. By also employing the bi-clustering mechanism for split searching,
% we hope to enable decision-tree learning methods, including Random Forests,
% ExtraTrees and Gradient Boosting Machines, to fully utilize large bipartite
% interaction datasets.

% how the standard ones are used, GSO LMO

% TT is a highlight of our paper

\section{What is \emph{bipartite learning}?}
\label{sec:bipartite learning}
%\section{Context and motivation}


% the problem is common
% machine learning is interesting
% main challenges

The effect a drug molecule has on our organism is tightly associated with the specific microscopic structures it physically and chemically interacts with~\cite{bagherian2020machine,chen2018machine}. The regulatory roles of microRNAs are often mediated by their binding to a defined set of messenger RNAs. The activity of a transcription factor is determined by which genes it is capable of regulating~\cite{faith2007largescale}. The success of a recommendation system depends on its ability to predict a user's preference for a set of items in a catalog~\cite{lu2012recommender}.
%
All those scenarios share the same underlying structure: there are two distinct domains of objects that interact with one another, forming a heterogeneous \emph{bipartite network}~\cite{asratian1998bipartite}.
%
%A bipartite network is a common representation for a multitude of
%phenomena. It consists of two separate groups of instances often representing
%two classes of distinct nature of objects. Each object from one group may
%interact with any of the objects from the other, so that each possible pair of
%objects from distinct groups holds a set of attributes describing their
%interaction.

% TODO {MORE STUFF}

Frequently, great importance lies in computationally describing such relationships. For instance, developing new drug molecules is a costly and time-consuming process,
%A large portion of that cost derives from the
requiring years of complex computer simulations, large screening assays, and extensive trials~\cite{chen2018machine,bagherian2020machine}.  % TODO: not ml citations
Being able to predict beforehand the interactions in a large dataset of drugs and protein targets holds a remarkable potential to accelerate drug development and reduce its costs, enabling the concentration of financial resources and human expertise to a set of most promising candidates. Furthermore, being able to characterize on a large scale the relationships between microRNAs and genes or other biomolecular entities can greatly assist the understanding of regulatory mechanisms of gene expression, a valuable step towards the discovery and development of new therapies. 

% TODO why machine learning
% TODO define bipartite learn in topics

Therefore, it is natural that numerous machine learning algorithms have been proposed to predict interactions in bipartite networks~\cite{chen2018machine,ezzat2019computational,bagherian2020machine}. 
The role of a learning algorithm in this setting is to receive a feature vector describing the heterogeneous pair, and characterize their interaction through a binary label or affinity score.
We focus on cases where the input vector can be split into two: one vector representing each interacting entity.
%The role of a learning algorithm in this setting is to receive a feature vector describing the heterogeneous pair, and characterize their interaction through, namely, a binary label, affinity score or predicted user-item rating. Additionally, we focus on cases where the input vector can be split into two: one vector representing each interacting entity.
%often composed of independent representations of each interacting entity,
%
More specifically, our setting is characterized by:
%
\begin{enumerate}
    \item \textbf{Interaction prediction:}
        The goal is to infer interactions that occur each between two objects (called a \emph{dyad}).
    \item \textbf{Heterogeneus dyads:}
        Each dyad is composed of objects of different types.
    \item \textbf{Bipartite network:}
        There are two types of objects.
    \item \textbf{Available side-features:}
        Each object has its own feature vector that is not derived from its set of known outputs.
\end{enumerate}

Several similar concepts have emerged when referring to learning tasks that aim to describe relationships.
%
%link prediction~\cite{lu2011link,zhou2021progresses}
%    non-bipartite
%    usualy assumes hidden node-attributes~\cite{lu2011link}
%    transductive~\cite{chapelle2006semisupervised}: predicting links between known nodes~\cite{lu2011link,zhou2021progresses}
%
\emph{Link prediction}~\cite{lu2011link,zhou2021progresses} refers to the task of predicting the existence of connections between two nodes in a network. Usually, is assumes that node attributes are unavailable, being concerned with recovering attributes from the network structure~\cite{lu2011link}. Furthermore, the output is usually binary (or probabilistic), indicating the presence or absence of a link~\cite{lu2011link}.
%
%The task is usually transductive, so that all nodes are present in the training set and only the links between them are to be predicted.
%evaluation in the context of link prediction is usually 
%
%It is also not concerned with bipartite networks specifically, and assumes all the nodes are of the same type and can interact with each other.
%
%dyadic prediction
%    non-bipartite
%    side-features sometimes
%    transductive
%    not necessarily binary
%
\emph{Dyadic prediction}~\cite{menon2010loglinear,pahikkala2014twostep,jin2017multitask} is a broader category of tasks involving predicting relationships between two objects. The availability of side-features is considered, but not required~\cite{menon2010loglinear}, and possible outputs encompass both classification and regression~\cite{menon2010loglinear}. The presence of two separate domains is sometimes assumed~\cite{menon2010loglinear,pahikkala2014twostep}, but there are also cases in which the network is homogeneous~\cite{jin2017multitask}. 
%
%% recommendation systems
%% collaborative filtering  % TODO
%interaction prediction
%    features
%    non-bipartite
%
\emph{Interaction prediction} is a term often used in the context of biological networks~\cite{schrynemackers2015classifying,pliakos2019network,chen2018machine,bagherian2020machine}. As such, it usually assumes the presence of side-features.
%
The output can be either a binary indicator or real values such as drug-target affinities~\cite{pahikkala2015more}.

None of these terms are specific to bipartite networks. They include cases in which a single type of entity composes the network, and thus a single feature matrix is sufficient to describe all the nodes.
Furthermore, it is common for these contexts to assume \emph{transductive}~\cite{chapelle2006semisupervised} approaches. In such cases, the attributes of all nodes are available for training, and only the set of links is split between training and validation sets~\cite{lu2011link,zhou2021progresses}. As a result, the models are not guaranteed to be able to predict interactions between unseen instances.

In this study, we are mostly concerned with the bipartite scenario, where two distinct sets of entities interact with each other and each set has its own feature matrix.
We are also interested in \emph{inductive}~\cite{chapelle2006semisupervised} techniques, in which the main goal is to model a function from the node feature space to the output space. This property allows us to predict interactions between new instances that were not present in the training set, while also enabling transparent models to provide new insights about the underlying phenomena.

If feature vectors are not available for both interacting entities, the problem becomes essentially a multi-output supervised task or a transductive dyadic prediction problem.
For that reason, we require side-features to be available for both axes.

Possible denominations for the current setting include \emph{inductive heterogeneous dyadic prediction} and \emph{inductive bipartite interaction learning}. For simplicity, we refer to this paradigm as \emph{bipartite learning}.

%The symmetry of homogeneous networks can be exploited to enhance our trees

%transductive

%, and involves both inductive and transductive approaches.


%inductive
%heterogeneous dyadic prediction
%bipartite interaction prediction


\section{Challenges of bipartite learning problems}
\label{sec:challenges}

Despite their importance, bipartite interaction prediction problems pose unique challenges to machine learning approaches, that we enumerate below.
%Despite their importance, interaction prediction problems pose unique challenges to machine learning approaches, that we enumerate below.
%
\begin{enumerate}
    \item \textbf{Heterogeneous data:} The input data is often composed of two very dissimilar types of objects, each with its own set of features. As a result, estimators may be required to simultaneously deal with descriptors of very different natures~\cite{bagherian2020machine}.  %TODO cite more
    % Namely, in the drug-target interaction prediction problem, the drug domain is composed of chemical compounds, while the target domain is composed of proteins. Each of those domains has its own set of features, such as the chemical structure of the drug or the amino acid sequence of the protein. The features of each domain are usually of a different nature, and therefore require different types of processing. For instance, the chemical structure of a drug can be represented as a graph, while the amino acid sequence of a protein is a string of characters.
    \item \textbf{Large datasets:} Due to the intrinsic combinatory nature of this type of problem, the interactions to be processed are usually very numerous, even if the number of training instances in each domain is not that expressive.  %TODO cite
    Specifically, for each new object introduced to the training set, a new interaction could be considered for each training sample in the other domain, rendering the number of interactions to grow quadratically with the number of entities of each type.
    \item \textbf{Knowledge sparsity:} As a consequence of the previous point, no amount of research efforts in characterizing new interactions can keep up with the rate at which possible relationships appear, resulting in a fundamental sparsity of confidently verified interactions~\cite{bagherian2020machine}. % TODO cite more
    \item \textbf{Lack of validated negative annotations:} In many cases, the absence of a relationship between two objects is much harder to validate than its presence. For instance, negative results in biochemical essays asserting molecular interactions can often be caused by a plethora of external experimental factors other than the actual lack of interaction under optimal conditions, unlike positive results that usually imply a successful assay~\cite{pahikkala2015more,chen2018machine,bagherian2020machine}.
    %Furthermore, since positive results are often much more informative, negative results in such interaction characterization settings are naturally less valued in the scientific community, and therefore less likely to be reported~\cite{}.  TODO find citation
    These factors result in a general lack of high-quality negative data in interaction datasets, requiring special considerations from machine learning pipelines that are often overlooked in the literature.
    %\item \textbf{New instances:} validation is delicate
    \item \textbf{Impact on model evaluation:} Two rank-based metrics are mainly used to evaluate the performance of interaction prediction models~\cite{lu2011link,zhou2021progresses,pahikkala2015more,chen2018machine}: the area under the receiver operating characteristic curve (AUROC)~\cite{davis2006relationship,hand2009measuring} and the area under the precision-recall curve (AUPR)~\cite{davis2006relationship,ozenne2015precision,flach2015precisionrecallgain}. These metrics are known to assess the performance of a model in different ways, and are often used in conjunction to provide a more comprehensive view.
    However, these differences are not fully clear, and neither is how they are affected by the missing annotations.
    This makes it difficult to interpret why different models are sometimes chosen,
    and to decide which metric to prioritize in each experiment. For instance, AUPR is often recommended for imbalanced scenarios~\cite{davis2006relationship,hand2009measuring,he2009learning,ezzat2019computational}, but metrics similar to AUROC are also suggested in interaction prediction contexts~\cite{pahikkala2015more,johnson2014logistic,hao2019opensource,ezzat2019computational,yu2020fpscdtia}.
    %which metric to take into account when selecting a model.
    %how are they affected by the particularities of the interaction prediction problem.
    %However, the differences between them are not fully clear, and the choice of which metric to use can have a significant impact on the conclusions drawn from the model's performance.
    % TODO no inductive algorithms, only transductive
    \item \textbf{Lack of inductive algorithms:} Many of the algorithms proposed for interaction prediction assume a \emph{transductive}~\cite{chapelle2006semisupervised} setting, where attributes of the test instances are available at training time~\cite{pahikkala2015more,ezzat2019computational}. This setting hinders the interpretability of those models and their application to completely new entities.
\end{enumerate}

%Those issues can be obfuscated or even amplified if one fails to acknowledge defining aspects of the interaction prediction problem.
Most of these issues are an integral part of interaction prediction problems,
%arising from their fundamental definition.
that arise from how they are fundamentally defined around combinations of objects.
%
However, these differences are often overlooked when developing machine learning algorithms~\cite{pahikkala2015more}.
%
%If, in a given bipartite interaction problem, the feature vector characterizing a pair of possibly interacting entities is a simple combination of independent representations of each entity, we can see the training set as two separate design matrices rather than the usual single one, disregarding their combination as a data preprocessing step and leaving the treatment of both in conjunction as a matter of estimator design. This view defines a different machine learning paradigm, in which what we call "instances" or "samples" are not the dyads themselves, but rather each individual interacting object.
%For instance, it is common for the input vector describing each dyad to be a simple concatenation of feature vectors describing each interacting entity.
For instance, it is common to represent each dyad with the concatenated feature vectors of its components. Nevertheless, it can be challenging to collect representations for all dyads in this format, which would result in a very large matrix to be processed by the algorithm.
%
Instead, the training set for a bipartite problem is more naturally represented as two separate design matrices rather than the usual single one.
Ideally, we would want to avoid the combination of feature vectors as a data preprocessing step, leaving the conjoint treatment of both design matrices as a matter of estimator design. This view defines a different machine learning paradigm, in which what we call "instances" or "samples" are not the dyads themselves, but rather each individual interacting object. The estimator receives both feature matrices and it is up to the algorithm to decide how to use them in conjunction to predict new interactions.

%the fundamental way they are defined.
%These issues highlight intrinsic differences between interaction prediction problems and more general supervised tasks,
%arising from the fundamental way interaction problems deal with combinations.
%stemming from the fundamental way it is defined combinatory nature.

%Acknowledging such defining aspects and limitations and incorporating the bipartite nature of the problem from the ground up in the algorithm design process can be a crucial step in developing successful and scalable machine learning models for interaction prediction.
In conclusion, acknowledging such defining aspects and incorporating the bipartite nature of the problem are crucial steps in developing successful and scalable machine learning models for interaction prediction.

% RELATED TERMS section TODO
% \cite{liu2017computational} pu dyadic prediction
%one-class classification
%While \emph{link prediction} and \emph{dyadic prediction} are common terms in the literature referring to similar concepts, they are not specific to the presence of two distinct groups of objects nor make clear the presence of describing attributes for both of them (often called \emph{side-features} or \emph{side-information} in the context of recommendation systems). While \emph{interaction prediction} seems a closer alternative,
%usually assuming side-features, % TODO I do not think so
%it is also not concerned with the heterogeneity of the network.
%As an abuse of terminology, we thus refer to this "heterogeneous bipartite interaction prediction" paradigm as \emph{bipartite learning}.


\section{How can bipartite models be built?}
\label{sec:approaches}

There are two general strategies for developing machine learning algorithms for bipartite interaction prediction. We call them \emph{data-centric} and \emph{estimator-centric} approaches.
%
The majority of methods proposed for bipartite learning is based on organizing the data to enable the use of traditional learning algorithms.
These data-centric adaptations can be broadly classified into two categories~\cite{schrynemackers2015classifying,pliakos2018global,pliakos2020drugtarget}, which we describe below.

\begin{itemize}
    \item \textbf{Global single output:} The most common approach observed in the literature. The dataset is formatted so that each dyad is an instance. Each dyad has a single label associated with it, and the feature vector describing the dyad is the concatenation of the two feature vectors describing the interacting entities.
    Usual single-output learning algorithms can then be applied to the problem.
    \begin{itemize}
        \item \textbf{Main advantage:} Enables the estimator to consider features from both instance domains simultaneously.
        \item \textbf{Main disadvantage:} It can be computationally infeasible to build the feature vectors and train the estimator for all possible interactions.
        %The distinct roles of the two domains are not explicitly considered, and the estimator may not be able to effectively capture the interactions' heterogeneity.
    \end{itemize}
    %This approach is particularly impacted by the aforementioned scalability issues resulting from the large number of possible interactions, which leads many authors towards undersampling negative interactions, likely at the expense of increasing false positives. % TODO: confirm
    \item \textbf{Local multi-output:} This strategy separates the bipartite learning problem into two multi-output tasks.
    We first select one of the domains as input and the other as output. We train a multi-output estimator to receive the features of the input domain and predict its interactions with the output domain. Each instance in the output domain is considered an output to me modeled, so that the features of the output domain are completely disregarded in this step.
    The same procedure is then repeated with the roles of the domains reversed. Predicted interactions of these two models are fed into a new round of training that occurs in a similar fashion. A total of four models is necessary to predict interactions between instances that were both not present in the training set.
    \begin{itemize}
        \item \textbf{Main advantage:} A much smaller number of input samples is considered, in comparison to the global single output approach. As a consequence, the models are usually faster to train.
        \item \textbf{Main disadvantage:} It requires training new models for each new batch of interactions to be predicted. This severely limits the application on online learning settings.
    \end{itemize}
    %While faster to train in general, the application of this approach in online settings is severely hampered by the need to train secondary models for each previously unseen instance.
\end{itemize}

Other methods adapt the learning algorithm itself to take full advantage of the bipartite format. These strategies tend to be more computationally efficient than the global single output approach, since they consider the dataset in a more compact format. Notwithstanding, they are still able to consider both domains simultaneously, unlike the local multi-output procedure. There is, however, a limited number of learning algorithms that can be adapted in a more fundamental level. Notable examples of estimator-centered adaptations are listed below.

\begin{itemize}
    \item \textbf{Linear regression:} Instance-instance similarities among each domain could be used to build a dyad-dyad similarity matrix. This matrix could then be used as a kernel matrix for training a linear regression model in a global single output fashion. The main problem with this idea is that a dyad-dyad kernel matrix would be often prohibitively large. Kron-RLS~\cite{vanlaarhoven2011gaussian} solves this issue, building the hypothetical linear model we described without explicitly obtaining the dyad-dyad kernel matrix.
    \begin{itemize}
        \item \textbf{Main advantage:} It is a remarkably efficient method.
        \item \textbf{Main disadvantage:} Linear relationships frequently are too simplistic to capture the complexity of the underlying interactions.
    \end{itemize}
    \item \textbf{Matrix factorization:} The adjacency matrix of the bipartite network is decomposed into two latent feature matrices, one for each domain. The product of these matrices approximates the original adjacency matrix, and can be used to predict new interactions between known instances. \citeonline{liu2016neighborhood} introduces a method to extend this idea to predict interactions with new instances, by encouraging instance-instance vicinities to be transfered from the original to the latent feature space.
    % TODO nrlmf uses side info
    \begin{itemize}
        \item \textbf{Main advantage:} It is a naturally scalable technique, being designed from the ground up for interaction prediction tasks.
        \item \textbf{Main disadvantage:} Interpretability is limited and a large number of hyperparameters is necessary.  % TODO it does not seem too much serious
        % other algorithms were shown to surpass it
    \end{itemize}
    %
    %\item \textbf{Kernel-based methods:} \cite{} demonstrates that a linear regression model can be built as if a dyad-dyad kernel matrix was used (which would be prohibitively large), by instead separately eigendecomposing the two intra-domain kernel matrices. However, while remarkably efficient, linear relationships frequently are too simplistic to capture the complexity of the underlying interactions, displaying limited predictive power.
    \item \textbf{Decision trees:} \citeonline{pliakos2018global} proposes an adaptation to the split search procedure occurring at each decision tree node. The search is performed locally for each domain, similarly to the local multi-output approach, but the resulting model is a single decision tree.
    \begin{itemize}
        \item \textbf{Main advantage:} Better scores are achieved in comparison to decision trees adapted with data-centric approaches~\cite{pliakos2018global}.
        \item \textbf{Main disadvantage:} No improvement in training complexity is observed relative to decision trees under the global single output adaptation~\cite{pliakos2018global}.
    \end{itemize}
\end{enumerate}

%Interestingly, on a general level, not many options seem to exist when developing machine learning algorithms specifically for this type of problem.
%Interestingly, on the highest level, there seems not to be a staggering variety of general frameworks under which. 
% there is not an expressive variety of algorithms
%%Interestingly, on the highest level, the variety of general frameworks and strategies specifically built under the bipartite paradigm is not staggering. % TODO rephrase
%
%The most common approach is to work with simple combinations of the feature vectors describing the components of each pair (by concatenating them, for example), effectively resetting the problem around the dyad as the primary atomic entity to be labeled. Transformed into a more traditional format, usual machine learning algorithms can then be applied to the problem. This approach, referred to as \emph{global single output} (GSO) by \cite{}, is particularly impacted by the aforementioned scalability issues resulting from the large number of possible interactions, which leads many authors towards undersampling negative interactions, likely at the expense of increasing false positives. % TODO: confirm
%Furthermore, inadvertently sampling out test or validation sets directly from the GSO-transformed data results in feature vectors for individual instances to be shared with the training set, even if dyads themselves are disjoint, 
%raising considerations about the resulting estimates of model generalization~\citep{pahikalla2015}.

% Other strategies, termed \emph{local multi-output} (LMO)~\citep{} involve building separate traditional models for each bipartite domain, considering instances in the unselected domain as feature-less outputs to be modeled. While faster to train in general, the application of this approach in online settings is severely hampered by the need to train secondary models for each previously unseen instance.

% Some interesting proposals modify estimators in a more fundamental way, diverging from the aforementioned strategies by not depending on organizing the bipartite dataset to be processed by a generic traditional algorithm. \cite{} demonstrates that a linear regression model can be built as if a dyad-dyad kernel matrix was used (which would be prohibitively large), by instead separately eigendecomposing the two intra-domain kernel matrices. However, while remarkably efficient, linear relationships frequently are too simplistic to capture the complexity of the underlying interactions, displaying limited predictive power.
% Under the name of \emph{Predictive Bi-Clustering Trees} (PBCT) \cite{pliakos2018predictive}, proposes decision tree-based models to directly tackle bipartite datasets, by localizing to each domain not entire models, as in LMO, but rather the split search procedure occurring at each tree node. Nevertheless, no improvement in training complexity is observed relative to GSO-adapted decision trees.

% Matrix factorization methods are an outcast in this general taxonomy of bipartite algorithms. Common in the context of recommendation systems, they are native to link prediction settings,
% being already successfully applied to bipartite problems. This class of algorithms is based on determining latent feature matrices for each domain, in a way that their matrix product approximates the adjacency matrix formed from the training labels. While originally agnostic to side-information and thus incapable of labeling new instances (what is sometimes referred to as the \emph{cold start} problem), ingenious adaptations have been proposed~\citep{liu} to circumvent this issue by encouraging intra-domain pairwise distances to be a transferable property from the original to the latent feature space~\citep{}.

%Deep learning strategies have been gaining popularity in the last years for problems such as the prediction of drug-target affinities~\citep{} or microRNA-gene interactions~\citep{}. However, they often rely on the same GSO transformation we mentioned before, suffering from the scalability issues we mentioned and often not reporting predictive performance for new instances, only new combinations. Furthermore, the particular lack of interpretability of these models is a long-discussed matter in the scientific literature~\citep{}, and more transparent reasoning supporting the obtained predictions could be an invaluable asset in the advancement of human knowledge, potentially uncovering new insights about underlying molecular dynamics or genetic regulatory mechanisms of various interaction phenomena.

This work will focus on decision trees. We explore how tree-based algorithms can be adapted to the bipartite learning paradigm, and how they can be tailored to address the challenges imposed by our learning setup.


\section{Why decision trees?}
\label{sec:why dt}
%\subsection{Why forest estimators are the \textit{bestimators}}

Decision trees are a very interesting option to explore. They show remarkable interpretability, flexibility to different tasks, and straightforward usage, besides demonstrating remarkable predictive power when combined into ensembles~\cite{breiman2001random,chen2016xgboost,zhou2019deep,grinsztajn2022why}.  % TODO cite more
We list below some of the main advantages of learning algorithms based on decision trees.

\begin{enumerate}
    \item \textbf{Interpretability:} Decision trees and forests are well-known transparent models, with several methods being available to interpret their predictions~\cite{breiman1984classification, lundberg2019explainable, agarwal2023mdi}. For instance, the frequency with which a feature is used by the tree can reflect its importance in the prediction process~\cite{breiman1984classification}.
    \item \textbf{No need for preprocessing:} In most cases, very little preprocessing is necessary to use decision trees. They can directly handle continuous or categorical features, and natively deal with missing feature values~\cite{breiman1984classification}. They are also not sensible to feature scaling, so normalization is most often not necessary.
    \item \textbf{Flexibility:} Decision trees can be used for both classification and regression tasks, and can handle either single or multi-output problems. They were successfully adapted to a wide range of scenarios~\cite{grinsztajn2022why}.
    %including time series forecasting~\cite{}, survival analysis~\cite{}, and unstructured inputs in general~\cite{zhou2019deep}.
    \item \textbf{Small number of hyperparameters:}
    They have a small number of hyperparameters in comparison to other algorithms such as deep neural networks, which makes them easy to use and tune~\cite{zhou2019deep}. % TODO cite more
    %The most common hyperparameters are the maximum depth of the tree, the minimum number of samples required to split a node, and the minimum number of samples required to be at a leaf node.
    %\item \textbf{Non-parametric:}
    %Decision trees are non-parametric models, meaning they make no assumptions about the underlying distribution of the data. This flexibility allows them to capture complex relationships without being constrained by a specific functional form.
    \item \textbf{Predictive power:} They can uncover complex non-linear relationships between features and labels, while being notably resistant to overfitting when combined into ensembles. They are widely recognized for their predictive performance especially on structured data~\cite{grinsztajn2022why}.
    \item \textbf{Fast inference:} Once trained, they can make predictions in logarithmic time complexity~\cite{breiman1984classification}.
\end{enumerate}

%Whith all these benefits in mind, the main focus of this study is to explore the potential of decision forests for bipartite learning.
We thus investigate how the potential of decision forests can be applied to bipartite learning.
%
%We introduce three main methodology refinements to address the challenges imposed by our learning setup:
We introduce three main methodology refinements, aimed at the high dimensionality, sparsity, and lack of information imposed by our learning settings.
%
\begin{enumerate}
    \item \textbf{More scalable trees:}
    We experiment with different metrics to guide the tree growing procedure. Optimizing the algorithm for these metrics can offer improvements in training complexity, making bipartite trees more scalable.
    %\item \textbf{Missing information:}

    \item \textbf{Weighted neighbors prototypes:}
    We explore using similarity scores between instances to calculate the output value of each leaf node. This enables shallower trees to be built while increasing their ability to generalize. 

    \item \textbf{Semi-supervised impurities:}
    We build trees that consider how the instances are grouped in the feature space, and not only their labels. This improves the model's resilience to missing annotations.
\end{enumerate}

%Finally, we present the main research questions and objectives of our work.

%We address biclustering trees, proposing improvements to the original algorithm. Our proposed modifications are introduced below.
%
%The main focus of this study is decision tree strategies.
%Decision trees offer solid advantages 
%
%On the other hand, decision tree-based methods are widely known for their transparency, flexibility and straightforward usage~\cite{}, having a small number of hyperparameters and being successfully applied to a wide range of learning problems. Hence, in this study, we concentrate on decision tree algorithms for interaction prediction. More specifically, we address the presented challenges by proposing improvements to the concept of biclustering trees developed by \cite{pliakos2018}. Our proposed modifications are introduced below.
%
% \begin{itemize}
%     \item \textbf{Faster training:} By employing a global impurity metric and slightly modifying the training procedure of PBCTs, we are able to achieve a $\log n$ speedup in asymptotic training time while generating models identical to GSO-adapted trees built on all possible dyads in the training set. Theoretical complexity analysis and empirical results on artificial datasets showing high statistical significance solidly support this result, paving a promising path for the application of bipartite trees to naturally high-dimensional interaction problems.
%     %\item \textbf{Interpretability:} Tree-based models are 
%     \item \textbf{Weighted neighbors prototypes:} Results from the original PBCT paper~\cite{pliakos2018predictive} suggest that when one of the entities composing a pair being predicted is present in the training set, restricting the prototype calculation in the final leaf to the labels of the known instance improves predictive performance. We employ this procedure for the first time in forests of bipartite trees and generalize the idea by exploring several similarity-weighted averages for prototype calculation, showing that quadratic similarity weighting, in particular, significantly outperforms the remaining options when new entities are presented.
%     \item \textbf{Semi-supervised impurities:} Taking the intrinsic large amount of unlabeled data of interaction problems into account, we are the first to explore the use of semi-supervised impurity metrics in the training of bipartite forests, while also proposing an efficient impurity function for kernel matrices, once again acknowledging the scalability concerns of bipartite learning. We show that this modification can significantly improve the model's resilience to unknown interactions by measuring their performance under random masking of positive labels in the training set.
%     % \item \textbf{Implementation:}  %TODO
% \end{itemize}

%Methods such as \cite{} consist of a preprocessing step and further global adaptation

% TODO: our previous paper

\section{Related work}
\label{sec:related_work}

% REESCREVER E COMPLEMENTAR  %TODO (other than dti, more forest approaches)

This section provides a brief overview of previous investigations related to bipartite forests, pointing out limitations to be addressed in this work.

\citeonline{schrynemackers2015classifying} stablishes the idea of the global single output, local single output, and local multi-output adaptations of traditional machine learning algorithms. In their study, they analyse the performance of random forests (RF)~\cite{breiman2001random} and ensembles of randomized trees (ERT)~\cite{geurts2006extremely} adapted in both ways and applied to biological interaction prediction. They consider not only bipartite but also homogeneous networks, such as protein-protein interactions or gene-gene interactions~\cite{schrynemackers2015classifying}.

In \citeyear{pliakos2018global}, \citeauthor{pliakos2018global} present a global multi-output decision tree for interaction prediction, named the predictive bi-clustering tree (PBCT) algorithm.
The method adapts decision trees on a more fundamental level to interaction scenarios, resulting in improved results over the previous adaptations~\cite{pliakos2018global}.
However, the training complexity of the proposed trees is still the same as that of the previous adaptations~\cite{pliakos2018global}, and the authors do not explore the use of semi-supervised techniques.
% Although directly considering the bipartite data format, with separate design matrices for each domain, their proposal is still limited by the same asymptotic complexity of GSO-adapted trees.

The authors, in \citeyear{pliakos2019network}, extend the PBCT algorithm to ensembles, building bipartite versions of RF and ERT~\cite{pliakos2019network}. They demonstrate superior performance against the other models in the comparison.
%
Later, in \citeyear{pliakos2020drugtarget}, they explore the use of neighborhood-regularized logistic matrix factorization (NRLMF)~\cite{liu2016neighborhood} as a pre-training step to the bipartite ERT~\cite{pliakos2020drugtarget}. NRLMF is used to generate continuous pseudo-labels for negative interactions, that are imputed in the interaction matrix before training the forest estimator. They demonstrate significant improvements in prediction scores resulting from this procedure~\cite{pliakos2020drugtarget}, sugesting that label imputation and other semi-supervised techniques could be beneficial.

\citeonline{santos2021predictive} extends the application of PBCTs to hierarchical multi-label classification problems. In ths context, the bipartite trees are able to split apart not only samples but also labels, resulting in competitive predictive performance.

\citeonline{levatic2017semisupervised} and \citeonline{adiyeke2022semisupervised} independently propose a way to consider semi-supervised assumptions when building decision trees.
They do so by penalizing the selection of split points that separate close training instances. As a result, instances within the same tree node, that tend to yield the same output, also will tend to have similar feature vectors.
The method is based on the assumption that neighboring instances are more likely to have similar labels (the \emph{smoothness} assumption~\cite{chapelle2006semisupervised,vanengelen2020survey}). The impact of missing or incorrect labels on the split selection is then reduced, since we delegate part of their influence to the feature space directly.
%
While \citeonline{levatic2017semisupervised} uses variances of features as the impurity function, \citeonline{adiyeke2022semisupervised} uses the structure of random trees to determine similarity values~\cite{liu2008isolation}. Neither of these works, however, is centered in interaction prediction.

In \citeyear{alves2023semisupervised}, \citeonline{alves2023semisupervised} employed semi-supervised impurities with bipartite trees.
To enhance performance, the authors do not use the semi-supervised impurity to evaluate every possible split. Instead, they first determine the best split of each of each feature domain in a supervised manner, and then use the semi-supervised impurity to choose between the two best supervised splits. This procedure is performed at each tree node. The algorithm results in better predictions than the fully supervised approach, while being faster than the original semi-supervised proposal~\cite{alves2023semisupervised}.
The authors also propose a different way to balance the influences of the supervised and unsupervised objectives. They dynamically determine new weights for each objective in each tree node, based on the amount of labeled data available~\cite{alves2023semisupervised}.
Nonetheless, the authors do not explore ensembles of bipartite trees, or the more expensive semi-supervised evaluation of every candidate split. They also do not explore other heuristics for dynamic supervision.

%In the present work, we thus demonstrate how bipartite trees can be grown faster with a modification of their training procedure. We also apply the original proposal of a semi-supervised split choice criterion to bipartite trees and compare it to the more efficient variant proposed by \cite{alves2023semisupervised}. We also propose an intermediate-complexity approach for kernel features, enabling semi-supervised evaluation of every candidate split while still being faster than the original idea. 

%TODO cv pairwise, an important detail often not mentioned
%TODO new heuristics for dynamic supervision

\section{Research questions}
\label{sec:research questions}

%The present work will be centered at the following research questions:
The present work will explore current open questions regarding bipartite forests. We explore how these algorithms can be improved both in terms of training complexity and predictive performance, and how they compare with other prominent techniques for interaction prediction. We enumerate our main research questions below.

\begin{enumerate}
    \item \textbf{Can bipartite trees be faster?}
    
    Investigate how can we exploit the bipartite nature of the problem to achieve a speedup in the training of decision trees.

    \item \textbf{Are semi-supervised techniques beneficial for bipartite forests?}

    Test the hypothesis that semi-supervised techniques can improve the resilience of bipartite trees to missing annotations.

    \item \textbf{How do AUROC and AUPR differ in their assessment of model performance?}

    Evaluate in detail the differences between the two most common threshold-agnostic metrics for binary classification tasks: the Area Under the Receiver Operating Characteristic Curve (AUROC) and the Area Under the Precision-Recall Curve (AUPR).

    \item \textbf{How do bipartite forests compare with proficient models in the field?}

    Compare the performance of bipartite forests with other state-of-the-art models for bipartite learning.

\end{enumerate}

%\section{Organization}
%\label{sec:organization}
%
%We begin by describing the mathematical notation employed throughout this work (\autoref{sec:notation}), and by formally defining the learning problem at hand (\autoref{sec:problem_statement}). We then present the 