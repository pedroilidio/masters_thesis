%% USPSC-Cap2-Desenvolvimento.tex 

% ---
% Este cap√≠tulo, utilizado por diferentes exemplos do abnTeX2, ilustra o uso de
% comandos do abnTeX2 e de LaTeX.
% ---


\chapter{Development}
\label{cap:development}

% TEXTWIDTH: \printinunitsof{in}\prntlen{\textwidth}

% COLWIDTH: \printinunitsof{in}\prntlen{\columnwidth}

\section{Definitions}
\label{sec:definitions}

\subsection{Mathematical notation}
\label{sec:notation}

% TODO link prediction is different, no side features?

For any given matrix $M$, we denote by $M\el {ij}$ its element on the $i$-th row
and $j$-th column ($i, j \in \mathbb{N}^*$). Analogously, we represent by
$M\el{i\cdot}$ the vector containing $M$'s $i$-th row so that $M\el{i\cdot}\el j
= M\el{ij}$ and by $M\el{\cdot j}$ the column vector
$((M^\intercal)\el{j\cdot})^\intercal$ referring to the $j$-th column of $M$ so
that $M\el{\cdot j}\el{i1} = M\el{ij}$. Defining the index notations as
superscripts enables the subscripts to be used only as indentifiers, naming the
matrix or vector as a whole and not each element. Indices are also
always represented by a single letter, to dispense the use of separators between
them.

% If multiple characters are needed, a comma or parentheses can be used
% $X\el{25,389}$ $X\el{(25)(389)}$

%Inspired by the usual notation $|\cdot|$ for the cardinality of a set,
We write
the total number of $M$'s rows as $|M|_i$ and its number of columns as $|M|_j$.
The total number of elements in $M$ is written $|M| = |M|_j|M|_i$, not to be
confused with the determinant of $M$.

We display filtered matrices or vectors by writing the condition as the index,
optionally enclosed by parentheses when necessary or improving readability (Eq.
\ref{eq:filter_notation}).
%
\begin{equation*}
    M\el{(i<3)j} \equiv \{M\el{kj}\;\mid \; k < 3\}
    \label{eq:filter_notation}
\end{equation*}

When summing over all indices in a given dimension, we took the freedom of
omitting the start and end positions (\autoref{eq:sum_notation}).
%
\begin{equation*}
    \sum_i M\el{ij} \equiv \sum_{i=1}^{|M|_i} M\el{ij}
    \label{eq:sum_notation}
\end{equation*}

We also made the choice of representing averages in a more concise way,
optionally pondered by sets of $w_1$ and $w_2$ weights in each respective axis
(\autoref{eq:avg_notation}).
%
\begin{equation}
    \begin{split}
        M\mel i \el j
            &\equiv \frac{\sum_i w_1\el i M\el{ij}} {\sum_i w_1\el i}\\
        M\el i \mel j
            &\equiv \frac{\sum_j w_2\el j M\el{ij}} {\sum_j w_2\el j}\\
        M\mel{ij}
            &\equiv \frac{\sum_j \sum_i w_2\el j w_1\el i M\el{ij}} {\sum_j \sum_i w_2\el j w_i\el i}
    \end{split}
    \label{eq:avg_notation}
\end{equation}

The enclosing of indices within brackets also allows for the omission of
parentheses when concomitantly using exponents, as exemplified by Eq.
\ref{eq:avg_index_notation}, and we additionally reserve ourselves the freedom
of representing each index only once, which in the last two shown cases requires
preemptively defining that $i$ and $j$ respectively represent rows and columns.
Notice that dispensing parentheses makes important the order in which the
exponent and averaged indices (those within $\langle \cdot \rangle$) appear. The
position of indices within $[\cdot]$ is however facultative, and is here chosen
to be as close as $M$ as possible in order to avoid confusion with $M^2=MM$.
Also notice that the indices within $[\cdot]$ will be the indices of the
resulting matrix or vector.
%
\begin{equation}
    \begin{split}
        M \el {ij} ^2 &= ((M \el{ij})^2)\el{ij}\\
        M \mel {ij} ^2 &= (M \mel{ij})^2\\
        M ^2 \mel {ij} &= (M \el{ij}^2)\mel{ij}\\
        % M \el i \mel j ^2 &= ((M \el i \mel j)^2)\el i\\  % corollary
        % M \mel i \el j ^2 &= ((M \mel i \el j)^2)\el j\\  % corollary
        %
        M \el i ^2 \mel j &= M \el{ij}^2\mel j\\
        M \el j ^2 \mel i &= M \el{ij}^2\mel i
        %M \el i ^2 \mel j &= (M \el{ij}^2)\el i\mel j\\
        %M \el j ^2 \mel i &= (M \el{ij}^2)\mel i\el j\\
        %
        % M \mel i ^2 \el j &= (M \mel i \el j^2) \el j
        % M \el i \mel j ^2 &= (M \el i \mel j^2) \el i
    \end{split}
    \label{eq:avg_index_notation}
\end{equation}


\subsection{Problem statement}
\label{sec:problem_statement}
% TODO disambiguate similar terms

The supervised machine learning applications focus on modeling a function $f
\colon \mathbb{R}^{n_f} \to \mathbb{R}^{n_o}$ whose exact underlying mechanism
is unknown or costly to implement. As a result, the only information available
about such mapping is a set of inputs $\{x_i \in \mathbb R^n_f\}$ and their
corresponding outputs $\{y_i \in \mathbb R^n_o\}$ of that given function. The
goal is therefore to build an \textit{in silico} model (or estimator) $\tilde f$
that approximates $f$, yielding as similar as possible outputs for the same
given input, even and especially for outputs not utilized in the process of
building $\tilde f$.
%TODO: i1 i2 instead of i k.
%TODO: define 'sample' or 'instance'

%This includes a plethora of real-life phenomena, such as {...}.
% define sample/instance

The known input vectors are usually organized as rows of an $X$ matrix so that
$X\el{ij} = x_i\el j$, and we refer as \emph{feature} or \emph{attribute} to
each specific horizontal position $j$ of $x\el j$, which corresponds to a column
of $X$. Likewise, a $Y$ matrix is built with their corresponding outputs
($Y\el{ik} = y_i\el k$). Commonly referred to as "targets" in the context of
regression learning, we here call the known outputs of the modeled process by
\emph{labels}, as in classification, even if real-valued, to avoid confusion
when referring to the protein targets of a drug.

In the present setting, we concentrate on problems involving the interaction of
two domains of instances (also called sample groups). As such, each sample
domain forms a different $X$ matrix, that we term $X_1$ and $X_2$. Only
inter-domain interactions are allowed, that is, instances are restricted from
interacting with others in the same sample group, so that the interaction
network constitutes an undirected bipartite graph.
%
Each interacting pair of instances is also called a \emph{dyad}.

The output, in our case, is any scalar piece of information describing the
interaction between a given instance pair, such as the rating of a movie given
by a user or a kinetic parameter of an enzyme-substrate reaction.
The labels are
then disposed in a $|X_1|_i$ by $|X_2|_i$ adjacency matrix $Y$ (also called
interaction matrix) so that the function to be modeled can now be formulated as
mapping the pair's vector representations to the interaction label $f\colon
(X_1\el{i\cdot},\; X_2\el{k\cdot}) \mapsto Y\el{ik}$.

Since each sample in $X_1$ refers to a \emph{row} of $Y$ and each sample in
$X_2$ corresponds to a \emph{column} of $Y$, we sometimes refer to the sample
domains of $X_1$ and $X_2$ as \emph{row samples} and \emph{column samples},
respectively.
%We call these datasets \emph{bipartite}, to differentiate from the more common \emph{traditional} problems, in which a single $X$ matrix is utilized.
We call these datasets \emph{bipartite}, to differentiate from the more common problems in which a single $X$ matrix is utilized.
%TODO traditional?

%PU data, binary, drug-target

%Train test

% Let $X_a$ be a feature matrix, the index $a$ representing each the two
% bipartite sets of instances, such that each instance is written $X_a \el i\;
% \forall\; i \in \mathbb N,\, 1 \le i \le |X_a|_i$ and each instance's feature
% is denoted $X_a\el{i, j} \;\forall\; j \in \mathbb N,\, 1 \le j \le |X_a|_j$.
% Since data is bipartite in the setting we are considering, $a$ can only assume
% the values $1$ or $2$.

% Let $Y$ be the $(n_{1s}, n_{2s})$ labels matrix such that the element $Y\el{i,
% j}$ characterizes the interaction occurring between the instances $X_1\el i$
% and $X_2\el j$.

% $X$ in algorithms is $X_\text{SGSO}$ or ... all info about X


% \section{Learning algorithms for bipartite interaction prediction}
% \label{sec:literature_algorithms}

\section{Data-centered adaptations of learning algorithms to bipartite interaction prediction}
\label{sec:standard adaptations}

%As previously stated in \autoref{sec:problem_statement},
As previously discussed,
bipartite interaction problems fundamentally differ from the usual machine learning paradigm, in which input data represents a single entity to be labeled. Interaction tasks are instead concerned with labeling a relationship between two entities, and as such, each prediction is made upon a pair of feature vectors, each vector being specific to each of the two sample domains.

Such subtlety is most often bypassed by reformulating a bipartite dataset into the traditional machine learning setting~\cite{vert2008reconstruction}.
%
These strategies can be encompassed under two general approaches, initially termed \emph{global} and \emph{local} by \textcite{vert2008reconstruction} and later adopted and extended by \textcite{sschrynemackers2015}.  %TODO cite more
% TODO maybe direct citation to specify our changes
For the sake of clarity and generality, we further specify these categories by defining \emph{global} and \emph{local} as general properties of estimators, rather than specific training procedures:
%
\begin{itemize}
    \item \emph{\textbf{Global}} estimators are those aware of both instance domains during the training procedure ($X_1$ and $X_2$).
    \item \emph{\textbf{Local}} estimators are those which only have access to feature information from one of the two instance domains during training (either $X_1$ or $X_2$).
    As such, they are often employed in compositions, combining the predictions of several local models to produce the final output.
\end{itemize}
%
%We refer to \emph{traditional} estimators as those who are not specifically designed to deal with bipartite data, and \emph{bipartite} estimators as those who are. This distinction is important to differentiate between estimators that are naturally able to deal with bipartite data and those who are not, but can be adapted to do so.
Furthermore, to be consistent with \cite{schrynemackers2015classifying,pliakos2018,pliakos2019,pliakos2020}, we assume the following definitions in our current context of bipartite interactions:
%
\begin{itemize}
    \item \emph{\textbf{Single-output}} estimators are those which consider all labels, i.e. all $Y\el{ij}$ elements, irrespectively of the column $j$ or row $i$ they are in. They are all regarded as a \emph{single type of output}.
    \item \emph{\textbf{Multi-output}} estimators, on the other hand, are those which consider each instance, each row or column of $Y$, as a separate task, for example by defining a composite loss function formed by the combination of losses over each row or column.
\end{itemize}
%TODO another term for multi-output, please. This is too confusing with multi-output in the traditional sense. bipartite multi-output are local models that use multi-output traditional estimators? or 3D interaction matrices with multiple labels for each interaciton?
Notice that the label matrix $Y$ can still be represented in two dimensions even if the model is single-output in this sense, contrary to the usual case where bidimensionality of $Y$ is a defining characteristic of a multi-output problem.

Finally, the two most common ways of adapting traditional estimators to bipartite data are then named \emph{global single-output} (GSO) and \emph{local multi-output} (LMO), as proposed by \textcite{pliakos2018,pliakos2019,pliakos2020}. We further denote them \empth{standard}, to clearly distinguish them from new adaptation proposals that could share the globality, locality, or outputness properties, but work in an entirely different way.
%
Specific definitions and shortcomings of these procedures are now presented.


\subsection{The standard global single-output adaptation}
\label{sec:sgso}

% TODO define SGSO acronym
%TODO literature (ben-hur? vert? pliakos? schynemackers?)
%Arguably the most straightforward and commonly used bipartite adaptation strategy
Arguably the most straightforward way to provide bipartite data into a conventional learning algorithm 
%strategy for adapting bipartite data to
%be inputted into
%conventional estimators.
%is through presenting concatenated pairs of row-column samples, labeled by each element of $Y$.
is by presenting concatenated pairs of row-column samples, labeled by each element of $Y$.
%
% The interacting dyad itself is what we abstract as a sample in this case.
The interacting dyad is now what we consider a sample, and each feature vector is the combination of the feature vectors of the two interacting instances.
% , and each feature vector is the combination of the feature vectors.
This is usually done by converting the two $X$ matrices and the
interaction matrix $Y$ to a single design matrix we call $X_\text{SGSO}$ and a column-vector of labels we refer to as $Y_\text{SGSO}$.

One way of doing so is by choosing indices as described by \autoref{eq:gsodata}, where
$\begin{bmatrix} x_1 & x_2\end{bmatrix}$ denotes concatenation and all $i_1$ and
$i_2$ combinations are explored (see \autoref{fig:sgso}).
%$f\colon (X_1\el{i\cdot},\; X_2\el{k\cdot}) \mapsto Y\el{ik}$.  To make use of
%traditional learning algorithms, one would combine instances from both $X_a$
%sets in a matrix we hereafter call $X_\text{SGSO}$, where each row is built by
%concatenating a row from $X_1$ followed by a row from $X_2$. $Y_\text{SGSO}$ can then
%take the shape of a one-dimensional column vector labeling each instance pair,
%as usual for single-output problems.

%This way of formatting interaction datasets is named by \ref{pliakos2018} as
%\emph{global single output} (GSO) scenario, and a possible definition is
%presented by \ref{eq:gsodata}, where $\lceil \cdot \rceil$ is the ceil function
%and $\concat$ denotes vector concatenation.
%
\begin{equation}
    \begin{split}
    i_g &= (i_1-1)|X_2|_i+i_2-1\\
    X_\text{SGSO} \el{i_g\cdot} &= \begin{bmatrix} X_1 \el{i_1\cdot} & X_2\el{i_2\cdot}\end{bmatrix}\\
    Y_\text{SGSO} \el{i_g1} &= Y\el{i_1i_2}
    %X_\text{SGSO} \el{i} &= X_1 \el{\lceil i/|X_1|_j \rceil} \concat X_2\el{i \mod |X_2|_j}\\
    %Y_\text{SGSO} \el{i} &= Y\el{\lceil i/|X_1|_j \rceil,\; i \mod |X_2|_j}
    \end{split}
    \label{eq:gsodata}
\end{equation}

To consider all possible dyads, $X_\text{SGSO}$ would have $|X_1|_i|X_2|_i$ rows and $|X_1|_j+|X_2|_j$ columns. Thus, $X_\text{SGSO}$ as defined by \autoref{eq:gsodata} is highly redundant.
%Reformatting $Y$ has no impact in memory usage, with $Y_\text{SGSO}$ having a single column but all the same $|X_1|_i|X_2|_i$ elements as $Y$.
%However, as stated in section \ref{sec:definitions}, $X_\text{SGSO}$ would be a $|X_1|_i|X_2|_i$ by $|X_1|_j+|X_2|_j$ matrix if one intends to consider all interactions in $Y$, which
As a result, naively dealing with such a large $X_\text{SGSO}$ matrix is impeditive in many cases, both in terms of memory usage and computation time. Therefore, a commonly used workaround is to undersample the negative annotations, yielding a dataset with equal amounts of positive and negative interactions~\cite{}.
This strategy is justified by the negative-annotated dyads being usually far more numerous than positives and much more likely to be truly negative interactions than false negatives~\cite{}.
%Despite such reasoning, we demonstrate in \autoref{} that taking all negative samples into consideration instead has significant benefits, and we propose new model optimizations to enable it (\autoref{}).
Nonetheless, we demonstrate in \autoref{sec:adaptation_comparison} that undersampling negatives is significantly detrimental if the main goal is to indicate wich interactions are most likely to be true.

% TODO cross-validation is naively done

\subsection{The standard local multi-output adaptation}
\label{sec:slmo}

The local approaches, in contrast to global methods, propose training different models on $X_1$ and $X_2$, so that each estimator only has access to information regarding either row samples or column samples.

As such, multiple non-bipartite models need to be used in conjunction to predict interactions between new row samples and new column samples. The standard local multi-output (SLMO) approach uses two traditional estimators for each axis (four in total), that we here refer to as \emph{primary} and \emph{secondary} estimators.
In general, they must be multi-output estimators, each being able to output a bidimensional $Y$ matrix.
%receive $X_\text{train}$ and $Y_\text{train}$ bidimensional matrices in the training step, receive an $X_\text{new}$ in the prediction step such that $|X_\text{new}|_j=|X_\text{train}|_j$, and outputting $Y_\text{pred}$ with $|Y_\text{pred}|_i=|X_\text{new}|_i$ newly predicted rows and $|Y_\text{pred}|_j = |Y_\text{train}|_j$ output columns.

The procedure for training the estimators in an SLMO setting consists of simply training the primary estimators. The primary rows estimator is trained on $X_1$ and $Y$, and the primary columns estimator on $X_2$ and $Y^\intercal$ (\autoref{alg:train_local_model}). The prediction step is however more complicated, involving the training of the secondary estimators on predictions of the primary models.
The procedure is described in details by \autoref{alg:predict_local_model} and illustrated by \autoref{fig:slmo}. The \KwCombine function used in line \autoref{ln:combine_local_outputs} of the \autoref{alg:predict_local_model} procedure can be arbitrarily chosen, and is usually defined as the simple element-wise average of both matrices (\KwCombine{$Y_1$, $Y_2$} $= \frac{1}{2}(Y_1 + Y_2)$).

%, optionally combined with the original training data

\algTrainLocalModel
\algPredictLocalModel

An unusual behaviour occurs if the secondary estimators are able to exploit inter-output dependencies. In this case, the outputs of the SLMO composition would depend on the amount of test data and specific combinations of test instances provided. This is usually not the case in traditional learning problems, and complicates the evaluation procedure. The behaviour forces us to consider the size and sampling method of the batches of test data. For online aplications, the ideal strategy would be to retrain the secondary models for each batch of new instances, using the predictions from the primary models over all the previously received instances (labeled and unlabeled). As a result, the application of SLMO models is effectivelly restricted to offline learning contexts.

%Notice that, if the secondary multi-output estimators treat each label independently, including the $Y_\text{train}$ labels in their training will make no difference, and one should use only the predictions from the primary estimators.

%A specific case of a model with multiple independent-outputs occurs when a collection of single-output models is utilized as a unified entity, each being trained on each column of $Y_\text{train}$.

%% This setup was present in the first proposal of a local model \cite{}, and enables a wider range of learning algorithms, not just multi-output strategies, to be employed in interaction prediction.  (WRONG)
%
%On the contrary, if the secondary estimator can exploit inter-output dependencies, one might consider concatenating the primary estimators' predictions to $Y_\text{train}$ and use both to train the secondary estimators (see lines \ref{} of Function \ref{alg:predict_local_model}). This setting would enable the secondary models to explore the output relationships involving the original training set, which are arguably more reliable than those between the primary predictions alone.
%
%That said, another consideration regarding the use of dependent-outputs secondary estimators is whether or not to provide the whole $X_{1\text{new}}$ and $X_{2\text{new}}$ at once, since doing so would increase the amount of primarily-predicted data used to train the secondary estimators, and it may be desirable to have more columns coming directly from $Y_\text{train}$ rather than inferred by the primary models. The ideal scenario then would be to run \ref{alg:predict_local_model} once for every $X_{1\text{new}}$ and $X_{2\text{new}}$ row combination, in a total of $|X_{1\text{new}}|_i|X_{2\text{new}}|_i$ iterations, with possible performance drawbacks for most learning algorithms. The natural intermediate idea would be to provide $X_{1\text{new}}$'s and $X_{2\text{new}}$'s rows in batches, possibly increasing the prediction time but ensuring the $|X_{a\text{new}}|_i/|X_{a\text{train}}|_i$ ratio is not detrimentally high. Additionally, some algorithms allow for output weights to be used in the training procedure, enabling us to assign lower importance to the $Y_{\text{new}}$ columns inferred by the primary estimators.
%
%In any case, contrary to what is usually observed, the amount of test data and specific combinations of test instances provided to the SLMO ensemble clearly affect the resulting predictions when the secondary models have inter-dependent outputs. This characteristic should thus be taken into consideration when developing, evaluating and comparing bipartite estimators under the SLMO configuration, although seldomly addressed by previous authors in our experience.%TODO: harsh?
%
%Due to each traditional model being provided with a much lower number of instances in comparison to the SGSO procedure, SLMO models tends to be naturally faster to train than SGSO models. However, a striking limitation of the SLMO procedure is caused by its inference phase, which requires traing of the secondary models whenever new instances are inputted. The resulting large prediction times hinders the application of SLMO models on online learning scenarios.
%
% problems of predicting.

%provide examples of each case (dependent vs independent outputs)
% we try to join multiple similar ideas under a single framework

%The idea was first proposed by \cite{}, and \cite{} expanded the concept to multioutput models to predict completely new pairs. The \cite{}

%examples of estimators in each category DNLMF, DNILMF BLM BLM-NII, etc.

%list characteristics of each main category:
% gso receives x1x2 lines
% matrix reconstruction algs dont predict by themselves

\section{Bipartite decision forests}
\label{sec:bipartite forests}

%A \textit{sui generis} learning algorithm adaptation was proposed by \textcite{pliakos2018} to deal with bipartite data, without the need for dataset reformatting as in SGSO, or compositions of multiple estimators and secondary training steps as SLMO.

Some learning algorithms can be adapted in a deeper level to the bipartite context. These algorithms are able to directly receive the two $X$ matrices as input, without needing to combine them as a preprocessing step for training.  % TODO lmo?
We term them \emph{learner-centered} adaptations, in contrast with the data-centered strategies discussed in \autoref{sec:standard adaptations}.  % TODO standardize names

% TODO benefits are in the intro
% TODO shortcommings: slow and not semisupervised

This section explores the main focus of this work: learner-centered adaptations of decision forest models. \autoref{sec:learner centered} will then present two other learner-centered strategies, based on linear models and matrix factorization respectively.
%
A general description of top-down decision trees is now presented, as a theoretical foundation for the upcoming formal definition of bipartite decision trees (\autoref{sec:bipartite_trees}). % TOTOTO TODO


%Named Predictive Bi-Clustering Tree (PBCT) by the authors, it tunes the usual decision tree-growing algorithm to directly operate on bipartite interactions, building a single tree model directly on bipartite formatted datasets (using $X_1$, $X_2$ and $Y$). Importantly, their proposed algorithm inherits all the benefits of tree-based estimators, such as their well-known interpretability and remarkably low amount of hyperparameters~\cite().
%
%
%Intriguingly, PBCTs were only explored in a scarce number of previous studies~\cite{pliakos2018global,pliakos2019network,pliakos2020drugtarget}, to the best of our knowledge. A possible explanation is that no improvement in computational complexity of training times is observed with respect to SGSO-adapted decision trees~\cite{pliakos2018global}, even if drastically less memory is used by a PBCT in comparison to a naive implementation of SGSO.  %TODO more pbct papers
%Furthermore, no implementations of PBCTs are provided in sufficiently accessible and extensible formats, which could also have hindered its adoption by the scientific community.
%
%We thus turn our attention onto such tree-based algorithms, proposing optimizations proven to reduce asymptotic training times of bipartite trees by a $\log |X|_i$ factor, enabling larger bipartite datasets to have all its unknown (0-labeled) interactions considered in model training and bringing unprecedented scalability to tree and forest estimators on interaction prediction tasks.
%%Intriguingly, these estimators were only explored in a scarce number of previous studies\cite{}. We thus turn our attention onto them, proposing optimizations to enable all unknown interactions to be considered in training, so to bring unreported scalability of Random Forests in sparse problems of interaction prediction and recommendation.
%
%%TODO we made them accessible, bipartite_learn
%


%\subsection{Traditional decision trees}  % TODO traditional?
%\subsection{General decision trees}
\subsection{Decision trees}
\label{sec:dt}
%TODO describe origin and benefits of trees
% decision tree (DT)
% more accessible intro

Let's consider the scenario where a single protein of interest is selected, and we receive the task of determining which drug molecules will likely affect its physical structure or catalytic function.
We wish to find a systematic procedure to decide whether a given drug molecule $x_i$ will interact with our protein or not. To develop such a procedure, consider we have at our disposal a set of $n_f$ known drug molecules, whose degree of interaction with our protein of interest was previously experimentally determined. We can then describe a drug molecule $x_i$ in general by how similar it is to each of our $n_f$ known molecules, organizing this information as a vector $x_i = \begin{bmatrix}x_i\el{1} & x_i\el{2} & x_i\el{3} & \cdots & x_i\el{n_f}\end{bmatrix}$ so that $x_i\el{j}$ represents the similarity score between the drug $x_i$ and the $j$-th of our $n_f$ known drugs.

The hypothetical decision procedure we intend to determine could then be structured as a path with consecutive bifurcations. We always start at the same place, and, at each bifurcation, a question is asked about the drug $x_i$ in hands. The questions are in a standard format, exemplified by "Is $x_i$ more than 60\% similar to the 3rd known drug?", or $x_i\el{j} > t$, for a general known drug $j$ and similarity threshold $t$. The answer to the question in each bifurcation determines which of the two possible paths we should follow. No cycles are allowed in the path, and eventually, all routes reach final locations instead of bifurcations. Each final location contains a fixed value that will be returned as the final decision about the drug $x_i$'s effect on our protein of interest.

Such a decision procedure, structured as a binary tree path, is what is commonly referred to as a \emph{decision tree} (DT) model, illustrated by \autoref{fig:dt}. 
In this context, each bifurcation then represents a tree \emph{node}, and the final locations are called \emph{leaves}. The value outputted by each leaf is termed the leaf's \emph{prototype}.
%
%The well-known interpretability of decision trees results from these
%rules leading to each output being clearly defined along the tree structure
%is what results in the well-known transparency of decision trees, an attractive characteristic in fields such as drug discovery or regulatory network inference, where insights into the underlying processes are greatly valued.

%The described binary-tree structure is by far the most utilized~\cite{}, being ubiquitous to all tree-based estimators in the present study.

The main challenge, however, lies in the building process of such models: in how to determine the rules that define each fork and how we define the stopping criteria for a final decision to be yielded.

%As presented by \autoref{sec:definitions},
To build the decision tree, we are given a training set composed of the two bidimensional matrices $X$ and $Y$. The dataset represents $|X|_i = |Y|_i$ instances, each described by $|X|_j$ numeric features and labeled by $|Y|_j$ labels.
In the previous example, $|X|_i$ would be the number of drug molecules in the dataset
$|X|_j$ would be the number of reference drugs (used to obtain the similarities), and $|Y|_j$ would be the number of proteins of interest.

%(drug molecules in the previous example)

%so that $X\el{i\cdot} = x_i$ represents the $i$-th entity. Each entity is thus described by $|X|_j$ numeric descriptors (similarities to known drugs in the previous example). To each entity is assigned one or more numeric labels $Y\el{i\cdot}$ describing the known target prediction results. In analogy to the last example, the labels would denote interaction or not with one or more proteins of interest.

Consider now executing the prediction process of a decision tree for each training instance, going through the branched path. Each bifurcation would divide the training instances between those who answer the question affirmatively and those who answer negatively. Eventually, each leaf will contain a partition of the training instances.
From the resulting partitions, we can evaluate the decision tree:
a "good" decision tree would be one in which the prototype value of each leaf is a good estimate of the labels in the leaf's partition.
%
The training algorithm then approaches the problem from the other direction: what would be a good tree according to our training data?
A greedy top-down procedure is usually followed, that we introduce bellow.
%This evaluation is used by the training procedure to evaluate each candidate splitting rule. The best rules are selected 
%The training procedure uses such evaluation to determine the best possible splitting rules. The tree is built 

%The procedure for building a DT consists of determining features $f$ and threshold values $t$ that recursively split the dataset in two parts, named \emph{left} and \emph{right} partitions, as defined by \autoref{eq:datasplit}.
Formally, each rule is encoded by an index $f$ representing a feature column and a threshold value $t$. Each rule then represents a split of the training dataset in two partitions, named \emph{left} and \emph{right}, as defined by \autoref{eq:datasplit}.
%
\begin{equation}
    \begin{split} %TODO
        Y_\text{left} &=\{Y\el{k\cdot} \;\forall\; k \mid X\el{kf} \le t\}\\
        Y_\text{right} &=\{Y\el{k\cdot} \;\forall\; k \mid X\el{kf} > t\}\\
        X_\text{left} &=\{X\el{kf} \;\forall\; k \mid X\el{kf} \le t\}\\
        X_\text{right} &=\{X\el{kf} \;\forall\; k \mid X\el{kf} > t\}\\
    \end{split}
    \label{eq:datasplit}
\end{equation}

To build a decision tree, we start with the whole training set (the \emph{root node}). A procedure FindSplit is executed to yield a good splitting rule.
The training data is then partitioned according to this rule, as defined in \autoref{eq:datasplit}. Two descendant nodes are created, each receiving one of the partitions.
For each new node, we have two options: i) apply FindSplit to its data partition and continue recursively; or ii) set the node as a leaf, taking record of the partition it received.

The tree-building algorithms can thus be described by four key components:
%
\begin{enumerate}
    \item \textbf{The FindSplit procedure}, which determines a set of candidate splitting rules and selects the best one according to a quality metric.
    \item \textbf{The split quality metric} used by FindSplit to evaluate the candidate rules.
    \item \textbf{The stopping criteria}, that determine when a node should be set as a leaf.
    \item \textbf{The prototype function}, that determines the output value of a leaf.
\end{enumerate}
%
\autoref{alg:buildtree} describes in detail how these components come together to build a decision tree. \autoref{alg:predict} then formally details how predictions are made given a model built by \autoref{alg:buildtree} and a new data instance.
%
These procedures emcompass a wide range of decision tree algorithms, including the popular CART ~\cite{breiman1984classification}, ID3~\cite{quinlan1986induction}, and C4.5~\cite{quinlan}, as well as the bipartite trees we explore in the present work~\cite{pliakos2018global}.  %TODO VERIFY

The following sections provide further considerations on specific components.

\algPredict
\algBuildTree

%For each new node, we have two options: can either apply FindSplit to its data partition, or set the %node as a leaf, taking record of the partition it received.
%For each new node, we can either apply FindSplit to its data partition, or set the node as a leaf, taking record of the partition it received.

%For each partition, the same procedure is repeated, until a stopping criterion is met. The most common stopping criteria are a maximum depth of the tree or a minimum number of samples in a node, for instance.
%
%Each child node receives one of the data partitions generated by its parent, and the process is recursively repeated until a stopping criterion is met. The most common stopping criteria are a maximum depth of the tree or a minimum number of samples in a node, for instance.
%Each bifurcation of a DT, more commonly referred to as a \emph{node}, then represents one of such splits, defined by a selected feature $f$ and a threshold value $t$, and each of its two children receives one of the data partitions generated by its parent (see \autoref{}). Under specific pre-defined circumstances, a node stops generating descendant nodes, having no children and taking record of the dataset partition it received from its parent. Among possible stopping criteria are a maximum depth of the tree or a minimum number of samples in a node, for instance. These terminal nodes are called \emph{leaves}.

%We can also formally define the inference procedure illustrated in the introductory example.
%The prediction step for a new sample $x_\text{new}$ consists of transversing the tree from the root node until a leaf, following each node's test ($x_\text{new}\el{f_\text{node}} > t_\text{node}$).
%%and selecting the corresponding child node as given by the partitioning rule of \autoref{eq:datasplit}.
%Once a leaf is reached, the tree returns a prototype value calculated over the partition of the training data corresponding to that leaf. The average label ($Y_\text{leaf}^{\ang i [j]}$) of each output is a common choice for this prototype.
%


\subsection{Searching for the best split}
\label{sec:split search}

We explore two main strategies for the FindSplit procedure: the exhaustive search and the randomized search. The exhaustive search is the most common. The randomized search is a faster but less accurate alternative, intended to be used in the context of tree ensembles (\autoref{sec:ensembles}). We describe each procedure in the present section.

The most common approach to the FindSplit procedure is to consider all possible partitions of the given input data.
%
For a feature column $X\el{\cdot f}$, this exhaustive evaluation of partitions can be done by sorting $X\el{\cdot f}$ and considering a threshold $t$ between each two consecutive values in it. Notice that any threshold value between the same two consecutive $X\el{\cdot f}_\text{sorted}$ elements will result in the exact same partitioning of the training set (\autoref{eq:datasplit}). The common practice is thus to take the averages between each two neighboring feature values. The same procedure is then repeated for each feature column, and the overall best $t^\ast$ and corresponding feature index $f^\ast$ are selected.
%
%A greedy procedure is then followed for the overall tree growing, selecting at each node the best $t$ and $f$ to represent the split, according to a predefined quality criteria we further discuss in \autoref{sec:criteria}.
%
The exhaustive split search procedure is detailed by the \autoref{alg:find_best_split}.
%The algorithm \ref{alg:buildtree} then describes its application on growing a traditional decision tree (stablished by \textcite{breiman1984classification}),
%while \ref{alg:predict} formally details how predictions are made given a model built by \ref{alg:buildtree} and a new data instance.

\algFindBestSplit

The randomized search is an alternative that avoids considering all partitioning options and greatly reduces the amount of operations performed.
It consists of drawing a random threshold $t$ between the minimum and maximum value of each feature, thus evaluating only $|X|_j$ splits when choosing the best (\autoref{alg:find_random_split}). Although degrading the performance of a single tree, this procedure is an interesting option when building tree ensembles (\autoref{sec:ensembles}), being the core idea behind the extremely randomized trees (ERT) algorithm~\cite{geurts2006extremely}.  %TODO standardize alg names
Ensembles of decision trees will be discussed in \autoref{sec:ensembles}.

%\algSplitDataset
\algFindRandomSplit

%Notice that the described procedure requires us to sort the $X_\text{SGSO}$'s column corresponding to the $f$ feature ($X_\text{SGSO}\el {:,\,f}$), so that we can apply the same permutation that sorted $X_\text{SGSO}$ to $Y_\text{SGSO}$, yielding $X_{\text{permuted}}$ and $Y_{\text{permuted}}$, and generate the data split according to \ref{eq:sortedsplit}, being $i^*$ the index corresponding to the chosen threshold $t$ such that $t=\frac{1}{2}(X_{\text{permuted}}\el{i^*,\,f}+X_{\text{permuted}}\el{i^*+1,\,f})$.
%
%\begin{equation}
%    \begin{split}
%        X_l &= Y_{\text{permuted}}\el{:i^*}\\
%        X_r &= X_{\text{permuted}}\el{i^*:}\\
%        Y_l &= Y_{\text{permuted}}\el{:i^*}\\
%        Y_r &= Y_{\text{permuted}}\el{i^*:}
%    \end{split}
%    \label{eq:sortedsplit}
%\end{equation}

%prototypes

%\subsection{Split quality criteria and impurity metrics}
\subsection{Measuring the quality of a split}
\label{sec:criteria}

A split quality criterion must be defined so we can compare and select the best splitting rules at each node. The quality $\Delta I$ of a split is commonly framed as the decrease of an impurity metric calculated over the partitions of training labels (\autoref{eq:quality}). This decrease is taken for the combined impurities of the generated children nodes (\autoref{eq:datasplit}).
%relative to their parent's impurity.
All impurities are multiplied by the size of each partition relative to the total number of training samples ($|Y_\text{root}|$), restricting the effect of nodes with less data that could introduce spurious variations of impurity. Notice that $|Y_\text{node}|=|Y_\text{left}|+|Y_\text{right}|$.
% TODO: notice that |Y_a|_i / |Y_b|_i == |Y_a| / |Y_b|
%
\begin{equation}
    \Delta I(Y, t, f) =
        \frac{|Y_\text{node}|}{|Y_\text{root}|} I(Y_\text{node})
        - \frac{|Y_\text{left}|}{|Y_\text{root}|} I(Y_\text{left})
        - \frac{|Y_\text{right}|}{|Y_\text{root}|} I(Y_\text{right})
    \label{eq:quality}
\end{equation}

Several metrics can be chosen as the impurity function $I(\cdot)$, such as the Gini impurity, the Shannon entropy or the Poisson loss~\cite{}. In this study we utilize the variance of each output column, averaged over all outputs (\autoref{eq:mse}). 
%
%labels as the basis for calculating a split quality parameter (\ref{eq:mse}). We also set the prediction output of a leaf node (given by the prototype function) to be the average of labels in the training set encompassed by the leaf. Therefore, the impurity $I$ of a dataset partition corresponds to the variance of its labels: 
%
\begin{equation}
    I_\text{MSE}(Y)
        = (Y\el{ij} - Y^{\ang i[j]})^{2\ang{ij}}
        = Y^{2\ang{ij}} - Y^{\ang i 2\ang j}
    %I(Y) = \avg{\left(Y\el{i, j} - \avg{Y\el{i, j}}\right)^2} \text{,}
    %%\text{MSE}(Y) = \avg{(Y\el{i, j} - \avg{Y\el{i, j}})^2} \text{,}
    \label{eq:mse}
\end{equation}
%
Most commonly, the \textt{prototype} function returns the column averages of a leaf's partition of the training labels: $Y_\text{leaf}^{\langle i\rangle[j]}$.
In this case, the column variances correspond to the \emph{mean squared error} (MSE) for the training data in the node, as if the node holding $Y_\text{partition}$ were to become a leaf.
%
% Derivation:
%
%\begin{multline}
%    I_\text{MSE}(Y)
%        = (Y\el{ij} - Y\mel i\el j)^2\mel{ij}
%        \\= (Y\el{ij}^2 -2 Y\el{ij} Y\mel i\el j + Y\mel i\el j^2)\mel{ij}=\\
%        = (Y\el{j}^2\mel i -2 Y\mel i\el j Y\mel i\el j + Y\mel i\el j^2)\mel{j}=\\
%        = (Y\el{j}^2\mel i - Y\mel i^2\el j)\mel j=\\
%        = Y^2\mel{ij} - Y\mel i^2\mel j
%    \label{eq:mse}
%\end{multline}

Also notice that $I_\text{MSE}$ is equivalent to the Gini impurity if $Y$ contains only binary values. That can be shown by noticing that $Y_\text{bin}^{[ij]2} = Y_\text{bin}\el{ij}$ for binary labels, so $Y_\text{bin}^{2\ang{i}[j]}=Y_\text{bin}^{\ang i [j]}\equiv p\el j$, which yields \autoref{eq:gini}.
%, culminates in the usual form of the average of Gini impurities across all outputs.
%
\begin{equation}
    I_\text{MSE}(Y_\text{bin})
        = Y_\text{bin}^{2\ang{ij}} - Y_\text{bin}^{\ang{i}2\ang j}
        = (p\el j  - p^{\ang j 2})\mel j
        = [p\el j(1 - p\el j)]\mel j
    = I_\text{Gini}(Y)
    %I(Y) = \avg{\left(Y\el{i, j} - \avg{Y\el{i, j}}\right)^2} \text{,}
    %%\text{MSE}(Y) = \avg{(Y\el{i, j} - \avg{Y\el{i, j}})^2} \text{,}
    \label{eq:gini}
\end{equation}

%Referring back to bipartite scenarios, we now describe how intuitive adaptations of the decision tree algorithm are developed to deal with interaction data.


\subsection{Bipartite decision trees}
\label{sec:bipartite_trees}

In this section, we describe how the decision tree algorithm can be adapted to directly operate on bipartite interaction data.

Essentially, the adaptation process consists of defining bipartite versions of each of the four key components of the decision tree algorithm (\autoref{sec:dt}): the FindSplit procedure, the split quality metric, the stopping criteria, and the prototype function. From these, the most central is the FindSplit procedure adaptation.
%As presented by \autoref{sec:bipartite_forests}, \textcite{pliakos2018global} introduces an ingenious strategy to build a decision tree directly on bipartite-formatted datasets.
 
For bipartite trees, as proposed by \cite{pliakos2018global}, we perform the split search locally in each domain, in a similar fashion to the SLMO adaptation (\autoref{sec:slmo}).
% but specifically for each tree node. 
%
%The main concept behind it borrows the idea of the SLMO approach described in \autoref{sec:slmo},
%in that the training procedure is divided into two steps, separately considering each axis of the bipartite data. However, instead of training two separate models as in SLMO, two candidate versions of each node are generated, one for each axis, and the best between both is selected to integrate the final model.
%
%Formally speaking, consider a general procedure \FindSplit for finding a split threshold in a traditional multioutput decision tree (see \autoref{sec:dt}).
Specifically, let \FindSplit\textsubscript{trad} be a procedure for finding a split threshold in a traditional multioutput decision tree (see \autoref{sec:split search}).
%The algorithm by \cite{pliakos2018global}
A bipartite tree applies \FindSplit\textsubscript{trad} twice at each node: once over $X_1$ and $Y$ and once over $X_2$ and $Y\T$. This results in one split being chosen for each axis.
Finally, the best overall split is selected between the vertical and horizontal splits.
%horizontal split and the vertical split.
%, according to the quality criterion of \autoref{eq:q_lmo}.
%, so that each $Y$ \emph{row} is now interpreted as an output.
%in which case each $Y$ \emph{column} is considered a different output;
In essence, a bipartite decision tree algorithm uses a \FindSplit procedure that composes two traditional \FindSplit\textsubscript{trad} procedures, as detailed by \autoref{alg:find_bipartite_split}.

%Although each node locally performs the partitioning search on each axis, the resulting tree in its entirety is termed \emph{Global MultiOutput} (GMO) by the original authors, and its training procedure corresponds to using \FindSplit = \ref{alg:find_bipartite_split} in \ref{alg:buildtree}.

\algFindBipartiteSplit

%\begin{equation}
%    I = I(Y) + I(Y\T)
%\end{equation}
%
%Interestingly, contrary to the other adaptation approaches of traditional machine learning algorithms (\autoref{sec:standard adaptations}), the PBCT procedure does not require using multiple traditional estimators and training steps (such as SLMO, \autoref{sec:slmo}) or converting bipartite datasets to the memory-expensive global format such as SGSO (\autoref{sec:sgso}). Nevertheless, considering each row or column of $Y$ as a separate output results that growing a PBCT still occurs in the same asymptotic time complexity of a decision tree under the SGSO adaptation (derived in \autoref{sec:complexity_analysis}). 
%
%Remarkably, we show that leveraging the single-output assumption (\autoref{sec:standard adaptations}) in the tree impurity calculation can simplify the bipartite local split searching procedure introduced by \textcite{pliakos2018global}, yielding a new PBCT-based algorithm with expressive gains in training time efficiency. A formal description of our algorithm proposal is now presented.
%
%%Nevertheless, an argument could be made in opposition to this idea.
%
%%In some sense, a decision tree could be understood as an ensemble of trivial node-wise estimators, each predicting the mean of the training labels in its partition.
%
%%A decision tree training process could indeed be understood as a wrapper procedure around a simpler estimator. Within each node, the decision tree training searches for a way to separate the training samples into two complimentary partitions. Evaluating several of such splits, it is chosen the one which minimizes the variances of each of the two $Y$ partitions it generates. This could be interpreted as minimizing the mean squared error of a trivial estimator that outputs the mean of $Y$ values for any given input. Under this optics, a decision tree recursively searches for binary splits that most favor the performance of a wrapped estimator.
%%
%%Still under this interpretation, the PBCT algorithm then consists of applying to this simple estimator in each node a strategy that resembles the LMO adaptation discussed earlier. The search for the best split is executed locally, for each axis, and the best overall result is chosen between each axis' best.
%%
%%Although maybe a wrapper of a locally adapted trivial estimator, the tree structure generated after training has the exact same format as that of a traditional decision tree, so that the prediction procedure does not involve any training of component models as is expected with LMO-adapted models. Hence, these bipartite data-centered tree learning algorithms seem to not fit under any of the previously described categories, and we choose to inaugurate a third group we hereafter call \emph{native bipartite models}, encompassing estimators whose basic functioning is specifically designed to deal with interaction data in its bipartite format.

\subsection{Bipartite global single-output trees}
\label{sec:bgso_trees}
%Although being developed specifically to interaction data, the PBCT algorithm shows no improvement in training time complexity relative to the naive GSO approach, as derived in the Section \ref{sec:complexity_analysis}.

This section shows how we can grow bipartite trees more efficiently, making tree estimators more scalable to large bipartite datasets.

The original bipartite tree proposed by \cite{pliakos2018global} uses the impurity metric $I_\text{MSE}$ (\autoref{eq:mse}) to evaluate splits, so that the authors classify their technique as a \emph{Global MultiOutput} (GMO) estimator. \emph{Multi-output} because the impurity evaluates the average variance of each label column (or row), as if each column (or row) were a different output. \emph{Global} because each tree utilizes features from both feature domains for training (see \autoref{sec:standard adaptations}).
Their GMO trees, however, are shown to have the same algorithmic complexity as traditional decision trees trained with the SGSO adaptation. We demonstrate that faster training algorithms can be developed if we assume a single-output format under the context of bipartite trees. %instead of GMO, without .

%Notice that
The split rules as defined by \autoref{eq:datasplit} are agnostic to the specific arrangement of the bipartite data. They can be seamlessly applicable to either the ($X_\text{SGSO}$, $Y_\text{SGSO}$) format employed by the SGSO adaptation (\autoref{eq:sgso_data}) or directly to the $X_1$, $X_2$ and $Y$ matrices.
Furthermore, the impurity at each node of an SGSO tree,
%$I(Y_\text{SGSO})$,
in many cases can be translated to the bipartite format. \autoref{eq:bipartite_gso_equivalence} describes such a translation of the MSE impurity presented by \autoref{eq:mse}.
%
\begin{equation}
    I_\text{MSE}(Y_\text{SGSO})
        = Y_\text{SGSO}^{2\ang{ij}} - Y_\text{SGSO}^{\ang i 2 \ang j}
        % = (Y_\text{SGSO}\T)^{2\ang i} - (Y_\text{SGSO}\T)^{\ang i 2}
        = Y^{2\ang{ij}} - Y^{\ang{ij}^2}
    \label{eq:bipartite_gso_equivalence}
\end{equation}
 
%We can then define $I_\text{GMSE}$ as in \autoref{eq:gso_mse}:
If we then define
%
\begin{equation}
    I_\text{GMSE}(Y) \equiv I_\text{MSE}(Y_\text{SGSO})
        = Y^{2\ang{ij}} - Y^{\ang{ij}^2}\text{,}
    \label{eq:gso_mse}
\end{equation}
%
the exact same SGSO-adapted decision tree, with its node structure and split rules, can be grown by applying the bipartite procedure (\autoref{alg:find_bipartite_split}) with the $I_\text{GMSE}$ impurity instead of the original $I_\text{MSE}$. This again follows from the generality of the split rules defined in \autoref{eq:datasplit}, invariant under the SGSO data rearrangement (\autoref{eq:sgso_data}).

%Notice how the divergence in \ref{eq:mse_gmo} is computed relative to the average of each $Y$ column, while in \ref{eq:gso_mse} the inner average is computed over the whole $Y$ matrix.

%Using this new impurity poses a simple yet sound advantage over the original. 
Notice how the squared divergences in \ref{eq:mse_gmo} is computed relative to the average of each $Y$ column, while in \ref{eq:gso_mse} the inner average is computed over the whole $Y$ matrix.  % TODO ?
%This highlights the key difference between GSO and GMO impurities: GSO considers all elements of $Y$ as different values for the same output.

We can explore this property of the GSO impurity to iterate more efficiently over candidate splits.
This is done by pre-computing averages of each row and column of $Y_\text{node}$. We can then iterate over one-dimensional $\tilde Y_\text{node}$ proxies (\autoref{eq:y_proxies}) instead of the bi-dimensional matrix when evaluating splits (\autoref{} of \autoref{alg:}).
%Having only total averages in \autoref{eq:gso_mse}, always involving both $i$ and $j$ indices simultaneously, exempts the training function from storing column-wise label averages during split search, enabling us to pre-compute averages of each row and column of $Y_\text{node}$ and to iterate over one-dimensional $\tilde Y_\text{node}$ proxies (\autoref{eq:y_proxies}) instead of the bi-dimensional matrix when evaluating splits (\autoref{} of \autoref{alg:}).   % TODO REF ALGO
%
%This property can be explored to build a more efficient training procedure for bipartite GSO decision trees, as demonstrated in the asymptotic complexity analysis developed in \autoref{sec:complexity_analysis}.
%
\begin{equation}
    \begin{split}
        \tilde Y_1\el{i} = Y\el{i}\mel{j}\\
        \tilde Y_2\el{j} = Y\mel{i}\el{j}
        %\tilde Y_1\el{i} = \savg{Y\el{i,j}}_j\el{i}\\
        %\tilde Y_2\el{j} = \savg{Y\el{i,j}}_i\el{j}
    \end{split}
    \label{eq:y_proxies}
\end{equation}
%
The same is not possible for the GMO trees, as \autoref{eq:mse} requires storing averages for each $Y$ column, to be squared individually.
\autoref{alg:find_bipartite_split} describes the adapted procedure in each case.
%
The complexity improvements are demonstrated theoretically in \autoref{sec:complexity_analysis} and empirically in \autoref{sec:empirical_complexity}.

% --------------
%However, in \autoref{eq:q_optimization} we demonstrate how a single-output impurity metric can be used directly in the bipartite setup. Being single-output, no distinction is made among $Y$ columns or rows, minimizing label deviance relative to the global average $Y\mel{ij}$ instead of column averages $Y\mel i\el j$. Considering such metric enables further optimization of the split searching procedure by employing single-column proxies of the interaction matrix (\autoref{eq:y_proxies}), as also described by the Function \ref{alg:find_bipartite_split} and justified in Section \ref{sec:complexity_analysis}.

% With the \ref{alg:find_best_split} procedure we intend to evaluate all possible data partitions with format as defined by \ref{eq:datasplit}.
%The bipartite decision trees grown with this procedure on $X_1$, $X_2$ and $Y$ have the exact same structure as a usual traditional decision tree trained on a dataset $X_\text{SGSO}$ and $Y_\text{SGSO}$ adapted with the traditional global strategy (Section \ref{}). This property can be intuitively shown by noticing that $X_1$ and $X_2$ already contain all the information necessary to define a set of thresholds that yields all possible partitions, since each partition considers a single $X$ column and building $X_\text{SGSO}$ does not removes or adds different elements to each of them (only repeats them).
%%and no conversion to the global traditional format ($X_\text{SGSO}$ and $Y_\text{SGSO}$) is necessary.
%This result is a consequence of \autoref{eq:gsodata} and is formally stated by \autoref{eq:bipartite_gso_equivalence}, in which we assume $f_2 = f_g-|X_1|_j$.
%
% \begin{multline}
%     \{Y_\text{SGSO}\el{k} \;\forall\; k \mid X_\text{SGSO}\el{k,\; f} < t\} =\\
%     \{Y_a\el{i,\; j} \;\forall\; i,j \mid X_a\el{i,\; f-(a-1)|X_1|_j}<t\}
% \end{multline}
% \[
% a=\begin{cases}
%     1, & \text{if} \;f \le |X_1|_j\\
%     2, & \text{otherwise}.
% \end{cases}
% \]
%\begin{equation}
%    \{Y_\text{SGSO}\el{k1}\;\forall\; k \mid X_\text{SGSO}\el{kf_g} < t\}
%    = \begin{cases}
%        \{Y\el{i_1i_2} \;\forall\; i_1\text{, }i_2 \mid X_1\el{i_1f_g}<t\}, & \text{if } f \le |X_1|_j\\
%        \{Y\el{i_1i_2} \;\forall\; i_1\text{, }i_2 \mid X_2\el{i_2f_2}<t\}, & \text{if } f > |X_1|_j
%    \end{cases}
%    \label{eq:bipartite_gso_equivalence}
%\end{equation}
%https://www.overleaf.com/project/62fbafab63095e6cfcb9a8ec


%As a result, we can substitute the procedure \ref{alg:find_best_split} by the wrapper procedure \ref{alg:find_bipartite_split} to efficiently build decision trees on bipartite data, directly using the much more compact $X_1$ and $X_2$ feature matrices rather than constructing the $X_\text{SGSO}$ GSO representation. A theoretical complexity analysis is presented by the section \ref{sec:complexity_analysis}.

%\begin{align}
%\begin{split}
%    & I(Y) =
%        \avg{(Y\el{i,j} - \savg{Y\el{i,j}})^2} =\\
%        &=\avg{(Y\el{i,j})^2 - 2Y\el{i,j}\savg{Y\el{i,j}}+\savg{Y\el{i,j}}^2} =\\
%        &=\savg{(Y\el{i,j})^2} - \savg{Y\el{i,j}}^2
%%
%    \label{eq:mse_alternative}
%\end{split}
%\end{align}
%
%being
%
%\begin{align*}
%    \tilde Y\el{i}=\langle w_i\el{j}Y\el{i,j}\rangle_j\el{i}
%\end{align*}
%
%This reduces the complexity of a variance-guided split search from $O(|X_1|_in_{ns2}(|X_1|_i+n_{ns2}))$ to $O((|X_1|_i+n_{ns2})^2)$.  % TODO


\subsection{Prototype functions for bipartite trees}
\label{sec:prototype}

When a leaf is reached during the prediction step of a decision tree, the \KwPrototype function is called to determine the output value to be returned (\autoref{ln:prototype} of \autoref{alg:predict}).
With traditional datasets, the \KwPrototype function
%of traditional decision trees
most often returns the average label of the leaf's partition (\autoref{eq:prototype1})~\cite{breiman1984classification}.
%most often returns, for each output, the average label of the leaf's partition (\autoref{eq:prototype1})~\cite{breiman1984classification}.
%
\begin{equation}
    \KwPrototype{Y\textsubscript{leaf}}\el j=Y_\text{leaf}^{\ang{i}[j]}
    \label{eq:prototype1}
\end{equation}

As an extension, the most natural approach to use on single-output bipartite trees is the analogous average of the whole partition of the interaction matrix (\autoref{eq:prototype_gso}).
%
\begin{equation}
    \KwPrototype{Y\textsubscript{leaf}}
        = Y_\text{SGSO, leaf}^{\ang{i}[1]}
        = Y_\text{leaf}^{\ang{ij}}
    \label{eq:prototype_gso}
\end{equation}

Nevertheless, some considerations are possible when dealing with bipartite data, since there are cases in which one of the entities of the interaction being predicted is already known from the training set. As introduced by \textcite{pliakos2018global}, if a row or column instance is in the training set, we have the option of averaging only the column or row (respectively) of $Y_\text{leaf}$ corresponding to its known outputs. Specifically, when predicting the interaction between a sample pair $x_{1\text{, new}}$ and $x_{2\text{, new}}$, we can set \KwPrototype as in \autoref{eq:prototype2}.
\begin{equation}
    \KwPrototype{$Y_\text{leaf}$} =
    \begin{cases}
        Y_\text{leaf}^{[k]\ang{j}} & \text{ if }
            \exists \; k \mid x_{1\text{, new}} = X_{1\text{, leaf}}\el{k\cdot}\\
        Y_\text{leaf}^{\ang{i}[k]} & \text{ if }
            \exists \; k \mid x_{2\text{, new}} = X_{2\text{, leaf}}\el{k\cdot}\\
        Y_\text{leaf}\mel{ij} & \text{ otherwise.}
    \end{cases}
    \label{eq:prototype2}
\end{equation}

A drawback of this approach, especially when working with very imbalanced interaction matrices and sufficiently small leaf partitions, is a possibly greater susceptibility to random fluctuations, since the label averages in the prediction step are taken over a much smaller sample size (a single row or column of $Y_\text{leaf}$ instead of the whole $Y_\text{leaf}$). Given we are predominantly working with similarity scores as sample attributes, we propose an intermediate approach: to weight the rows and columns of $Y_\text{leaf}$ by the similarity values in the form $w_{s1}\el i \equiv \text{similarity}(x_1\el i, \, X_1\el{i\cdot})$ between $x_\text{new}$ and the training samples in the leaf node (\autoref{eq:prototype3}).
% \begin{multline}
%     \KwPrototype{Y\textsubscript{leaf}} =\\
%     =\frac{
%         \sum_{i\in \text{leaf}}
%             s(x_1\el i, X_1\el{i\cdot})
%             Y_\text{leaf}^{[i]\ang j}
%     }{
%         2\sum_{i\in \text{leaf}}
%             s(x_1\el i, X_1\el{i\cdot})
%     }
%     +
%     \frac{
%         \sum_{j\in \text{leaf}}
%             s(x_2\el j, X_j\el{j\cdot})
%             Y_\text{leaf}^{\ang i[j]}
%     }{
%         2\sum_{j\in \text{leaf}}
%             s(x_2\el j, X_2\el{j\cdot})
%     }
%     \label{eq:prototype3}
% \end{multline}

\begin{equation}
    \KwPrototype{Y\textsubscript{leaf}}
    = \frac{
        \sum_{i\in \text{leaf}}
            w_{s1}\el i
            Y_\text{leaf}^{[i]\ang j}
    }{
        2\sum_{i\in \text{leaf}}
            w_{s1}\el i
    }
    +
    \frac{
        \sum_{j\in \text{leaf}}
            w_{s2}\el j
            Y_\text{leaf}^{\ang i[j]}
    }{
        2\sum_{j\in \text{leaf}}
            w_{s2}\el j
    }
    \label{eq:prototype3}
\end{equation}


%XXX TODO: fix name of the similarities (from future pedro: call it S?)

Since we are dealing with precomputed pairwise similarities, $X_1$ and $X_2$ are square matrices in which $X_a\el{i_1i_2} = \text{similarity}(X_a\el{i_1\cdot},\, X_a \el{i_2\cdot})$. We explore three different cases:

\begin{enumerate}
    \item $w_s = x\el i$
    \item $w_s = (x\el i )^2$
    \item $w_s = e^{x\el i}$
\end{enumerate}

% If the input values consist of precomputed pairwise similarities so that $X_1$ and $X_2$ are square matrices in which $X_a\el{i_1i_2} = s(X_a\el{i_1\cdot},\; X_a\el{i_2\cdot})$, \autoref{eq:prototype3} reduces to \autoref{eq:prototype4}.
% \begin{multline}
%     \KwPrototype{Y\textsubscript{leaf}} =\\
%     =\frac{
%         \sum_{i\in \text{leaf}}
%             x_1\el i
%             Y_\text{leaf}^{[i]\ang j}
%     }{
%         2\sum_{i\in \text{leaf}}
%             x_1\el i
%     }
%     +
%     \frac{
%         \sum_{j\in \text{leaf}}
%             x_2\el j
%             Y_\text{leaf}^{\ang i[j]}
%     }{
%         2\sum_{j\in \text{leaf}}
%             x_2\el j
%     }
%     \label{eq:prototype4}
% \end{multline}

%---------------------------------------

%When the traditional global adaptation $Y_\text{SGSO}$ is utilized (see Section \ref{sec:definitions}), a global version of $I_\text{MSE}(\cdot)$ can be used to achieve the same result in the corresponding bipartite partition $Y$.
%%
%\begin{multline*}
%        I_\text{MSE}(Y_\text{SGSO})
%            = Y_\text{SGSO}^2\mel{ij} - Y_\text{SGSO}\mel i^2\mel j
%            = (Y_\text{SGSO}^\intercal)^2\mel i - (Y_\text{SGSO}^\intercal)\mel i^2 =\\
%            = Y^2\mel{ij} - Y\mel{ij}^2
%\end{multline*}
%so that we can define $I_\text{GMSE}$ as in \autoref{eq:gso_mse}.
%\begin{equation}
%    I_\text{GMSE}(Y) \equiv I_\text{MSE}(Y_\text{SGSO})
%    = Y^2\mel{ij} - Y\mel{ij}^2
%    \label{eq:gso_mse}
%\end{equation}
%
%%and therefore the sorting procedure to search for split thresholds can be applied to the bipartite feature matrices $X_1$ and $X_2$ instead of $X_\text{SGSO}$, naturally avoiding the redundant values generated by \ref{eq:gsodata} but still generating the exatc same tree structure.
%
%%Since the sorting procedure itself often is the most computationally expensive step in decision tree growing, avoiding iteration through these repetitions drastically improves time and memory usage during training (see \ref{sec:complexity_analysis}).
%
%%Furthermore, many criteria utilized for split evaluation can be optimized for this setup. Take for instance the split quality calculation presented by \ref{eq:quality}. Its last term can be rewritten as in \ref{eq:q_last_term}.
%In such global scenario, the quality criteria can be rewritten as in \autoref{eq:q_optimization}.
%%
%\begin{multline}
%    %&\frac{n_l I(Y_l)+ n_rI(Y_r)}{n_l+n_r} =\\
%    %\frac{n_l \avg{Y_l^2}-n_l\avg{Y_l}^2+ n_r\avg{Y_r^2}-n_r\avg{Y_r}^2}{n_l+n_r} =\\
%    %&=\frac{n_l \avg{Y_l^2}+ n_r\avg{Y_r^2}}{n_l+n_r}-\frac{n_l\avg{Y_l}^2+n_r\avg{Y_r}^2}{n_l+n_r} =\\
%    %&=\avg{Y^2}-\frac{n_l\avg{Y_l}^2+n_r\avg{Y_r}^2}{n_l+n_r}
%    Q_\text{GMSE}(Y, t, f)
%        =\\
%            %=\frac{|Y_\text{node}|}{|Y_\text{root}|}
%            % \left[
%            %    1
%            %    - \frac{|Y_l| Y_l^2\mel{ij} + |Y_r|Y^2\mel{ij}}
%            %        {|Y_\text{node}|I(Y_\text{node})}
%            %    + \frac{
%            %        |Y_l|Y_l\mel{ij}^2 + |Y_r|Y_r\mel{ij}^2
%            %    }
%            %        {|Y_\text{node}|I(Y_\text{node})}
%            %\right]\\
%            =\frac{|Y_\text{node}|}{|Y_\text{root}|I(Y_\text{node})}
%            \left(
%                \frac{
%                    |Y_l|Y_l^{\ang{ij}2} + |Y_r|Y_r^{\ang{ij}2}
%                }
%                    {|Y_\text{node}|}
%            - Y_\text{node}^{\ang{ij}2}
%            \right)
%    \label{eq:q_optimization}
%\end{multline}
%where we used that
%\begin{multline*}
%            1-\frac{|Y_l| Y_l^{2\ang{ij}} + |Y_r|Y_r^{2\ang{ij}}}
%                {|Y_\text{node}|I(Y_\text{node})}
%            =1-\frac{\sum_i \sum_j Y_\text{node}^{\ang{ij}2}}
%                {|Y_\text{node}|I(Y_\text{node})}=\\
%            =1-\frac{Y_\text{node}^{2\ang{ij}}}
%                {I(Y_\text{node})}
%            =1-\frac{Y_\text{node}^{2\ang{ij}}}
%                {Y_\text{node}^{2\ang{ij}}-Y_\text{node}^{\ang{ij}2}}=\\
%            =\frac{-Y_\text{node}^{\ang{ij}2}}
%                {Y_\text{node}^{2\ang{ij}}-Y_\text{node}^{\ang{ij}2}}
%            =-\frac{Y_\text{node}^{\ang{ij}2}}
%                {I(Y_\text{node})}
%\end{multline*}

%So that we can also rewrite $Q$ as shown by \ref{eq:q_proxies}.
%
%\begin{align}
%    \begin{split}
%        &Q(Y, t, f) = \\
%        &=\frac{1}{I(Y)} \left(I(Y)-\avg{Y^2}+\frac{n_l\avg{Y_l}^2+n_r\avg{Y_r}^2}{n_l+n_r}\right)=\\
%        &=\frac{1}{I(Y)} \left(\avg{Y}^2+\frac{n_l\avg{Y_l}^2+n_r\avg{Y_r}^2}{n_l+n_r}\right)
%    \end{split}
%    \label{eq:q_proxies}
%\end{align}

%Having only global averages in \autoref{eq:gso_mse}, i.e. always involving both $i$ and $j$ indices simultaneously, enables us to pre-compute averages of each row and column of $Y_\text{node}$, iterating over one-dimensional $\tilde Y_\text{node}$ proxies (\autoref{eq:y_proxies}) instead of the bi-dimensional matrix when searching for the best split. This property can be explored to build a more efficient training procedure for bipartite GSO decision trees in comparison to the naive approach (Section \ref{}), as discussed in the Section \ref{sec:bipartite_trees} and demonstrated in the asymptotic complexity analysis developed in Section \ref{sec:complexity_analysis}.
%%
%\begin{equation}
%    \begin{split}
%        \tilde Y_1\el{i} = Y\el{i}\mel{j}\\
%        \tilde Y_2\el{j} = Y\mel{i}\el{j}
%        %\tilde Y_1\el{i} = \savg{Y\el{i,j}}_j\el{i}\\
%        %\tilde Y_2\el{j} = \savg{Y\el{i,j}}_i\el{j}
%    \end{split}
%    \label{eq:y_proxies}
%\end{equation}
%
%Dealing directly with bipartite data, another idea would be to take inspiration from the LMO strategy (see Section \ref{}) and define the quality of a node partition as $\frac{1}{2}[Q(Y_\text{node}, t, f)+Q(Y\T_\text{node}, t, f)]$, the simple average between both directions. However, for a horizontal split, the impurity improvement on the columns axis is null for any impurity metric consisting of a simple average of impurities of each output, i.e. $I_\text{total}(Y)=(I(Y\el{ij})\el j)\mel j$, as briefly shown by \autoref{eq:q_T_is_zero} (which uses \autoref{eq:quality}). As a consequence, the split search procedure with such impurities on a bipartite dataset is essentially local, considering different outputs in only a single axis at a time.
%\begin{multline}
%    % I_\text{total}(Y) = I(Y\el{ij})\mel j\implies\\
%    I_\text{total}(Y\T) = I(Y\el{ji})\mel i\implies\\
%    \implies |Y_l|_i I(Y_l\T) + |Y_r|_i I(Y_r\T)=\\
%    = \sum_i I(Y_l\el{ji})\el i + \sum_i I(Y_r\el{ji})\el i %=\\
%    = \sum_i I(Y\el{ji})\el i\implies\\
%    \implies \frac{|Y_l|_i I(Y_l\T) + |Y_r|_i I(Y_r\T)}{|Y|_i}=I_\text{total}(Y)\\
%    %\implies\\
%    \implies Q(Y\T, t, f) = 0 \;\square
%    \label{eq:q_T_is_zero}
%\end{multline}
%
%This result is valid for the majority of multioutput decision tree implementations \cite{}, and leads us to simply define the LMO quality of a split on bipartite data as in \autoref{eq:q_lmo}, where $f$ being a row feature means it represents a column of $X_1$ and, as such, a horizontal split. Otherwise, $f$ is a column feature and designates a column of $X_2$, imposing a split in the vertical axis.
%\begin{equation}
%    Q_\text{LMO}(Y, t, f) =
%    \begin{cases}
%        Q(Y, t, f)&\text{ if $f$ is a row feature}\\
%        Q(Y\T, t, f)&\text{ if $f$ is a column feature}
%    \end{cases}
%    \label{eq:q_lmo}
%\end{equation}
%
%% (Specific demonstration for MSE)
%%
%% The majority of popular impurity metrics fall under this category \cite{}, and we now briefly demonstrate this result for the MSE.
%% When applied to the original $I_\text{MSE}$ of \autoref{eq:mse}, the same manipulation from above results in \autoref{eq:q_mse}.
%% %The definition of $I_\text{MSE}$ also results in $Q_\text{bipartite}$
%% \begin{multline}
%%     Q_\text{MSE}(Y, t, f) =\\
%%     =\frac{|Y_\text{node}|}{|Y_\text{root}|I(Y_\text{node})}
%%     \left(
%%         \frac{
%%             |Y_l|Y_l^{\ang{i}2\ang j} + |Y_r|Y_r^{\ang{i}2\ang j}
%%         }
%%             {|Y_\text{node}|}
%%     -Y_\text{node}^{\ang{i}2\ang j}
%%     \right)
%%     \label{eq:q_mse}
%% \end{multline}
%% 
%% The first term of \autoref{eq:q_mse}, now considering the transposed $Y\T$, can then be simplified as shown by \autoref{eq:q_bip_is_local}.
%% \begin{multline}
%%     \frac{
%%         |Y_l|Y_l^{\ang{j}2\ang i} + |Y_r|Y_r^{\ang{j}2\ang i}
%%     }{
%%         |Y_\text{node}|
%%     }=\\
%%     = \frac{
%%         |Y_l|_i|Y_l|_jY_l^{\ang{j}2\ang i}
%%         + |Y_r|_i|Y_r|_j Y_r^{\ang{j}2\ang i}
%%     }{
%%         |Y_\text{node}|_i|Y_\text{node}|_j
%%     }=\\
%%     = \frac{
%%         |Y_l|_j \sum_i Y_l^{[i]\ang{j}2}
%%         + |Y_r|_j \sum_i Y_r^{[i]\ang{j}2}
%%     }{
%%         |Y_\text{node}|_i|Y_\text{node}|_j
%%     }=\\
%%     = \frac{
%%         |Y_\text{node}|_j (
%%             \sum_i Y_l^{[i]\ang{j}2}
%%             + \sum_i Y_r^{[i]\ang{j}2}
%%         )
%%     }{
%%         |Y_\text{node}|_i|Y_\text{node}|_j
%%     }=\\
%%     = \frac{
%%             \sum_i Y_\text{node}^{[i]\ang{j}2}
%%     }{
%%         |Y_\text{node}|_i
%%     }
%%     = Y_\text{node}^{\ang{j}2\ang{i}}
%%     \label{eq:q_bip_is_local}
%% \end{multline}
%% 
%% Which finally results in \autoref{eq:q_T_is_const}.
%% \begin{multline}
%%     Q_\text{MSE}(Y\T, t, f) =\\
%%     =
%%     \frac{|Y_\text{node}|}{|Y_\text{root}|I(Y\T_\text{node})}
%%     (
%%         Y_\text{node}^{\ang{j}2\ang{i}}
%%         -
%%         Y_\text{node}^{\ang{j}2\ang{i}}
%%     )=\\
%%     =
%%     0    
%%     \label{eq:q_T_is_const}
%% \end{multline}


\subsection{Asymptotic complexity analysis}
\label{sec:complexity_analysis}

% \begin{equation}  % SLMO
%     %O(\ref{alg:find_bipartite_split})
%     = O(\tilde n_{f1} |Y|_i|(\log |Y|_i + |Y|_j))
%     = O(\tilde n_{f1} |Y|_i| |Y|_j \log |Y|_i)
%     = O(\tilde n_{f1} n^2 \log n)
%     = O(n^3 \log n)
%     \label{eq:O_slmo}
% \end{equation}

%TODO cite known complexities
From the algorithm description, one can infer that \ref{alg:find_best_split}'s complexity will be given by
%
\begin{align}
    %O(\ref{alg:find_best_split})
    O(\FindSplitBest)
    %= O(\tilde n_f S(|Y|_i) + \tilde n_f |Y|_i|Y|_j)
    %= O(\tilde n_f |Y|_i \log |Y|_i + \tilde n_f |Y|_i|Y|_j)
    %= O(\tilde n_f |Y|_i|Y|_j)
    &= O(\tilde n_f S(|Y|_i) + \tilde n_f |Y|_i|Y|_j)
    =\nonumber\\
    &= O(\tilde n_f |Y|_i \log |Y|_i + \tilde n_f |Y|_i|Y|_j)
    =\nonumber\\
    &= O(\tilde n_f |Y|_i (\log |Y|_i + |Y|_j))
    %= O(\tilde n_f |Y|_i|Y|_j)
    \label{eq:O_find_best_split}
\end{align}
%where we assume $|Y|_i \approx |Y|_j$ and $S(n)$ is the complexity of the chosen sorting algorithm of \atoref{} when operating on $n$ values.
where $\tilde n_f$ is the number of features being considered in each node and $S(n)$ is the complexity of the chosen sorting algorithm of \atoref{} when operating on $n$ values. $\tilde n_f$ is a hyperparameter of random forests (\autoref{sec:bipartite_forests}) usually defined as a function of the total number of features $|X|_j$. We further employ $\tilde n_{f1}$ and $\tilde n_{f2}$ to refer to the number of features to be selected from $X_1$ and $X_2$, respectively, in each node.

The most effective sorting algorithms currently known for real-valued data, such as Quick Sort or Merge Sort~\cite{}, have $O(n \log n)$ asymptotic complexity. However, since the sorting of multiple subsets of the same $X$ values will be performed, it is often effective to spend $O(|X|_j|X|_i\log |X|_i)$ time previously obtaining ranks for each column of $X$ and for each axis, so that subsequent partition sorting steps can be performed in linear time with integer-specific algorithms such as Radix Sort \cite{}.
%
That said, considering $S(n) = O(n \log n)$ does not affect $O(\ref{alg:find_best_split})$ under the assumption $|Y|_j >> \log |Y|_i$, in which case the second term in the result of \autoref{eq:O_find_best_split} dominates in the asymptotic regime.

%ours and the majority of other implementations do not perform this pre-sorting step.% In any case, ...%TODO
%That said, we hereafter consider $S(n) = O(n)$, and the pre-sorting term of the complexity will be disregarded in favor of the asymptotically dominant tree-building complexity described ahead.

When applied on $X_\text{SGSO}$ and $Y_\text{SGSO}$ as in the naive GSO approach, we have \autoref{eq:O_sgso}.
%
\begin{align}
    %O(\ref{alg:find_best_split})
    O(\FindSplitBest)
    &= O(
        (\tilde n_{f1} + \tilde n_{f2})
        |Y_\text{SGSO}|_i
        (\log |Y_\text{SGSO}|_i + |Y_\text{SGSO}|_j)
    )
    =\nonumber\\
    %= O(\tilde n_f |Y_\text{SGSO}|_i)
    &= O((\tilde n_{f1} + \tilde n_{f2}) |Y_\text{SGSO}|_i|\log Y_\text{SGSO}|_i)
    =\nonumber\\
    &= O((\tilde n_{f1} + \tilde n_{f2}) |Y|_i|Y|_j (\log |Y|_i + \log |Y|_j))
    \label{eq:O_sgso}
\end{align}

For \ref{alg:find_bipartite_split} employing the GMO approach (the original PBCT procedure~\cite{pliakos2018global}), the time complexity is given by \autoref{eq:O_gmo}, which renders it equivalent to the SGSO strategy. $\tilde n_{f1}$ and $\tilde n_{f2}$ stand for the numbers of row and column features to be drawn at each node, respectively.
%
\begin{align}
    O(\FindSplitBest_\text{GMO})
    = O(
        \tilde n_{f1} |Y|_i|(\log |Y|_i + |Y|_j)
        + \tilde n_{f2} |Y|_j|(\log |Y|_j + |Y|_i)
    )
    =\nonumber\\
    = O((\tilde n_{f1} + \tilde n_{f2}) |Y|_i||Y|_j)
    \label{eq:O_gmo}
\end{align}

When considering the BGSO approach however, $\tilde Y_1$ and $\tilde Y_2$ column vectors are used instead of $Y$, effectively eliminating the \texttt{for} loops in \autoref{algline:gmo_loop1} and \autoref{algline:gmo_loop2} of \autoref{alg:find_best_split}. Considering that building the $\tilde Y$ proxies takes $O(|Y|_i|Y|_j)$, the split search procedure in this strategy has its complexity described by \autoref{eq:O_bgso}.
%
\begin{align}
    %O(\ref{alg:find_bipartite_split}_\text{BGSO})
    O(\FindSplitBest_\text{GSO})
    = O(
        |Y|_i|Y|_j
        + \tilde n_{f1} |\tilde Y_1|_i \log |\tilde Y_1|_i
        + \tilde n_{f2} |\tilde Y_2|_i \log |\tilde Y_2|_i
    )
    =\nonumber\\
    = O(
        |Y|_i|Y|_j
        + \tilde n_{f1} |Y|_i \log |Y|_i
        + \tilde n_{f2} |Y|_j \log |Y|_j
    )
    \label{eq:O_bgso}
\end{align}

For the whole tree-building process, considering $|Y|_i \propto |Y|_j = n$ and given any constant $k \in \mathbb{N}$ so that $O(\text{FindSplit})=n^k$ % \log n$,
we analyze the best case of a balanced decision tree, where the number of samples in each node ($|Y| = n^2$) halves with each level added:
%
\begin{equation}
    |Y_\text{child node}| \approx |Y_\text{parent node}/2| = n_\text{parent node}^2/2
\end{equation}

As a consequence, the number of nodes in the $(l+1)$-th level is given by $2^l$, while the number of samples to process at each node is estimated by $n^2 / 2^l$ and the total number of levels is given by $L = \log_2 n^2 = 2 \log_2 n$.
%
%The time $T(n)$ to continue building the tree from a given level then obeys the recurrence relation
%
% \begin{equation}
%     T(n) = 2T(n/\sqrt 2) + O(\text{FindSplit}(n))
% \end{equation}

%\autoref{eq:O_tree} demonstrates the expected overall training complexity for the considered \FindSplit functions, as given by the Master Theorem~\cite{}. Table \ref{tab:O_comparison} summarizes the last results for the different cases.
%
%\begin{multline}  % With log
%    %O(\ref{alg:buildtree})
%    O(alg:buildtree)
%    %%= O\left(\sum_{l=0}^{L} 2^l \text{FindSplit}(n/\sqrt{2}^l)\right)
%    = O\left(\sum_{l=0}^{L} 2^l \text{FindSplit}(n^2/2^l)\right)
%    =\\
%    %= O\left(\sum_{l=0}^{L} 2^l n^{2k}/2^{kl} n^{2k}/2^{kl}
%    = O\left(\sum_{l=0}^{L} 2^l n^{2k}/2^{kl} \log (n^{2k}/2^{kl})\right)
%    %= O\left(\sum_{l=0}^{L} 2^l n^{2k}/2^{kl} (2k\log n - kl)\right)
%    = O\left(\sum_{l=0}^{L} 2^{l(1-k)} k n^{2k} (2\log n - l)\right)
%    = O\left(n^{2k} \sum_{l=0}^{L} 2^{l(1-k)} (L - l)\right)
%    %%= O\left(\sum_{l=0}^{L} 2^l n^k/2^{kl/2}\right)
%    %%= O\left(n^k \sum_{l=0}^{L} 1 /2^{(k/2-1)l}\right)
%    \\
%    =
%    \begin{cases}
%        O(n^{2(k+1)}) & \text{if}\; k < 1\\
%        O(n^2\log n) & \text{if}\; k = 1\\
%        O(n^{2k}) & \text{if}\; k > 1\\
%        %O(n^{2k})
%        %   = O(\frac{n^{2k}}{1-2^{1-k}} & \text{if}\; k > 1\\
%        %O(n^2)
%        %   = O(n^{2k} 2^{L} = n^{2(k+1)} & \text{if}\; k < 1\\
%    \end{cases}
%    \label{eq:O_tree_log}
%\end{multline}

\begin{multline}  %without log
    \label{eq:O_tree}
    %O(\ref{alg:buildtree})
    O(alg:buildtree)
    %%= O\left(\sum_{l=0}^{L} 2^l \text{FindSplit}(n/\sqrt{2}^l)\right)
    = O\left(\sum_{l=0}^{L-1} 2^l \text{FindSplit}(n^2/2^l)\right)
    = O\left(\sum_{l=0}^{L-1} 2^l n^{2k}/2^{kl}\right)
    = O\left(n^{2k} \sum_{l=0}^{L-1} 2^{l(1-k)}\right)
    \\
    =
    \begin{cases}
        O(n^{2(k+1)}) & \text{if}\; k < 1\\
        O(n^2\log n) & \text{if}\; k = 1\\
        O(n^{2k}) & \text{if}\; k > 1\\
        %O(n^{2k})
        %   = O(\frac{n^{2k}}{1-2^{1-k}} & \text{if}\; k > 1\\
        %O(n^2)
        %   = O(n^{2k} 2^{L} = n^{2(k+1)} & \text{if}\; k < 1\\
    \end{cases}
\end{multline}
%We can observe that, if the pre-sorting step is utilized, no asymptotic complexity improvement is expected for the GMO approach in comparison to the original naive GSO. Insofar, to the extent of our knowledge this essential step is not mentioned in previous works, so that we here reaffirm its importance and encourage future studies to take similar preprocessing procedures into more attentive consideration.

We can observe that our proposed optimization for the GSO strategy reduces the tree-building complexity by a factor of
%
\begin{equation}
    \frac{O(\text{BGSO})}{O(\text{GMO})}
        = \frac{\log n + \tilde f} {\tilde f \log n}
        = \frac{1}{\tilde f} + \frac{1}{\log n}
    \label{eq:O_reduction}
\end{equation}
%
in comparison to GMO, a major improvement especially for datasets with a large number of features. Remarkably, when kernel or similarity matrices are used, as the datasets explored in this work do, or in general when $\tilde f \propto n$, BGSO is $\log n$ times faster than GMO in the asymptotic regime, yielding unseen scalability of decision trees for interaction prediction problems.

Regarding the performance of \ref{alg:find_random_split} as a substitute for \ref{alg:find_best_split}, although considerable amounts of operations are saved, the procedure for generating a split point for each feature column still requires finding minimum and maximum values, occurring in the same linear time complexity as the greedy approach.
%
As a result, the asymptotic computational time needed by the algorithm is expected to grow with no different ratio relative to the number of samples, not resulting in any improvements in complexity relative to \ref{alg:find_best_split}.

\begin{table*}[h]
    \centering
    \begin{tabular}{c|c|c|c|c}
        Strategy
        & Split search
        & If $Y$ is square
        & Tree building
        & $\tilde n_f \propto n$
        \\
        \hline \hline
        %SLMO
        %    & $O(\tilde n_{f1} n_1(\log n_1 + n_2))$
        %    %& $O((\tilde n_{f1} + \tilde n_{f2}) n_1 n_2)$
        %    & $O(\tilde n_f n^2 \log n)$
        %    & $O(\tilde n_f n^2 \log n)$
        %    & $O(n^3\log n)$
        %\\
        SGSO
            & $O((\tilde n_{f1} + \tilde n_{f2}) n_1 n_2 (\log n_1 + \log n_2))$
            & $O(\tilde n_f n^2 \log n)$
            & $O(\tilde n_f n^2 \log n)$
            & $O(n^3\log n)$
        \\
        GMO
            & $O(
                \tilde n_{f1} n_1|(\log n_1 + n_2)
                + \tilde n_{f2} n_2|(\log n_2 + n_1)
            )$
            %& $O((\tilde n_{f1} + \tilde n_{f2}) n_1 n_2)$
            & $O(\tilde n_f n^2)$
            & $O(\tilde n_f n^2 \log n)$
            & $O(n^3\log n)$
        \\
        BGSO
            & $O(
                n_1 n_2
                + \tilde n_{f1} n_1 \log n_1
                + \tilde n_{f2} n_2 \log n_2
            )
            $
            & $O(n^2 + \tilde n_f n \log n)$
            % & $O(n^2 \log n + \tilde n_f n^2)$
            & $O(\tilde n_f n^2)$
            & $O(n^3)$
        \\
        \hline
    \end{tabular}
    \caption{
        Comparison between asymptotic time complexities of bipartite decision tree-building procedures.
        $n_1$ and $n_2$ respectively designate the number of samples in $X_1$ and $X_2$, while $n$ is defined so that $n_1 \propto n_2 \propto n$.
        Similarly, $\tilde n_{f1}$ and $\tilde n_{f2}$ represent the number of features from each axis to be considered for split search in each node,
        %($\tilde n_{f1}=|X_1|_j$ and $\tilde n_{f2}=|X_2|_j$)
        while $\tilde n_f$ is used to illustrate the cases where $\tilde n_{f1} \propto \tilde n_{f2} \propto \tilde n_f$.
        %
        The last column refers to the case where the number of features considered in each node is proportional to the number of samples in each axis ($\tide n_f \propto n$). This scenario could arise, for instance, if one is dealing with pairwise similarities or kernel matrices as the model's input data.
    }
    \label{tab:O_comparison}
\end{table*}


%%%%%%%%%%%%%
%The equation \ref{eq:q_optimization} allows the inner loop o\tilde f the \ref{alg:find_best_split} algorithm to run in linear time with respect to $|X_\text{SGSO}|_i$, the number of rows in the provided feature matrix $X_\text{SGSO}$, by calculating $\savg{(Y_\text{SGSO}\el{i})^2}$ and $\savg{Y_\text{SGSO}\el{i}}$ beforehand. Additionally denoting by $S(n)$ the complexity of the sorting procedure ($S(n) = \Omega(n)$) and noticing that the initial average calculation is $O(|X_\text{SGSO}|_i)$, we have the Equation\ref{eq:O_find_best_split}.
%The equation \ref{eq:q_optimization} allows the inner loop of the \ref{alg:find_best_split} algorithm to run in linear time with respect to $|X_\text{SGSO}|_i$, the number of rows in the provided feature matrix $X_\text{SGSO}$, by calculating $\savg{(Y_\text{SGSO}\el{i})^2}$ and $\savg{Y_\text{SGSO}\el{i}}$ beforehand. Additionally denoting by $S(n)$ the complexity of the sorting procedure ($S(n) = \Omega(n)$) and noticing that the initial average calculation is $O(|X_\text{SGSO}|_i)$, we have the Equation\ref{eq:O_find_best_split}.
%
%\begin{multline}
%    O(\text{\ref{alg:find_best_split}}(|X_\text{SGSO}|_i,\tilde \,|X_\text{SGSO}|_j)) = \\
%    = O(|X_\text{SGSO}|_i+|X_\text{SGSO}|_i|X_\text{SGSO}|_j+|X_\text{SGSO}|_jS(|X_\text{SGSO}|_i)) =\\
%    = O(|X_\text{SGSO}|_jS(|X_\text{SGSO}|_i))
%    = O(|X_\text{SGSO}|_jS(|X_\text{SGSO}|_i))
%\end{multlinen_{f1} %
%\end{multlinen_{f1} %
%Thus, on our n_{\log |Y|_i f2}ttings, applying .. \log |Y|_j.
%
%[GMO] 
n_{f1} %\begin{multline}
%    O(\\log |Y|_i text{n_{f2}\ \log |Y|_j\
%    =O(|X_1|_j(S(|X_1|_i) + |X_1|_i|X_2|_i) + |X_2|_j(S(|X_2|_i) + |X_1|_i|X_2|_i)\\
%    =O(|X_1|_j(|X_1|_i|X_2|_i) + |X_2|_j(|X_1|_i|X_2|_i)\\
%    =O(|X_\text{SGSO}|_j|X_\text{SGSO}|_i)
%    \label{eq:O_find_bipartite_split}
%\end{multline}
%
%On the other side, \ref{alg:find_bipartite_split} calculates $\tilde Y_1$ and $\tilde Y_2$ ($O(|X_1|_i|X_2|_i) = O(|X_\text{SGSO}|_i)$, see \autoref{eq:y_proxies}) and applies \ref{alg:find_best_split} separately to each instance group in the bipartite dataset, so that its complexity is given by \ref{eq:O_find_bipartite_split}.
%
%\begin{multline}
%    O(\text{\ref{alg:find_bipartite_split}}) =\\
%    =O(|X_\text{SGSO}|_i + |X_1|_jS(|X_1|_i) + |X_2|_jS(|X_2|_i))
%    \label{eq:O_find_bipartite_split}
%\end{multline}
%
%Assuming $|X_1|_i \propto |X_2|_i \propto n_s$, we can write Equation \ref{eq:O_ns}.
%
%\begin{equation}
%    \begin{split}
%        O(\text{\ref{alg:find_best_split}}) = O(|X_\text{SGSO}|_j S(n_s^2))\\
%        O(\text{\ref{alg:find_bipartite_split}}) = O(n_s^2 + |X_\text{SGSO}|_j S(n_s))\\
%    \end{split}
%    \label{eq:O_ns}
%\end{equation}
%
%And finally, we expect to reduce computation time by at least a whole order of magnitude with respect to the total number of samples in a bipartite dataset when using \ref{alg:find_bipartite_split} in comparison to the GSO approach using solely \ref{alg:find_best_split} (Equation \ref{eq:complexity_reduction}).
%
%\begin{equation}
%    \Omega\left(\frac{\text{\ref{alg:find_best_split}}}
%                {\text{\ref{alg:find_bipartite_split}}}\right) =
%     \Omega\left(\frac{S(n_s^2)}{S(n_s)}\right) =
%     \Omega(n_s)
%     \label{eq:complexity_reduction}
%\end{equation}
%%
%Where again we use the fact that no sorting algorithm better than $O(n)$ exists ($S(n) = \Omega(n)$).


\subsection{Decision forests}
\label{sec:decision forests}

%Although valuable tools to understand the learning problem in hands, sole decision trees are often not enough to effectively model complex relationships among the data, lacking generalization power and being prone to overfitting~\cite{}. % under or overfitting?
%Their most powerful prediction capabilities are unleashed when committees of such models are employed 
%Their most valuable contribution to learning performance capabilities are observed when committees of such models are employed instead.
While single decision trees are valuable tools for comprehending the learning problem at hand, they often fall short in effectively modeling intricate relationships within the data, displaying limited generalization capabilities and high susceptibility to overfitting~\cite{}.
%
Their most significant impact on machine learning applications is observed when committees of such models are utilized instead, in which the final output values are most commonly computed by averaging or summing the predictions of all trees.

These compositions of estimators are usually referred to as \emph{ensembles}. Being studied in the context of machine learning since the 1970s, they are based on the intuitive idea that combining multiple opinions from a diverse set of experts frequently results in better decision taking. % TODO elaborate, jury theorem

%A longstanding idea is that combining predictions of multiple estimators yields better results, much like averaging opinions of several people aids in decision taking \cite{}.

In fact, it has been extensively demonstrated both empirically and theoretically that the predictive perfomance a group of learners always surpasses that of its individual components if and only if the individual estimators are sufficiently accurate and diverse~\cite{dieterich2000}.% TODO cite more
Importantly, requiring diversity means that the individual estimators must ideally commit errors on different instances for the composition to succeed~\cite{polikar2006ensemble}.

\textcite{dietterich2000ensemble,polikar2006ensemble} describe three ways in which combining diverse estimators could benefit the ensemble's performance.

\begin{enumerate}
    \item \textbf{Statistical:} Building each estimator can be seen as finding a hipothesis that explains the prediction problem as well as possible. If multiple different hypotheses are found, the likelihood that at least one of them is close to the true underlying function is increased. Furthermore, averaging multiple hypotheses can contribute to alleviate the influence of each hipothesis' variance, resulting in a more accurate approximation and reduced propensity to overfitting.
    \item \textbf{Computational:} Finding the globally optimal decision tree or neural network is known tobe an NP-complete problem~\cite{}. Thus, finding and combining multiple approximate hypotheses, that may represent local maxima of the objetive function, is often much cheaper than expending time on the search for a global solution. % TODO
    \item \textbf{Representational:} Sometimes, a single estimator is not complex enough to represent the intricacies of the probem at hand. However, combination of models can be utilized to enhance the representational power of the ensemble, building a more general decision function as a combination of the simpler decision boundaries of each estimator.  % TODO figure and example with stumps (rokach2018)
\end{enumerate}

The balance between diversity 
and individual strength is thus of central role when designing ensemble models. Both these properties act in the direction of increasing the final ensemble performance, so that mechanisms of introducing model heterogeneity, and not only strategies for boosting individual gains, must be carefully considered.

Even if hampering single component performance to some extent, promoting diversity between components, so to reduce as far as possible the correlation between their outputs, is often proved to result in performance improvements for the whole ensemble~\cite{breiman2001}.  % TODO cite more
%
This is the core idea of decision forests. Even simple estimators such as decision trees, traditionally prone to overfitting, can be leveraged to compose powerful ensemble models, generically called \emph{decision forests}, as long as proper diversification is employed.
%
We briefly describe some popular strategies for introducing heterogeneity in the decision tree growing procedure.

\begin{enumerate}  % XXX
    \item \textbf{Instance sampling:} A new version of the training set is built by randomly drawing instances from it, with or without replacement (selecting rows of $X$). If drawing with replacement, one can optionally draw the same number of instances as the original set, a procedure known as bootstrapping.
    \item \textbf{Feature sampling:} A random subset of features is selected to be used (selecting columns of $X$). The number of features $\tilde n_f$ selected in each node is commonly defined as a function of the total number of features $|X|_j$. Usual choices are $\tilde n_f = \lceil \sqrt{|X|_j} \rceil$ or $\tilde n_f = \lceil \log |X|_j \rceil$.
    \item \textbf{Split threshold randomization:} Instead of searching for the best split threshold for each feature as in \autoref{alg:find_split_best}, a random threshold value is drawn between the minimum and maximum values of each feature column.
    %\item Feature transformation (rotation forest, oblique trees)
\end{enumerate}

Both instance and feature undersampling can be performed node-wise, occurring before the split search procedure of each node, or tree-wise, occurring once for each tree in the ensemble. Both can also be performed with or without replacement, but notice that sampling features with replacement only makes sense if randomization of split threshold is also employed. In such a case, a random candidate split threshold is selected for each repetition of a feature, whereas the same split threshold would be selected for all duplicates if the greedy approach was used, spending more time with no different result than if omitting the repeated features.

The concept of sampling instances with replacement to create a different training set for each estimator in the ensemble was first proposed by \textcite{breiman1996bagging} under the name of bootstrap aggregation or \emph{bagging}.

The idea of selecting a subset of features was introduced independently by \textcite{amit1997shape} and \textcite{tinkamho1998random}. While \textcite{amit1997shape} explored feature sampling as a remedy for a shape recognition problem with impeditively large sample sets, \textcite{tinkamho1998random} was mainly focused on overfitting-prevention for decision forests.

It was \textcite{breiman2001random} who first combined the two ideas, proposing and greatly popularizing the Random Forest algorithm, with over 115 thousand citations according to Google Scholar as of september 2023.

\textcite{geurts2006extremely} later introduced the randomized split threshold concept, presenting the Extremely Randomized Trees algorithm (Extra-Trees) and showcasing its competitive prediction scores and clear superiority in terms of training speed compared to bagging and Random Forests.

The two main ensemble-building strategies we explore in the current work can now be defined as follows:

\begin{itemize}
    \item \textbf{Random Forests (RF):} Tree-wise instance sampling with replacement and node-wise feature undersampling without replacement are employed.
    \item \textbf{Extremely Randomized Trees (ERT):} Split threshold randomization and node-wise feature undersampling without replacement are employed. No resampling of instances is performed in the original proposal.
\end{itemize}

One of the notable features of decision forests that contribute to their expressive popularity is their low need for data preprocessing, since the order of the values among each feature is the main factor for determining the best split points, and not their scale. Additonally, the relative scaling of different features is not a concern, since the split search procedure is performed separately for each feature. These factors, combined with their strikingly small set of hyperparameters, crown decision forests as remarkable "plug-and-play", easily configurable, models, especially suited for unstructured tabular data~\cite{breiman2001}. % XXX cite more

We refer the reader to \textcite{sagi2018ensemble} and \textcite{fawagreh2014random} for a more in-depth description of prominent decision forest strategies and previous work in the field. \textcite{amasyali2011comparison} provides an experimental comparison of the most popular ensemble methods.  % XXX more (preferably newer) experimental comparisons

Regarding interaction problems, \textcite{schrynemackers2015classifying} explores the use of decision forests under the standard global multi-output and local single-output adaptations presented by \autoref{sec:standard adaptations}.

However, the same tree-diversification and forest building techniques discussed in this section can also be applied to bipartite decision trees, with very small modifications regarding the data sampling procedures: instance sampling and feature sampling must now occurr on both domains of the interaction dataset. Under this assumption, the adptation to bipartite problems can thus occur at the tree level, not at the ensemble level as would be the case with SLMO and SGSO strategies.

\cite{pliakos2019network} explores these ideas, using bipartite global multi-output decision trees to build both Random Forests and Extra-Trees ensembles. We hereafter refer to these forests as Bipartite Random Forests (BRF) and Bipartite Extra-Trees (BXT), respectively. Their study suggests superior performance of BXT in comparison to SLMO- and SGSO-adapted forests, as well as in comparison to previously proposed algorithms, although no significance level or test statistic is used to describe the comparisons.

The authors later extended their work to include a self-learning step for predicting drug-target interactions~\cite{pliakos2020drugtarget}. In that study, their GMO BXT model is not directly built upon the bipartite dataset but rather on a reconstructed version of the interaction matrix obtained through applying the Neighborhood-Regularized Logistic Matrix Factorization technique (NRLMF, \textcite{liu2016neighborhood}).

% TODO how to combine tree outputs
In the present work, we refine such comparisons and inroduce ensembles of our newly proposed bipartite global single-output decision trees (\autoref{sec:bgso_trees}), demonstrating competitive prediction scores under an asymptotically more efficient training framework.

% balance, we can impove perf by randomizing, but not too much to impact perf

%In fact, it is well demonstrated that the generalization error for a group of weak learners asymptotically decreases with a higher number of individual estimators \cite{}. Due to their simplicity and transparency, decision trees are frequently chosen as individual learners to compose an ensemble of estimators.

% ===================

%Many strategies are possible to combine predictions of multiple models in a ensemble, one of the simplest of them being a majority voting system. With this approach, each individual estimator's prediction is considered a vote on the class to be outputted and the most voted class is regarded as the final prediction \cite{}. Nevertheless, several other methods have been explored, namely weighted voting \cite{} and {} \cite{Fawagreh_2014}. In regression tasks, the individual predictions could be simply combined by taking the average output value of them as the whole ensemble final guess \cite{}.

%An important result by \cite{Breiman_2001} was that the strength of the total ensemble model not only depends on the strength of the individual estimators but also on the correlation between them, so that reducing correlation between the individual components increases performance overall. Multiple ideas were then developed to generate a set of uncorrelated estimators to be further combined. \cite{} proposes to grow each tree on a bootstrap set of samples data, in which a predefined number of samples are drawn with replacement from the original dataset, so that they are equally distributed to the total samples. The number of samples drawn usually equals the total number of samples, making each bootstrap set also the same size as the original set. Since samples are chosen with replacement, each set lacks about a third of the original input data \ref{}, yielding thus distinct trees unaware of the whole dataset. This procedure is currently know as \emph{bagging}, and was proposed by \ref{}.

%Taking another step in reducing individual trees correlation, \cite{Breiman_2001} proposed to, besides bootstrapping samples ($X_a$'s rows) before creating a new tree, also to subsample $\tilde |X_a|_j$ features ($X_aj$'s columns) at each tree split, this time without replacement, defining one the most widely used machine learning algorithms today, the Random Forests \cite{}. The feature subsampling enables faster training in comparison to Bagging or AdaBoost, their contemporaneous counterparts, but maintaining competitive prediction scores \cite{Dietterich_2000}. Furthermore, their popularity might also stem from a low need for data preprocessing and hyperparameter tuning, making Random Forests easily configurable models especially suited for unstructured tabular data \cite{}. {} A typical choice is $\tilde |X_a|_j = \lceil \sqrt |X_a|_j \rceil$.

%An even more aggressive randomization approach was proposed by \cite{Geurts_2006}, named \emph{ExtraTrees}, from extremely randomized trees. In each node, instead of searching for the overall best split threshold $t$, ExtraTrees first draw a random $t_f$ between the minimum and maximum values for each of $\tilde n_f$ randomly chosen feature columnns. The best $t_f$ and its corresponding feature, among the $\tilde n_f$ selected ones, is then returned. The higher randomization dispenses the use of sample bootstraping and turns the process of finding a split search a $O(\tilde n_f \tilde n_s)$ {{}} procedure, rather than Random Forests' {}.

%In both Random Forests or ExtraTrees, the tree components are usually grown to their maximum size, without pruning or using early-stopping parameters.
%
%Since PBCTs can be generated with the exact structure as common decision trees, the same ensemble techniques are possible for these models, with very small modifications regarding data sampling. \cite{Pliakos_2020} explores the use of ExtraTrees ensemble of PBCTs for drug-target interaction prediction, obtaining favorable results in comparison to other methods. However, superiority of Random Forests are often verified \cite{}, and no previous work was found to explore this algorithm.
%
%We thus present an implementation for Random Forests of PBCTs, to which we suggest the name Biclustering Random Forests (BRF). Similarly, we hereafter call ExtraTrees of PBCT by Biclustering Extra Trees (BXT). The procedures to build Random Forests and BRFs are described by algorithms \ref{} and \ref{}, respectively.
%
%%The estimators we tested, with its criterion functions and other hyperparameters are described by Table \ref{}.
%

%\subsection{Implementation details}  % TODO


%\subsection{Dataset}
%In this work, we use data from Drug-Target interactions in the experimental validation of the proposed model. DTI is an area of the literature that has been the focus of several recent advances and consists of methods for predicting interactions between drugs and targets (Proteins, Diseases, Ligands). This area has already been investigated in several applications present in the literature, such as~\cite{Fattahi2019,Nasution2019}. In this context, the dataset used is the Drug-Protein Interaction Networks, defined in~\cite{Yamanishi2008}. This dataset consists of four bipartite interaction networks between proteins and drugs: Ion channels (DPI-I), Nuclear receptors (DPI-N), protein-coupled receptors G (DPI-G), and Enzymes (DPI-E). Both networks of interactions form datasets, and the interaction prediction in these contexts and applying them to the real world can bring innovations and discoveries.

%\subsection{Semi-supervised decision trees}
\subsection{Incorporating semi-supervision into decision trees}
\label{sec:ss trees}

The tree algorithms presented up until this point disregard the positive-unlabeled characteristic of our datasets of interest, either ignoring the possibility of abscence of labels in the training set, or, equivalently, considering all non-occurring interactions as unknown interactions.
In other words, negative labels represent both the interactions known to not occur and the interactions about which no information is available.

Therefore, the machine learning paradigm under which the discussed decision trees are proposed is still \emph{supervised}, in the sense that all training instances are associated with a label (for each sample in $X$ there is a corresponding expected output in $Y$).

As several previous authors~\cite{liu2017lpinrlmf,he2017simboost}, %TODO cite more
we argue that taking into account the partial availability of information that is often intrinsic to interaction prediction problems could thus improve prediction performance of bipartite models and reduce the necessity for labeled data.

When the learning problem at hand does not involve predefined labels, the taks is termed \emph{unsupervised}. The most common example is clustering problems, where the goal is to group similar instances together based solely on their numerical attributes, without any \textit{a priori} information regarding hypothetical classes to which each instance could belong. In such problems, only the $X$ matrix is utilized, there is not an associated label matrix $Y$.

In summary, while supervised learning problems are concerned with the relationship between $X$ and $Y$, aiming to model how a label is determined by the descriptive features of each instance, unsupervised learning problems target the relationship between instances themselves (between rows of $X$), describing how similar they are and how they can be taxonomically organized.

Some problems, however, lie in the intersection of the supervised-unsupervised spectrum. They usually arise when the available labels are scarce or unreliable, so that both supervised and unsupervised objectives are of interest. Take for instance the case of an image gallery phone applicative capable of grouping all photos of a person into a folder with their name. While the name must be asked once to the user, subsequent photos of the same person are automatically grouped under the same directory. The algorithm is both concerned with labeling each photo with the correct name and with grouping similar photos together. Another example can occur when diagnostic data of a large collection of patients is employed to build an automated diagnosis system to assess multiple diseases simultaneously. Almost certainly, not all labels for the occurrence of each medical condition will be available for all patients, so that the algorithm must be able to deal with considerable amounts of missing data. One possible strategy is to infer missing labels from the labels of similar instances, so that the concept of grouping is again into play. Still in the same example, the diagnosis for each disease may be presented in different (unknown) degrees of certainty, resulting from diagnostic procedures of different natures. In this case, an even more refined strategy can be employed, in which the amount of importance given by the model to correctly classify a given label may be adjusted to allow prioritizing information resulting from clustering rather than the label itself, essentially regulating the relative importance between the supervised and unsupervised objectives.

While in the first example the known labels are usually much more scarce than in the diagnostics example, both problems deal with the same underlying issue: the lack of reliable labels for all instances. As such, both label inference (that characterizes supervised problems) and clustering (that characterizes unsupervised problems) are closely intertwined as the model's objective. The goal is to simultaneously learn to correctly classify the instances with known labels in the training set while also considering similarities to infer the labels of the unlabeled instances (or to rectify labels on which one cannot fully rely). The class of such hybrid learning paradigms is called \emph{semi-supervised learning}~\cite{}.

Since missing labels are a defining characteristic of interaction prediction tasks, it is commonly suggested~\cite{he2017simboost,liu2016neighborhood}  %TODO cite more
that applying semi-supervised concepts to our problems could significantly improve the performance of bipartite models.

%TODO survey
%TODO terms: pu, weak labels, one-class, dyadic prediction (pu is a subclass of ss)
%TODO terms: what are standard semi-supervision techniques

For decision trees specifically, there are straightforward ways to incorporate semi-supervised assumptions.
%
Notice that the growing of decision trees unavoidably induces groupings of samples in the training set. 
%since the recursive splitting procedure essentially clusters instances of similar labels together.
Specifically, the structure of each tree represents a hierarchical clustering of the training samples, in which each tree node represents a partition of the training set composed by the training instances that reach that node.
%
However, this clustering procedure is usually preformed under the objective of grouping instances with similar \emph{labels}, which not necessarily means that instances in the same group will have similar \emph{features}.
%
This results from the definition of the impurity function $I$ governing the split search procedure: $I$ is usually chosen as to minimize the divergence of labels within each partition (see \autoref{sec:traditional trees}). As such, $I$ commonly depends on $Y_\text{node}$ alone.
%
With that in mind, decision trees can be naturally adapted to unsupervised or semi-supervised tasks by redefining the impurity function to consider the feature matrix $X_\text{node}$ instead of only the label matrix $Y_\text{node}$.

In a purely unsupervised context, an example would be to utilize the average of column variances in $X_\text{node}$ as the impurity function (\autoref{eq:I unsup}). This would result in a tree that groups instances with similar features together, regardless of their labels. This idea is the main concept behind the CLUS algorithm~\cite{boley1998unsupervised}.
%
\begin{equation}
    I_\text{u, MSE}(X) = \text{MSE}(X) = \left( X\el{ij} - X^{\ang{i}[j]} \right)^{\ang{i}2\ang{j}}
    \label{eq:I unsup}
\end{equation}

To address semi-supervised scenarios, we can consider both supervised and unsupervised objectives simultaneously. This can be done by using a linear combination of unsupervised and supervised impurities to guide the split selection, taking into account both the similarities between features and between labels to build a semi-supervised decision tree.
\autoref{eq:I ss} defines such a hybrid impurity function.
%
\begin{equation}
    I_{ss}(X_\text{node}, Y_\text{node}) =
        (1 - \sigma) \frac{I_u(X_\text{node})}{I_u(X_\text{root})}
        + \sigma \frac{I_s(Y_\text{node})}{I_s(Y_\text{root})}
    \label{eq:I ss}
\end{equation}
%
We divide each term by the corresponding impurities on the root node, thus calculated over the entire training set. The reason is to avoid the influence of possible differences in scale between the values in $X$ and in $Y$. It also compensates relative scale differences that could arise when choosing different functions for $I_u$ and $I_s$.
%
The parameter $\sigma \in [0, 1]$, that we call supervision balance,  % TODO consider alternative names: supervision fraction, ratio, importance, 
weights the relative importance given to each objective, with $\sigma = 0$ corresponding to a fully unsupervised tree and $\sigma = 1$ to a fully supervised tree. Strategies for adjusting $\sigma$ are discussed in \autoref{sec:sigma heuristics}.

For semi-supervised tasks in general, where we have confirmed positive and confirmed negative annotations alongside missing entries, notice how $I_s$ can only be calculated over the non-missing annotations, while $I_u$ can always utilize both labeled and unlabeled instances.
%
Nevertheless, please notice that all confirmed annotations are positive in our present scenario of positive-unlabeled learning. Thus, we must still consider the missing labels as zero entries for the calculation of $I_s$. Even so, we argue 
that decreasing the relative importance of the supervised objective can compensate the label uncertainty and improve generalization. The idea is to encourage the tree to appreciate the structures in the feature space. For example, it should select splits that isolate very compact groups of instances, even if the labels in a given group are not satisfactorily homogeneous.

A caveat of using a semi-supervised impurity as in \autoref{eq:I ss} is that trees could continue to find splits even if all instances in a given node have the same label. This is because we could keep reducing the unsupervised impurity by further splitting even if the supervised impurity is already zero.
However, the output value of each node is still calculated over $Y_\text{node}$ alone, so all nodes descending from a homogeneous node would yield the same output. We avoid such redundant splits by forcefully stopping the split search for a node when $I_s(Y_\text{node})=0$.

Another important notice is that features must be normalized before training the tree: the impurity function $I_u$ (\autoref{eq:I unsup}) is most often sensitive to the relative scale of the different columns of $X$. This is normally not a requirement for decision trees, since the split search procedure is performed separately for each feature and usually only depends on the order of the values, not on their specific magnitude.

% assumes true 0 is more likely
% TODO: impurity that considers 0 as both 0 and 1 (with relative proba (c estimation))

\subsubsection{Unsupervised impurities}
\label{sec:Iu}

We explore two different unsupervised impurity functions for the semi-supervised decision trees: the mean feature variance (\autoref{eq:I unsup}) and the mean pairwise distance between the samples (\autoref{eq:mean distance impurity}). We describe and motivate them in this section.

The initial proposal of a semi-supervised impurity function~\cite{levatic2017semisupervised} employed the mean variance of the feature matrix $X$ as the unsupervised term (\autoref{eq:I unsup}), being concerned with traditional decision trees.
The strategy, however, does not scale well with the number of features.
%which is especially a concern for bipartite trees working 
%main concern with this strategy is that it considerably slows down the training procedure of bipartite trees.
This results from each node having to always consider the same number of features, even though the number of instances decrease.
The scalability is especially a concern in the scenarios under study, in which the feature matrices are square similarity matrices.

% % TODO andre solves this by...
%To remedy this issue, \citet{alves2023semisupervised} proposes to calculate the unsupervised impurity only twice in each node, as a way to choose between the best split axis 

Notice that, in the learning tasks under study, the $X$ matrix already represents similarities between samples, and an additional metric such as the variance of each feature is not necessary to capture how close the samples are to each other. 
%
Exploring this property, we propose a more efficient unsupervised impurity function based on the average similarity between the samples in the tree node (equation \ref{eq:mean distance impurity}).
% only upper triangle is used, X symmetric
%
\begin{equation}
    I_\text{MeanDistance}(X) = \frac{1}{|\mathbf{s}_\text{node}|}
        \sum_{j \in \mathbf{s}_\text{node}}
        \sum_{i \in \mathbf{s}_\text{node}} (1 - X\el{ij})
        % \sum_{(i, j) \in \mathbf{s}_\text{node}} (1 - X\el{ij})
    \label{eq:mean distance impurity}
\end{equation}
%
$\mathbf{s}_\text{node}$ denotes the set of indices representing the samples in the node.
Notice that the number of features to consider equals the number of samples in the node: we always consider a square partition of $X$.
As such, the number of operations required by $I_u$ drops more steeply with respect to the node size in comparison to $I_\text{MSE GMO}$ or similar impurities, 
which is especially beneficial in our case of large $X$ matrices. We can reduce even further the number of operations by considering only the upper or lower triangle of $X$, exploiting $X$'s symmetry.

We note that the subset of features is only used to calculate the impurity function, and not to select split points. The split search procedure is still performed considering all features as usual.



\subsubsection{Heuristics for $\sigma$}
\label{sec:sigma heuristics}
% TODO: the smaller the partition, the smaller we should set the sigma, contrary to what we did. few labels are less informative, arguably, than the distance between few instances

Determining the ideal value of $\sigma$ is not a trivial task. In fact, it is not even clear whether a single constant $\sigma$ for the whole tree is enough or it should be adjusted for each node. We explore four different strategies for setting $\sigma$, that we present in this section.

% For instance, one could argue that nodes with a larger number of unlabeled instances should prioritize the unsupervised objective, since the supervised information is more likely to be unreliable.

% One may also make the point that nodes with fewer instances in total (labeled or unlabeled) should favor the supervised objective, since the unsupervised impurity would be more prone to noise, and thus set larger values of $\sigma$ for smaller nodes or for deeper nodes in the tree structure.

\textcite{alves2023semisupervised} argues that the unsupervised impurity should be more important for nodes with a larger number of unlabeled instances, since the supervised information would be more likely to be unreliable. Thus, they propose updating $\sigma$ according to the label density of each node, as defined by \autoref{eq:sigma density}.
%
\begin{equation}
    \sigma = 0.1 + 0.9 \cdot Y_\text{node}\mel{ij}
    \label{eq:sigma density}
\end{equation}
%
% TODO move to discussion?
However, we must recall that the prototype value of each node is only dependent on its partition of the label matrix. Therefore, if $Y_\text{node}$ is close to being homogeneous, a new split is unable to cause a big overall change in the outputs for the instances involved. Specifically, the most drastic prototype change possible occurs if we perfectly separate the instances with positive labels from the instances with negative labels.
When the node partition is very imbalanced, even this ideal separation will only greatly affect the very few instances with the minority label. Thus, using the heuristic of \autoref{eq:sigma density} prioritizes the unsupervised objective only in cases where no significant change in the output values is expected. Additionally, few new splits are possible in near-homogeneuous nodes, and we are more likely to achieve label homogeneity (and thus stop the split search) right after we achieve the cases in which $I_u$ is highly prioritized. In summary, the heuristic of \autoref{eq:sigma density} is likely to undermine the effect of the unsupervised objective.

% less split to activate stopping

To test this hypothesis, we propose another heuristic for $\sigma$ that prioritizes the unsupervised objective in the earliest stages of the tree growing process and the supervised objective in nodes closer to the leaves (\autoref{eq:sigma size}).
%
\begin{equation}
    \sigma = 1 - \frac{|Y_\text{node}|}{|Y_\text{root}|}
    \label{eq:sigma size}
\end{equation}
%
This way, we take advantage of the unsupervised impurity prior to when the label partitions are already homogeneous. Essentially, we start by performing clustering in the feature space, and then gradually move towards separating instances based on their labels.
This process should be similar to a two step procedure: we first identify large structures in the feature space (when a large number of instances is still available) and then apply the label clustering separately to each of these structures. The difference is that each of our semi-supervised trees represents a gradual transition between the two steps.

We also speculate that the observed benefit of a dynamic $\sigma$ could emerge from the diversity it promotes in tree learners. As discussed in \autoref{sec:bipartite_forests}, the diversity of the individual estimators is a key factor for the success of ensemble models. In this sense, ensuring variety of the $\sigma$ parameter could even be more important than estimating its "correct" value for each partition. Therefore, we also evaluate the strategy of selecting a random $\sigma$ at each node, drawn from a uniform distribution in the interval $[0, 1]$ (\autoref{eq:sigma random}).
%
\begin{equation}
    \sigma \sim \mathcal{U}(0, 1)
    \label{eq:sigma random}
\end{equation}


\section{Other estimator-centered strategies}
\label{sec:learner centered}

This section presents two other 

\subsection{Linear models}
\label{sec:linear_models}

One of the simplest approaches to learning problems in general is to assume a linear relationship between the input features and output labels. Formally, for the non-bipartite case, one assumes that the training output matrix $Y$ can be approximated as follows:
%
\begin{equation}
    \hat Y = X W
\end{equation}
%
in which $W$ is a matrix representing the set of parameters to be learned. To determine $W$, the mean squared error (MSE) is usually defined as the loss function to be minimized, to which we add an extra regularization term controlled by the hyperparameter $\alpha$:
%
\begin{equation}
    \mathcal{J} = \frac{1}{2} \|Y - \hat Y\|^2 + \frac{\alpha}{2} \|W\|^2
    = \frac{1}{2} \|Y - XW\|^2 + \frac{\alpha}{2} \|W\|^2
\end{equation}
%
An analytical solution for $W$ can be obtained by taking the derivative of $\mathcal{J}$ with respect to $W$ and setting it to zero:
%
\begin{equation} %TODO FIX THE ORDER OF THE multiplications
    \frac{\partial \mathcal{J}}{\partial W} = 0
    = X\T (XW - Y) + \alpha W
    %= (Y - XW) + (\alpha\mathbb{I}) W
    \implies W = (X\T X + \alpha I)^{-1} X\T Y
\end{equation}

There are scenarios, however, where the specific values of $X$ are less interesting than the pairwise similarities between them. In those settings, while $X$ may not be directly available, we do have access to similarity matrices $S$ (also called \emph{kernel} matrices) in which $S\el{ij}$ designates a similarity score between $X\el{i}$ and $X\el{j}$. Rather than simply considering $S$ in the same way we would treat $X$ in linear regression, we could instead employ the \emph{kernel trick}~\cite{murphy2012machine}: replacing the $XX\T$ terms in the above equations by $S$. In this case, we are assuming that similarities $S\el{ij}$ represent the internal product of the vectors $X\el{i}$ and $X\el{j}$ in some feature space, which is based on the intuition that the internal product by itself can be regarded as a similarity metric.
%
We also define $W$ slightly differently, ommiting the $X\T$ factor as $W = (S + \alpha I)^{-1} Y$, so that the final prediction is given by
%
\begin{equation}
    \hat Y = S W = (S + \alpha I)^{-1} S
\end{equation}

For the bipartite interaction prediction case, besides standard adaptations as described in \autoref{sec:standard adaptations}, a unique formulation is presented by \textcite{vanlaarhoven2011gaussian}. Similar in concept to the standard global single output procedure (\autoref{sec:standard adaptations}), the authors consider each interaction pair as a unitary instance. The authors then propose building a kernel matrix relating each pair of instances to another pair, and not each interacting entity to another of the same domain. If $S_1 \in \mathbb{R}^{n_1 \times n_1}$ and $S_2 \in \mathbb{R}^{n_2 \times n_2}$ are the intra-domain similarity matrices, the global kernel matrix $S$ is defined as
%
\begin{equation}
    S\el{(i_1n_2 + i_2)(j_1 n_2 + j_2)} = S_1\el{i_1j_1} S_2\el{i_2j_2}
\end{equation}
%
or, more succinctly, as the Kronecker product of $S_1$ and $S_2$:
%
\begin{equation}
    S = S_1 \otimes S_2
\end{equation}
%
Each entry on $S$ thus represents the similarity between the pair $X_1\el{i_1}$-$X_2\el{j_1}$ and another pair $X_1\el{i_2}$-$X_2\el{j_2}$ by the product of the similarities between $X_1\el{i_1}$ and $X_1\el{j_1}$ and between $X_2\el{i_2}$ and $X_2\el{j_2}$.

The bipartite linear regression is then framed on the vectorized $Y$, denoted $\text{vec}(Y)$, built by concatenating the columns of $Y$ into a single $|Y|$ by $1$ column vector.
Purely for notation purposes, we organize the weight parameters as the vectorized version of a matrix $W$ with the same dimensions of $Y$.
%
\begin{gather}
    \text{vec}(\hat Y) = S\,\text{vec}(W) \approx \text{vec}(Y)
    \\
    \text{vec}(W) = (S + \alpha I)^{-1} \text{vec}(Y)
\end{gather}

As the reader may imagine, $S$ gets prohibitively large for big datasets (it's a $n_1 n_2$-sized square matrix!), both in terms of memory usage and the time needed to perform the matrix inversion. The authors, however, provide a clever way of circumventing this issue by decomposing each of the $S_1$ and $S_2$ kernel matrices separately and exploiting the properties of the Kronecker product.

Given that $S_1$ and $S_2$ are symmetric square matrices, it follows from the spectral theorem~\cite{} that they can be decomposed as follows:
%
\begin{gather*}
    S_1 = U_1 \Lambda_1 U_1\T \\
    S_2 = U_2 \Lambda_2 U_2\T
\end{gather*}
%
where, if $\mathbf{\lambda}_1$ represents the vector of eigenvalues of $S_1$, $\Lambda_1$ is the diagonal matrix of those eigenvalues ($\Lambda_1 = \text{diag}(\mathbf{\lambda}_1)$), with $U_1$ columns representing their corresponding eigenvectors $U^{\intercal[i]}$ for each $\mathbf{\lambda}_1\el{i}$. The symmetry of the similarity matrices also implies that $U_1$ and $U_2$ are orthogonal, i.e. $U_1\T U_1 = U_1 U_1\T = \mathbb{I}$ and $U_2\T U_2 = U_2 U_2\T = \mathbb{I}$, or, equivalently, $U_1^{-1} = U_1\T$ and $U_2^{-1} = U_2\T$.
%
Utilizing the fact that $(AB) \otimes (CD) = (A \otimes C)(B \otimes D)$~\cite{}, the Kronecker product of $S_1$ and $S_2$ can be written as
%
\begin{equation}
    S = S_1 \otimes S_2
    = (U_1 \Lambda_1 U_1\T) \otimes (U_2 \Lambda_2 U_2\T)
    % = (U_1 \otimes U_2) (\Lambda_1 V_1\T \otimes \Lambda_2 V_2\T)
    % = (U_1 \otimes U_2) (\Lambda_1 \otimes \Lambda_2) (V_1\T \otimes V_2\T)
    = (U_1 \otimes U_2) (\Lambda_1 \otimes \Lambda_2) (U_1 \otimes U_2)\T
    = U \Lambda U\T
\end{equation}
%
in which we denote $U = U_1 \otimes U_2$ and $\Lambda = \Lambda_1 \otimes \Lambda_2$.
$W$ now becomes
%
\begin{equation*}
    \text{vec}(W) = (U \Lambda U\T + \alpha \mathbb{I})^{-1} \text{vec}(Y)
\end{equation*}
%
Further exploring the orthogonality of $U$, $U U\T = U \mathbb{I} U\T = \mathbb{I}$, so that
%
\begin{equation*}
    \text{vec}(W) = U (\Lambda + \alpha\mathbb{I})^{-1} U\T \text{vec}(Y)
\end{equation*}
%
The most crucial property of the Kronecker product for our application is its relationship with the vectorization operator~\cite{}: $(A \otimes B)\text{vec} (C) = \text{vec}(BCA\T)$, which allows us to write
%
\begin{equation*}
    \text{vec}(W)
    = U (\Lambda + \alpha\mathbb{I})^{-1} (U_1\T \otimes U_2\T) \text{vec}(Y)
    = U (\Lambda + \alpha\mathbb{I})^{-1} \text{vec}(U_2\T Y U_1)
\end{equation*}
%
Since $(\Lambda + \alpha\mathbb{I})^{-1}$ is diagonal, its multiplication by the vector $\text{vec}(U_2\T Y U_1)\T$ can be expressed as a Hadamard product (element-wise multiplication, denoted by $\odot$) between two vectors. Acting element-wise, the Hadamard product is unaffected by vectorization, so that we can simplify the above expression by employing the matrix
%
\begin{equation}
    (\alpha + \mathbf{\lambda}_1 \otimes \mathbf{\lambda}_2\T)^{\circ -1[ij]}
    = \frac{1}{\alpha + \mathbf{\lambda}_1\el{i} \mathbf{\lambda}_2\el{j}}
\end{equation}
%
In this context, $\mathbf{\lambda}_1 \otimes \mathbf{\lambda}_2\T$ represents the $n_1$ by $n_2$ matrix resulting from the \emph{outer product} of the vectors containing the eigenvalues of $S_1$ and $S_2$, respectively, while $A^{\circ -1}$ denotes the Hadamard inverse, corresponding to the matrix formed by taking the reciprocal of each element in $A$. Thus,
%
\begin{multline*}
    \text{vec}(W)
    = U \text{diag}(\Lambda + \alpha^{-1}\mathbb{I}) \odot \text{vec}(U_2\T Y U_1)
    =\\
    = (U_1 \otimes U_2) \text{vec}(
        (\alpha + \mathbf{\lambda}_1 \otimes \mathbf{\lambda}_2\T)^{\circ -1}
        \odot (U_2\T Y U_1)
    )
    =\\
    = \text{vec}(
        U_2
        [
            (\alpha + \mathbf{\lambda}_1 \otimes \mathbf{\lambda}_2\T)^{\circ -1}
            \odot (U_2\T Y U_1)
        ]
        U_1\T
    )
\end{multline*}
%
Which yields
%
\begin{equation}
    W = 
        U_2
        [
            (\alpha + \mathbf{\lambda}_1 \otimes \mathbf{\lambda}_2\T)^{\circ -1}
            \odot (U_2\T Y U_1)
        ]
        U_1\T
\end{equation}

Finally, predictions for a new group of instances in the test set are obtained as follows from the similarities with the train instances ($S_\text{1, test}\el{ij}$ specifies the similarity between $X_\text{1, test}\el{i}$ and $X_\text{1, train}\el{j}$).
%
\begin{equation}
    \text{vec}(\hat Y_\text{test})
    = (S_\text{1, test} \otimes S_\text{2, test}) \text{vec}(W)
    = \text{vec}(S_\text{2, test} W S_\text{1, test}\T)
\end{equation}
%
which summarizes to
%
\begin{equation}
    \hat Y_\text{test} =
        S_\text{2, test}
        U_2
        [
            (\alpha + \mathbf{\lambda}_1 \otimes \mathbf{\lambda}_2\T)^{\circ -1}
            \odot (U_2\T Y U_1)
        ]
        U_1\T
        S_\text{1, test}\T
\end{equation}

%TODO rlskron did not worry about new instances
%TODO complexity
%TODO using all neighbors in BLM is prone to overfitting
%TODO converting usual problems to bipartite format
%TODO although nrlmf is fast, building a complete tree after is not
%TODO grid search can be even detrimental in small sparse datasets, since test sets are not representative



\subsection{Neighborhood-Regularized Matrix Factorization}
\label{sec:nrlmf}

\subsubsection{Traditional matrix factorization}
\label{sec:traditional matrix factorization}

% Several algorithm proposals have been made throughout the last decades to tackle the problem of bipartite interaction prediction. 
%TODO collaborative filtering in the context of recommendation systems


% Traditional in the context of recommendation systems, matrix 
The idea behind matrix factorization is to find an approximation of the interaction matrix $Y$ by decomposing it into two matrices $U$ and $V$ of lower dimensions, such that %TODO not decomposing
%
\begin{equation}
    \hat Y = U V^T \approx Y
    \label{eq:traditional matrix_factorization}
\end{equation}
%
The rows of $U$ and $V$ can be seen as learned representations in a new vector space (usually called \emph{latent} space) of each sample in the row and column domains, so that $U\el{i}$ represents $X_1\el{i}$ and $V\el{j}$ represents $X_2\el{j}$.
%
Notice that the number of latent features is constrained by \autoref{eq:traditional matrix_factorization} to be the same for both instance domains: $|U|_j=|V|_j$, being an arbitrary hyperparameter to be defined by the user. Usually, the number of latent features is set to be much smaller than the number of original features ($|U|_j \ll |X_1|_j$ and $|V|_j \ll |X_2|_j$), requiring less computational labor and generating models less susceptible to overfitting.

The learning procedure of matrix factorization algorithms thus consists of obtaining such latent feature matrices $U$ and $V$ so to approximate $Y$ as best as possible. The most common approach is to define a loss function that penalizes the difference between the predicted and the true values of $Y$ and to employ gradient descent techniques to gradually change $U$ and $V$ in the direction that minimizes such loss.
%
% TODO For instance, 

As can be deduced from \autoref{eq:traditional matrix_factorization}, the dot product of each row of $U$ and $V$ approximates the corresponding element of $Y$:
%
\begin{equation}
    \hat Y\el{ij} = U\el{i} \cdot V\el{j}
\end{equation}
%
Logistic matrix factorization (LMF) slightly redefines the problem by introducing one more step to obtain $\hat Y$ from $U$ and $V$~\cite{johnsonlogistic}: it assumes that the interaction matrix $Y$ is the result of the logistic function applied to $UV\T$ and not only $UV\T$ anymore.
%
\begin{equation}
    \hat Y\el{ij} = \text{logistic}(U\el{i} \cdot V\el{j})
    = \frac{\exp(U\el{i} \cdot V\el{j})}{1 + \exp(U\el{i} \cdot V\el{j})}
    \label{eq:lmf_prediction}
\end{equation}
%
where $\mathbf{a} \cdot \mathbf{b}$ represents the dot product between the vectors $\mathbf{a}$ and $\mathbf{b}$. When applied to a matrix, we assume that $\log$ and $\exp$ functions operate element-wise:
%
\begin{equation*}
    (\log M)\el{ij} = \log M\el{ij}
\end{equation*}

If $\hat Y$ is interpreted as the probability of being positive as predicted by the model ($\hat Y\el{ij} = P(Y\el{ij} = 1)$ and $1-\hat Y\el{ij} = P(Y\el{ij} = 0)$),
the optimization objective is usually based on maximizing the joint probability of correctly guessing all interactions in $Y$:
%
\begin{equation}
    P(\hat Y = Y) = \prod_{ij} | \hat Y\el{ij} + Y\el{ij} - 1|
    \label{eq:lmf_objective_raw}
\end{equation}
%
in which $|a|$ represents the absolute value of $a$. A few modifications are then further made:
%
\begin{enumerate}
    \item The logarithm of the objective function is taken to simplify the expression without affecting the optimization problem, since $\log$ is a monotonic function;
    \item Since positive interactions are usually less numerous but more important in matrix completion problems, a factor $\alpha$ is introduced to prioritize them, multiplying the terms corresponding to $Y\el{ij}=1$ in the objective function. It results as if $alpha$ copies of each positive interaction are present in the training set;
    \item To discourage overfitting and avoid $U$ and $V$ being arbitrarily large, quadratic regularization terms are added, penalizing the magnitude of the elements of $U$ and $V$.
    \item Similarity information between samples is incorporated by NRLMF,%TODO
\end{enumerate}
%
%The resulting objective function is then given by
To simplify notation, we define matrices $J$ whose combined sum of all elements corresponds to the objective function $\mathcal{J}$:
%
\begin{equation}
    \mathcal{J} =
        \sum J_\text{labels}
        + \sum J_\text{1, regularization}
        + \sum J_\text{2, regularization}
        + \sum J_\text{1, neighborhood}
        + \sum J_\text{2, neighborhood}
    \label{eq:lmf_objective_matrices}
\end{equation}
%
where by $\sum J$ we denote $\sum_i^{|J|i}\sum_j^{|J|_j} J$. The sums must be performed individually due to the $J$ matrices having different dimensions.

Applying the logarithm to \autoref{eq:lmf_objective_raw} we have our first term of $\mathcal{J}$:
%
\begin{equation*}
    J_\text{labels} = \log |\hat Y + Y - 1| = Y \odot \log \hat Y + (1 - Y) \odot \log (1 - \hat Y)
\end{equation*}
%
where we separate the cases in which $Y\el{ij} = 1$ and $Y\el{ij} = 0$. $\odot$ represents the Hadamard product (element-wise multiplication) between matrices:
%
\begin{equation*}
    (A \odot B)\el{ij} = A\el{ij} B\el{ij}
\end{equation*}
%
Adding the aforementioned positive importance factor $\alpha$ and expanding $\hat Y$ according to \autoref{eq:lmf_prediction} we have
%
%%% Using indices instead of J matrix
% \begin{multline}
%     %- \lambda \|U\|^2 - \|V\|^2
%     \mathcal{L} =
%         \sum_{ij} \alpha Y\el{ij} \log \hat Y\el{ij}
%         + (1 - Y\el{ij}) \log (1 - \hat Y\el{ij})
%     =\\
%     =
%     \sum_{ij}
%         \alpha Y\el{ij} \left\{W\el{ij} - \log\left[1 + \exp(W\el{ij})\right]\right\} 
%         - (1 - Y\el{ij}) \log \left[1 + \exp(W\el{ij})\right]
%     =\\
%     =
%     \sum_{ij}
%         \alpha Y\el{ij} W\el{ij}
%         + [(1 - \alpha) Y\el{ij} - 1] \log \left[1+\exp(W\el{ij})\right]
% \end{multline}
%
\begin{multline}
    J_\text{labels} =
        \alpha Y \odot \left\{UV\T - \log\left[1 + \exp(UV\T)\right]\right\} 
        - (1 - Y) \odot \log \left[1 + \exp(UV\T)\right]
    =\\
    =
        \alpha Y \odot UV\T
        + [(1 - \alpha) Y - 1] \odot \log \left[1+\exp(UV\T)\right]
\end{multline}

To discourage large values in $U$ and $V$, we consider quadratic regularization terms weighted by hyperparameters $\lambda_1$ and $\lambda_2$:
%
\begin{gather}
    J_\text{1, regularization}
        = -\frac{\lambda_1}{2} U \odot U
        = -\frac{\lambda_1}{2} \mathbb{I} \odot (UU\T)\\
    J_\text{2, regularization}
        = - \frac{\lambda_2}{2} V \odot V
        = - \frac{\lambda_2}{2} \mathbb{I} \odot (VV\T)
    \label{eq:lmf_regularization}
\end{gather}
%
%In the LMF context of \autoref{eq:lmf_prediction}, the regularization terms can be interpreted as a Gaussian prior on the latent features, centered at the origin with variance $\lambda_1$ and $\lambda_2$ for $U$ and $V$, respectively~\cite{johnsonlogistic}.
If the initial objective proposal can be interpreted as maximizing the probability of guessing all labels correctly given specific $U$ and $V$ ($\sum J_\text{labels} = \log P(\hat Y=Y\mid U,V)$), adding the regularization terms is equivalent to introduce prior assumptions about $U$ and $V$ distributions and define a slightly different objective: maximizing the posterior probability to obtain the current $U$ and $V$ given that $\hat Y = Y$. Applying Bayes' theorem and assuming $P(Y) = 1$ we have
%
\begin{equation}
    P(U,V\mid \lambda_1, \lambda_2, Y) = P(Y\mid U,V)\; P(U\mid \lambda_1) \; P(V\mid \lambda_2)
\end{equation}
%
Under the assumption that the values in $U$ and $V$ follow zero-centered spherical gaussian distributions with variances given by $\frac{1}{\lambda}$, that is, $U\el{ij}\sim\mathcal{N}(0, \lambda_1^{-1}\mathbf{I})$ and $V\el{ij}\sim\mathcal{N}(0, \lambda_2^{-1}\mathbf{I})$, we recover the regularized objective function of \autoref{eq:lmf_regularization}~\cite{johnsonlogistic}.
%
\begin{multline}
    \log P(U,V\mid \lambda_1, \lambda_2, Y)
    =\\
    = \log P(Y\mid U,V)
        + \sum \log(\exp(-\frac{\lambda_1}{2} U \odot U))
        + \sum \log(\exp(-\frac{\lambda_2}{2} V \odot V))
    =\\
    = \sum J_\text{labels}
        - \sum \frac{\lambda_1}{2} U \odot U
        - \sum \frac{\lambda_2}{2} V \odot V
\end{multline}
%
Therefore, if multiple values of $U$ and $V$ possibly generate the same $\hat Y = Y$, applying the regularization can be understood as not only finding one of such combinations but, between those $U$ and $V$ that satisfy $\hat Y = Y$, finding the $U$ and $V$ that are most likely to be randomly sampled. If $P(U, V\mid \lambda_1, \lambda_2, Y)$ continuously varies as a function of $U$ and $V$, pooling $U$ and $V$ from a region of maximal $P(U, V\mid \lambda_1, \lambda_2, Y)$ should improve generality, arguably ensuring that stochastic deviations of $U$ and $V$ would still result in high $P(\hat Y = Y)$ and justifying the use of regularization as a way to avoid overfitting. %TODO confuso?

% Being bounded by $[0, 1]$, the predictions $\hat Y$ are interpreted as the probabilities of each interaction to be positive.
% %
% \begin{equation}
%     \hat Y\el{ij} = P(Y\el{ij} = 1)
% \end{equation}
% %
% The probability of correctly guessing the label $Y\el{ij}$ is then given by
% \begin{equation}
%     P(\hat Y\el{ij} = Y\el{ij})
%     =
%     \begin{cases}
%         \hat Y\el{ij} & \text{if } Y\el{ij} = 1\\
%         1 - \hat Y\el{ij} & \text{if } Y\el{ij} = 0
%     \end{cases}
% \end{equation}
% %
% alternatively formulated as
% %
% \begin{equation}
%     P(\hat Y\el{ij} = Y\el{ij})
%     = \hat Y\el{ij}^{Y\el{ij}} (1 - \hat Y\el{ij})^{1 - Y\el{ij}}
% \end{equation}
% 
% We will then define the objective function based on the probability of correctly guessing all interactions in $Y$:
% TODO must assume independent Y

One may have noticed that the original feature matrices $X_1$ and $X_2$ were not considered in any regard when describing matrix factorization and detailing the objective functions.
Born in the context of recommendation systems where the relationship labels are usually the only information available, matrix factorization algorithms in general encounter a significant issue when brought to our current scenario of bipartite interaction prediction:
in its canonic formulation, they do not take sample-level features into account, often called \emph{side information} or \emph{side features} in the recommendation field~\cite{}, possibly overlooking valuable data. As a consequence, they are unable to provide predictions for new samples that were not present in the training set, since no information about them is available to be inputted to the model. This is commonly regarded as the \emph{cold-start problem}~\cite{}. As such, matrix factorization usage is usually restricted to the task of matrix completion, in which the goal is to predict the missing values of a matrix given the values of the remaining elements, without receiving completely new rows or columns during model evaluation~\cite{}.


\subsubsection{Neighborhood regularization}
\label{sec:neighborhood regularization}

Targeting these issues, \textcite{liu2016neighborhood,liu2017lpinrlmf,liu2020predicting} proposes a modification to LMF that incorporates side information into the model, successfully applying their method to interaction problems such as drug-target interaction prediction~\cite{liu2016neighborhood}, long non-coding RNA interactions~\cite{liu2017lpinrlmf} and microRNA interactions~\cite{liu2020predicting}.
%
The core idea of their technique lies in adding one more term to the objective function, penalizing instances regarded as close when considering the original features but were separated by $U$ and $V$, placed far from each other in the latent space. As such, the algorithm is called \emph{Neighborhood-Regularized Logistic Matrix Factorization} (NRLMF).
%
To precisely define it, let's consider similarity-weighted adjacency matrices $A_1$ and $A_2$ referring to each sample domain that specifies neighborhood relationships between samples. If $S_1\el{ij}$ denotes a similarity score between $X_1\el i$ and $X_1\el j$, $A_1\el{ij}$ is set to this similarity value if $X_1\el{j}$ belongs to the neighborhood of $X_1\el{i}$, denoted $N(X_1\el i)$, and 0 otherwise:
%
\begin{equation}
    A_1\el{ij} =
    \begin{cases}
        S_1\el{ij} & \text{if } X_1\el{j} \in N(X_1\el i)\\
        0 & \text{otherwise}
    \end{cases}
    \label{eq:nrlmf_A}
\end{equation}
%
Multiple options are available for the definition of neighborhoods, such as considering all samples within a certain radius of each other or only the $k$ nearest neighbors of each sample. In this work, following the original proposal of NRLMF~\cite{liu2016neighborhood}, we will consider the latter, defining $A_1$ and $A_2$ as the adjacency matrices of the $k$-nearest neighbors graphs of $X_1$ and $X_2$, respectively. In other words, $N(X_1\el{i})$ is the set formed by the $k$ rows $X_1\el{j}$ with the $k$ highest $s_ij$. In the interaction prediction problems we analyze, similarities are precalculated so that the $X$ matrices directly provide the distance metric over which the nearest neighbors are selected. That is, $X_1$ and $X_2$ themselves already constitute pairwise similarity matrices: $S_1\el{ij} = X_1\el{ij}$. In general, however, one may need to define a kernel matrix $S$ as a preprocessing step, choosing a distance metric over the original features to be used in the nearest neighbors search such as the Euclidean distance or a radial basis function~\cite{vanlaarhoven2011gaussian}. In any case, notice that $A$ is a function of $X$ alone for NRLMF. While $Y$ may also be considered in similar scenarios (as will be discussed ahead), $A$ does not depend on $U$ or $V$ and can be built as a single pre-training step.

The loss term proposed by NRLMF is then given by the sum of the Euclidean distances in the latent space between samples in the same neighborhood, weighted by their similarities:
%
\begin{gather}
    \mathcal{J}_\text{1, neighborhood} =
    - \sum_{ij}A_1\el{ij} \|U\el{i} - U\el{j}\|^2
    \\
    \mathcal{J}_\text{2, neighborhood} =
    - \sum_{ij}A_2\el{ij} \|V\el{i} - V\el{j}\|^2
    \label{eq:neighborhood_regularization}
\end{gather}
%
in which $\|\mathbf{v}\|$ represents the Euclidean norm of a vector $\mathbf{v}$.
Concentrating on the row instances and expanding the last definition we have
%
\begin{multline*}
    \sum_{ij}A_1\el{ij} \|U\el{i} - U\el{j}\|^2
    = \sum_{ij} A\el{ij}
    \left(
        U\el{i}\cdot U\el{i} + U\el{j}\cdot U\el{j} - 2 U\el{i}\cdot U\el{j}
    \right)
    =\\
    =
        \sum_i \left(\sum_j A_1\el{ij}\right) U\el{i}\cdot U\el{i}
        + \sum_j \left(\sum_i A_1\el{ij}\right) U\el{j}\cdot U\el{i}
\end{multline*}
%
The terms in which $U$ appears with the same index ($U\el{i}\cdot U\el{i}$ and $U\el{j}\cdot U\el{j}$) can be rewritten to include both by multiplying them by the identity matrix $\mathbb{I}$. Essentially, we consider $\sum_i U\el{i}\cdot U\el{i} = \text{trace}(U U\T) = \sum_{ij} (\mathbb{I} \odot UU\T)\el{ij}$.
%
\begin{equation*}
    \sum_i \left(\sum_j A_1\el{ij}\right) U\el{i}\cdot U\el{i}
    = \sum_i \left(\sum_j A_1\el{ij}\right) \sum_k \mathbb{I}\el{ik}U\el{i}\cdot U\el{k}
    = \sum_{ij} \left(\sum_k A_1\el{ik}\right) \mathbb{I}\el{ij} U\el{i}\cdot U\el{j}
\end{equation*}
%
This allows us to write
%
\begin{multline*}
    \sum_{ij}A_1\el{ij} \|U\el{i} - U\el{j}\|^2
    =\\
    = \sum_{ij}
        \left[
            \left(\sum_k A_1\el{ik} + \sum_l A_1\el{lj}\right)
            \mathbb{I}\el{ij}
            - 2 A_1\el{ij} 
        \right]
        U\el{i}\cdot U\el{j}
    =\\
    = \sum L_1 \odot (UU\T)
    %= tr ( U L U\T )
\end{multline*}
%
in which we define
%
\begin{equation}
    L_1\el{ij}
    = \left(\sum_k A_1\el{ik} + \sum_l A_1\el{lj}\right)
        \mathbb{I}\el{ij}
        - 2 A_1\el{ij} 
    %
    = \left(\sum_k A_1\el{ik} + A_1\el{kj}\right)
        \mathbb{I}\el{ij}
        - 2 A_1\el{ij} 
    \label{eq:laplacian}
\end{equation}
%
Notice that taking $A_1\T$ instead of $A_1$ has no effect on the final result, since $\sum A_1 UU\T = \sum A_1\T UU\T$. We could then work with a symmetrized version of $A_1$ from the start:
%
\begin{equation*}
    \tilde A_1 = A_1 + A_1\T
\end{equation*}
yielding
%
\begin{equation*}
    L_1 = \left(\sum_k \tilde A_1\el{ik}\right)
        \mathbb{I}\el{ij}
        - \tilde A_1\el{ij} 
\end{equation*}
%
We can see that the first term of $L_1$ acts in a similar way to the quadratic regularization terms presented by \autoref{eq:lmf_regularization}, multiplying the main diagonal of $UU\T$ and thus penalizing the model for latent vectors with large Euclidean norms (the diagonal of $UU\T$ holds the squared norms $U\el{i}\cdot U\el{i}$). The amount of regularization is however pondered by the weighted number of neighbors of each sample in this case: $\sum_k A_1\el{ik}$ represents the sum of similarities of $X_1\el{i}$ with all its neighbors, also called the \emph{degree} of a sample. This results in samples with larger and more compact neighborhoods being more heavily penalized for having large norms in the latent space. The second term of \autoref{eq:laplacian}, on the other hand, rewards the model for placing close neighbors colinear to each other in the latent space, summing over $S\el{ij} U\el{i}\cdot U\el{j}$ terms
%(a metric of vector colinearity)
between each sample and its neighbors ($A_1\el{ij}$ is $0$ if $U\el{i}$ and $U\el{j}$ are not neighbors).
%TODO more on interpretation and properties of laplacian matrix

Finally, the neighborhood regularization terms are written as
%
\begin{gather}
    J_\text{1, neighborhood} = -\frac{\beta_1}{2} L_1 \odot (UU\T)\\
    J_\text{2, neighborhood} = -\frac{\beta_2}{2} L_2 \odot (VV\T)
\end{gather}
%
%
% \begin{equation}
%     \begin{split}
%         \mathcal{L} &=\\
%             &= \alpha Y \odot UV\T
%             + [(1 - \alpha) Y - 1] \odot \log \left[1+\exp(UV\T)\right]\\
%             &- \frac{\lambda_1}{2} U \odot U
%             - \frac{\lambda_2}{2} V \odot V\\
%             &- \frac{\beta_1}{2} L_1 \odot (UU\T)
%             - \frac{\beta_2}{2} L_2 \odot (VV\T)
%     \end{split}
% \end{equation}

Combining the matrix terms as in \autoref{eq:lmf_objective_matrices}, NRLMF's objective function is given by
%
\begin{equation}
    \begin{split}
        \mathcal{J} = &\; \sum
                \alpha Y \odot UV\T
                + [(1 - \alpha) Y - 1] \odot \log \left[1+\exp(UV\T)\right]\\
            &- \sum \frac{1}{2} (\lambda_1\mathbb{I} + \beta_1 L_1) \odot (U U\T)\\
            &- \sum \frac{1}{2} (\lambda_2\mathbb{I} + \beta_2 L_2) \odot (V V\T)
    \end{split}
    \label{eq:nrlmf_objective}
\end{equation}
%
and the derivatives of the objective function with respect to $U$ and $V$ to be used in the gradient descent procedure are given by
% \begin{equation}
%     \begin{split}
%         \mathcal{L} =
%             \alpha Y \odot UV\T
%             + [(1 - \alpha) Y - 1] \odot \log \left[1+\exp(UV\T)\right]
%             - \frac{\lambda_1}{2} U \odot U
%             - \frac{\lambda_2}{2} V \odot V
%             - \frac{\beta_1}{2} L_1 \odot (UU\T)
%             - \frac{\beta_2}{2} L_2 \odot (VV\T)
%     \end{split}
% \end{equation}
% TODO: appendix showing d(A \odot U V\T) / dU = A V
% TODO: appendix showing d(A \odot U V\T) / dV = A\T U
% TODO: appendix showing d(A \odot U U\T) / dU = A U + A\T U
\begin{gather}
    G_U = \frac{\partial \mathcal{J}}{\partial U} =
        \{[(1 - \alpha) Y - 1] \odot \hat Y + \alpha Y\}V
        - (\lambda_1\mathbb{I} + \beta_1 L_1) U
    \label{eq:nrlmf_gradient_U}
    \\
    G_V = \frac{\partial \mathcal{J}}{\partial V} =
        \{[(1 - \alpha) Y - 1] \odot \hat Y + \alpha Y\}\T U
        - (\lambda_2\mathbb{I} + \beta_2 L_2) V
    \label{eq:nrlmf_gradient_V}
\end{gather}
%
The training procedure of NRLMF is presented by \autoref{alg:nrlmf_train}, and consists of alternated updates on $U$ and $V$ in the gradient's direction until certain stop criteria are satisfied. Common choices for stopping conditions are a maximum number of iterations or a minimum change in the objective function between iterations.

Since faster convergence is reported by the original authors~\cite{johnsonlogistic}, we follow \textcite{johnsonlogistic,liu2016neighborhooda} by implementing the AdaGrad procedure~\cite{duchi2011adaptive}, in which the length of each gradient step is divided by the square-root sum of squared previous steps:
%
\begin{equation}
    U_{t+1} = U_t + \frac{\eta G_{U,t}}{\sqrt{\sum_{t'=0}^t G_{U,t'}^2}}
\end{equation}
%
where $G_{U,t}$ is the partial derivative $\frac{\partial\mathcal{J}}{\partial U}$ of the objective function with respect to $U$ at step $t$, and $\eta$ is the user-defined learning rate. The same is done for $V$.

\algNRLMFTrain

The main importance of NRLMF however lies in the inference phase. As mentioned, matrix factorization methods are not designed to deal with new input samples, that are not present in the training set. Specifically, traditional matrix factorization is incapable of generating latent vectors for the unseen samples to be used for label prediction. An idea that may seem natural at first glance is to delay training to the arrival of new instances, including them in the training set with zero-only labels before performing the optimization. But even then, using an objective function based only on $Y$ as is traditionally done, no new information is brought by the new instances and adding new zeroed rows or columns to $Y$ will mainly introduce noise to the training data and likely degrade the model's predictive performance.

NRLMF, however, leverages proximity information encoded by $X$ to remarkably enable determining latent feature vectors for completely new instances. The neighborhood regularization terms in the objective function now reveal their full importance: they support proximity as a transferable property between the original and the latent spaces. By encouraging that neighbors in $X_1$ and $X_2$ remain close in $U$ and $V$, we can infer latent features of new instances based on their neighborhood.

Consider the test similarity matrices $S_{1\text{, test}}$ and $S_{2\text{, test}}$ respectively derived from $X_\text{1, test}$ and $X_\text{2, test}$, relating the new instances to the known training samples. For instance, $S_{1\text{, test}}\el{ij}$ represents the similarity between $X_\text{1, test}\el{i}$ and $X_\text{1, train}\el{j}$. If $A_\text{1, test}$, like before in \autoref{eq:nrlmf_A}, accordingly restricts the similarity matrix to the neighborhood of each sample,
%
\begin{equation}
    A_\text{1, test}\el{ij} =
    \begin{cases}
        S_\text{1, test}\el{ij} & \text{if } X_\text{1, train}\el{j} \in N(X_\text{1, test}\el i)\\
        0 & \text{otherwise}
    \end{cases}
    \label{eq:nrlmf_A_test}
\end{equation}
%
the latent feature vector of a new instance is simply estimated as the weighted average of its neighbors' latent representations:
% \begin{equation}
%     U_\text{test} = A_\text{1, test} U_\text{train}
% \end{equation}
%
\begin{equation}
    U_\text{test}\el{i} = \frac{A_\text{1, test}\el{i} U_\text{train}}{\sum A_\text{1, test}\el{i}}
\end{equation}
%
the analogous being held for $V$, so that new predictions are made as usual with \autoref{eq:lmf_prediction}, where $U_\text{test}$, $U_\text{train}$, $V_\text{test}$ and $V_\text{train}$ can be used in accordance with the prediction task under study (see \autoref{sec:cross_validation} for details on the different prediction scenarios):
%
\begin{gather}
    \begin{aligned}
        \hat Y_\text{TT} = \frac{\exp(U_\text{test} V_\text{test}\T)}{1 + \exp(U_\text{test} V_\text{test}\T)}&&
        \hat Y_\text{TL} = \frac{\exp(U_\text{test} V_\text{train}\T)}{1 + \exp(U_\text{test} V_\text{train}\T)}&&
        \hat Y_\text{LT} = \frac{\exp(U_\text{train} V_\text{test}\T)}{1 + \exp(U_\text{train} V_\text{test}\T)}
    \end{aligned}
\end{gather}


%\subsubsection{Further developments on NRLMF}
%\label{sec:further nrlmf}




\section{Assessing the performance of bipartite models}
\label{sec:experiments}

\subsection{Datasets}
\label{sec:datasets}
% TODO: detailed description of similarity metrics
% TODO: detailed description of biological concepts

We gathered ten publicly available interaction datasets to evaluate the performance of the proposed models. Quantitative information about each of them is presented by \autoref{tab:datasets}, and more detailed descriptions are provided in this section.

% TODO: organize
\begin{table}[tb]
    \centering
    \footnotesize
    \begin{tabular}{lrrr}%r}
        \toprule
        Dataset & Type of interaction & $Y$ shape & Density\\% & References\\
        \midrule
        DPI-E & Drug-enzyme & $664 \times 445$ & 0.9902\%\\% &\cite{yamanishi2008}\\
        DPI-G & Drug-GPCR & $95 \times 223$ & 2.997\%\\% &\cite{yamanishi2008}\\
        DPI-G & Drug-GPCR & $95 \times 223$ & 2.997\%\\% &\cite{yamanishi2008}\\
        DPI-I & Drug-ion channel & $204 \times 210$ & 3.445\%\\% &\cite{yamanishi2008}\\
        DPI-N & Drug-nuclear receptor & $26 \times 54$ & 6.410\%\\% &\cite{yamanishi2008}\\
        ERN & Gene-transcription factor & $1164 \times 154$ & 1.837\%\\% &\cite{faith2007}\\
        SRN & Gene-transcription factor & $1821 times 113$ & 1.780\%\\% &\cite{macisaac2006, hughes2000, hu2007, chua2006, schrynemackers2015}\\
        %SRN & Gene-transcription factor & $1821 \times 113$ & 1.780\% &\cite{macisaac2006}\\
        DAVIS & Inhibitor-kinase & $68 \times 442$ & 5.011\%\\% &\cite{davis2011,he2017simboost,huang2020deeppurpose}\\
        KIBA & Inhibitor-kinase & $2111 \times 229$ & 19.74\%\\% &\cite{tang2014,he2017simboost,huang2020deeppurpose}\\
        NPInter & lncRNA-protein & $586 \times 446$ & 18.12\%\\% &\cite{wu2006npinter, teng2020npinter}\\
        %NPInter & lncRNA-protein & $586 \times 446$ & 18.12\% &\cite{teng2020npinter}\\
        mirTarBase & miRNA-mRNA & $1873 \times 415$ & 7.065\%\\% &\cite{hsu2011mirtarbase, huang2022mirtarbase}\\
        %mirTarBase & miRNA-mRNA & $1873 \times 415$ & 7.065\% &\cite{huang2022mirtarbase}\\
        \bottomrule
    \end{tabular}
    \caption{Summary of the datasets used in this study. The similarity scores for mirTarBase and NPInter were obtained from the raw sequences as their normalized Smith-Waterman alignment scores. %substitution matrices and density filters
    %
    Original references are:
        DPI~\cite{yamanishi2008},
        ERN~\cite{faith2007},
        SRN~\cite{macisaac2006}, %, hughes2000, hu2007, chua2006, schrynemackers2015},
        DAVIS~\cite{davis2011}, %,he2017simboost,huang2020deeppurpose},
        KIBA~\cite{tang2014}, %,he2017simboost,huang2020deeppurpose},
        %NPInter~\cite{wu2006npinter, teng2020npinter},
        NPInter~\cite{teng2020npinter}, 
        %mirTarBase~\cite{hsu2011mirtarbase, huang2022mirtarbase}.
        mirTarBase~\cite{huang2022mirtarbase}. 
        DAVIS and KIBA are provided by \citet{huang2020deeppurpose}. DPI datasets, SRN, and ERN are provided by \citet{schrynemackers2015}.
}
    \label{tab:datasets}
\end{table}


%\subsection{DPI-E, DPI-G, DPI-I, DPI-N~\cite{yamanishi2008}}
\subsubsection{DPI-E, DPI-G, DPI-I, DPI-N}

These datasets comprise drug-protein interactions for four distinct classes of proteins: enzymes, GPCRs, ion channels, and nuclear receptors, respectively. Drug similarities were computed using the SIMCOMP metric, while protein similarities were computed as normalized scores of Smith-Waterman pairwise alignments~\cite{yamanishi2008}.

\subsubsection{ERN and SRN}
%\subsubsection{ERN~\cite{faith2007} and SRN~\cite{macisaac2006, hughes2000, hu2007, chua2006, schrynemackers2015}}
The datasets represent interactions between genes and transcription factors in \textit{E. coli} and \textit{S. cerevisiae}, respectively. Features for genes and trasncription factors are initially composed of experimentally measured expression levels and, in SRN, gene motif features~\cite{brohee2011,schrynemackers2015}. We compute the RBF kernel of such values to obtain the final similarity matrices. %TODO rbf equation or cite

%\subsection{DAVIS~\cite{davis2011,pahikkala2015}}
\subsubsection{DAVIS}
The DAVIS dataset contains experimentally measured drug-kinase dissociation constants~\cite{davis2011}. The dataset was binarized by considering interactions with dissociation constants $\le 30 nM$ as the positive ones, as suggested by \cite{pahikkala2015}. Drug similarities were computed using the Extended Connectivity Fingerprints (ECFP4)~\cite{rogers2005, pahikkala2015} while protein similarities were taken as the normalized Smith-Waterman score~\cite{yamanishi2008,pahikkala2015}.

%\subsection{KIBA~\cite{tang2014,he2017simboost,huang2020deeppurpose}}
\subsubsection{KIBA}

The KIBA dataset was initially built by \citeauthor{tang2014} and contains experimentally verified affinity scores between kinase and kinase inhibitors.

\cite{he2017simboost} further processed the dataset by removing all drugs and targets with less than 10 observations. In alignment with \cite{tang2014,he2017simboost}, we consider positive interactions as those with $log_10$ KIBA-scores $\leq 3.0$ to reframe the task as binary classification.

The utilized version of the dataset with corresponding amino acid sequences and SMILES representations were provided by \cite{huang2020deeppurpose}. From them, we generated the protein similarity matrix using the same procedure employed in the preprocessing of NPInter proteins. The drug similarities were computed similarly to how \cite{pahikkala2015} processed the DAVIS dataset, using the Tanimoto distances of ECFP4 fingerprints~\cite{rogers2005, pahikkala2015}. The Python library \texttt{rdkit}~\cite{rdkit} was used to this calculation.


%\subsection{mirTarBase~\cite{hsu2011mirtarbase, huang2022mirtarbase}}
\subsubsection{mirTarBase}

The mirTarBase dataset contains experimentally validated microRNA-messengerRNA interactions. MicroRNA sequences were obtained from miRBase~\cite{griffiths2006mirbase} while transcript sequences were obtained from GENCODE~\cite{frankish2021gencode}. The longest transcript for each gene was selected and the 3' UTR exonic sequences were recovered from the genome and annotation files provided by GENCODE. The similarity matrices were then built from the normalized Smith-Waterman~\cite{yamanishi2008} alignment scores among microRNAs and among the genes' 3' UTRs. The alignments were performed using the BLASTN substitution matrix and no gap penalty, with the help of the Biopython package~\cite{cock2009biopython}.

Each miRNA was required to have at least 10 interactions in the dataset, and each gene was required to have at least 100 interactions.


% \subsubsection{NPInter~\cite{wu2006npinter, teng2020npinter}}
\subsubsection{NPInter}

Interactions between long non-coding RNAs (lncRNA) and proteins were recovered from NPInter~\cite{wu2006npinter, teng2020npinter}. The lncRNA sequences were obtained from NONCODE~\cite{zhao2016noncode} and the protein sequences were obtained from UniProt~\cite{uniprot2019}. The similarity matrices were built from the normalized Smith-Waterman~\cite{yamanishi2008} alignment scores among lncRNAs and among the proteins. Similarly to the preprocessing of mirTarBase, we utilized the Biopython package~\cite{cock2009biopython} to perform the alignments. using the BLASTN and BLOSUM62 substitution matrices for the lncRNA and protein alignments, respectively, and no gap penalty in both cases.

Each lncRNA was required to interact with 50 proteins or more to be incorporated in the dataset, and each protein was required to have at least 2 interactions.


\subsection{Evaluating bipartite models}
\label{sec:evaluation_protocol}

% stat testing, multiple comparisons, etc

\subsection{Model validation}
\label{sec:cross_validation}

To evaluate machine learning models, the standard procedure consists of separating a subset of data samples not to be used in the training process. These samples are subsequently inputted to the trained model and its known labels are compared to the model's predictions in order to estimate the algorithm performance. The hold-out samples are collectively called the \emph{test set} while the remaining ones used for model building are called the \emph{training set}.

Since bipartite interaction datasets present two distinct categories of instances and the model's input is a pair of them, one from each group, additionally to a traditional "unknown test set" there are two mixed training/test folds possible: we could test our model performance when predicting interactions between instances from $X_1$ that are present in the training set and instances from $X_2$ present in the test set, and vice-versa. Similarly to \ref{pliakos2018}, % TODO: did they invent it?
we name those settings \emph{LT}, after "learned $X_1$, test $X_2$", and \emph{TL}, after "test $X_1$, learned $X_2$". The usual cross-validation setting with completely new test pairs is then called \emph{TT}, and the training set could alternatively be called the \emph{LL} set.

In the present work, we make use of an adapted $k$-fold cross-validation procedure to evaluate our models' performance. With customary datasets formatted as $X_\text{SGSO}$ and $Y_\text{SGSO}$, $k$-fold cross-validation consists in equally and randomly dividing both $X_\text{SGSO}$ and $Y_\text{SGSO}$ together in $k$ non-overlapping partitions (or folds). The model is then evaluated $k$ times, each time selecting a fold as the test set and the remaining ones as the training set (Figure \ref{}).

In the interaction setting though, with a two-dimensional interaction matrix, fold division can be done in each of the two axis, corresponding to each of the two $X_a$ sample groups. Each of the $k_1$ "axis-folds" of $X_1$ can be combined with one of the $k_2$ axis-folds of $X_2$ to make up a $Y$ fold and split the dataset in the corresponding four LL, LT, TL and TT subsets. If all axis-fold combinations are explored, a $k_1$ by $k_2$ two-dimensional cross-validation naturally has a total of $k_1k_2$ folds. 

However, an argument can be made about not sharing axis-folds between $Y$-folds, to ensure all folds are completely independent and no information is shared between models built on each fold. For instance, if a particular $X_1$ axis-fold happens by chance to be unrepresentative of the remaining instances in $X_1$, all $k_2$ folds that include this axis-fold are expected to yield poor prediction scores. A statistical test comparing two of such score populations then would be biased towards considering those $k_2$ anomalously distributed points as a significant difference, while in reality they come from a single stochastic event, not $k_2$ events as could be apparent.

To achieve fold-independence, each fold must be built from a completely different pair of axis-folds, which can be simply done by selecting $k=k_1=k_2$ and pairing each $X_1$ axis-fold with a single $X_2$ axis-fold, yielding a total of $k$ folds, not $k^2$ as when all axis-fold combinations are used (Figure \ref{}). While $k_1\neq k_2$ is still theoretically possible, the total number of folds will always be equal to the least $k_a$ value, and the axis corresponding to the greater $k_a$ would have unexploited axis-folds when creating the test sets.

We refer to the aforementioned two-dimensional cross-validation procedure built from a one-to-one mapping of $k$ $X_1$ axis-folds to $k$ $X_2$ axis-folds as $k$-fold \emph{diagonal} cross-validation.

% TODO: why we don't use it.

In order to maximize the amount of training data in each fold, several studies \ref{} perform LT and TL validation separately from the TT validation, employing 1 by $k$ and $k$ by 1 cross-validation procedures respectively for LT and TL settings. Nevertheless, this requires performing cross-validation three times for each estimator, while TT cross-validation already unavoidably generates LT and TL partitions that could be used for scoring. Furthermore, using separate LT, TL and TT validation procedures hinders score comparison between LT and TT and between TL and TT, since different amounts of training data would be used for validating TT in comparison to validating the partially-learned test sets.

% TODO: consider that X is pairwise! important detail not considered by many


\subsection{Prediction scoring metrics}
\label{sec:prediction_metrics}

% TODO classification

This section is concerned with defining the two metrics used throughout this work to evaluate the predictive performance of an estimator and enable comparison between them. % TODO and explain, justify, etc

Consider a test set of $N$ interaction labels to be inferred by a classifier (we are not concerned with the shape of $Y$ in this section, and $N=|Y|$). Let the classifier's predictions then be represented by a matrix $\hat Y$ of the same shape as $Y$, with $\hat Y\el{ij}$ being the predicted value for the ground-truth label $Y\el{ij}$. Since $Y$ and $\hat Y$ are both binary matrices, there are four possible outcomes when a prediction is made, traditionally quantified~\cite{} as follows:
%
\begin{itemize}
    \item \textbf{True Positives (TP):} the number of positive labels correctly predicted, where both the predicted and actual labels are 1.
    \begin{equation}
        TP = \sum_{i,j} \mathbb{I}(Y\el{ij} = 1 \text{ and } \hat{Y}\el{ij} = 1)
    \end{equation}

    \item \textbf{True Negatives (TN):} the number of negative labels correctly predicted, where both the predicted and actual labels are 0.
    \begin{equation}
        TN = \sum_{i,j} \mathbb{I}(Y\el{ij} = 0 \text{ and } \hat{Y}\el{ij} = 0)
    \end{equation}

    \item \textbf{False Positives (FP):} the number of instances where the predicted label is positive (1), but the actual label is negative (0).
    \begin{equation}
        FP = \sum_{i,j} \mathbb{I}(Y\el{ij} = 0 \text{ and } \hat{Y}\el{ij} = 1)
    \end{equation}

    \item \textbf{False Negatives (FN):} the number of instances where the predicted label is negative (0), but the actual label is positive (1).
    \begin{equation}
        FN = \sum_{i,j} \mathbb{I}(Y\el{ij} = 1 \text{ and } \hat{Y}\el{ij} = 0)
    \end{equation}
\end{itemize}
%
where \(\mathbb{I}(A)\) is the indicator function that equals 1 if statement \(A\) is true and 0 otherwise:
%
\begin{equation} % TODO define in general (sec:definitions) or consider always binary
    \mathbb{I} = \begin{cases}
        1 & \text{if } A\\
        0 & \text{otherwise}
    \end{cases}
    \label{eq:indicator}
\end{equation}
%
Notice that the sum of TP, TN, FP and FN is equal to the total number of instances $T$, and we also define $\text{P}=TP+FN$, the total number of \textit{a priori} positive labels in the test set, and $\text{N}=TN+FP$, the total number of \textit{a priori} negative labels. The total number of predicted positives is termed $\text{PP}=TP+FP$ and the remaining predicted negatives are called $\text{PN}=TN+FN$.%TODO cite
%- \(n\) is the total number of instances.
%- \(Y_i\) represents the ground-truth label for instance \(i\).
%- \(\hat{Y}_i\) represents the predicted label for instance \(i\).

% TODO: we use micro version of the metrics. should we use other forms?

Those quantities can be organized in the so-called \emph{confusion matrix}, illustrated by \autoref{fig:scoring}.
%
Naturally, one wants their estimator to maximize TP and TN while minimizing FP and FN.

Most commonly, we do not use these metrics directly, but instead normalize them to the interval [0, 1] in numerous ways. This enables score comparisons across datasets with different numbers of samples and different densities of positive annotations. Below we list the most common normalized scoring metrics for binary classification problems, from which the metrics used in this work are derived.
%
\begin{itemize}
    \item \textbf{True positive rate (TPR)} or \textbf{recall:} the ratio of correctly predicted positive labels to the total number of positive labels.
    \begin{equation}
        TPR = \frac{TP}{P} = \frac{TP}{TP + FN}
        \label{eq:tpr}
    \end{equation}
    \item \textbf{True negative rate (TNR):} the ratio of correctly predicted negative labels to the total number of negative labels.
    \begin{equation}
        TNR = \frac{TN}{N} = \frac{TN}{TN + FP}
        \label{eq:tnr}
    \end{equation}
    \item \textbf{False positive rate (FPR):} the ratio of incorrectly predicted positive labels to the total number of negative labels.
    \begin{equation}
        FPR = \frac{FP}{N} = \frac{FP}{FP + TN} = 1-TNR 
        \label{eq:fpr}
    \end{equation}
    \item \textbf{False negative rate (FNR):} the ratio of incorrectly predicted negative labels to the total number of positive labels.
    \begin{equation}
        FNR = \frac{FN}{P} = \frac{FN}{FN + TP} = 1-TPR
        \label{eq:fnr}
    \end{equation}
    \item \textbf{Precision:} the ratio of correctly predicted positive labels to the total number of predicted positive labels.
    \begin{equation}
        Pr = \frac{TP}{PP} = \frac{TP}{TP + FP}
    \end{equation}
\end{itemize}
%
These metrics are better visualized in \autoref{fig:scoring}.

An optimal binary classifier will thus present high TPR, TNR and precision while minimizing FPR and FNR.
% TODO note that recall is not affected by PU, point for AUROC

Notice that each of these metrics still allows trivial solutions: if a classifier outputs positive labels for all instances irrespectively of the input features, it will achieve perfect TPR, FNR and Pr, but its TNR and FNR will be null. On the other hand, if a negative label is outputted every time, the opposite will happen.
Thus, we will consider pairs of these metrics simultaneously, such as TPR and TNR.
The selected pair must include at least three of TP, TN, FP, or FN to ensure the whole confusion matrix is taken into account.

Minimizing all the four components of the confusion matrix is not always possible.
%
Most often, the learning algorithms are subject to a tradeoff between the ability to correctly infer positive annotations and the ability to correctly infer negative annotations, that we express as a balance between TPR and TNR. When evaluating models, we must be aware of the relative importances being assigned to each of these tendencies.
%This balance between effective classification of each class is usually formulated as a balance between TPR and TNR.
%
%In real-world scenarios, however, building estimators to simultaneously optimize all these metrics is not a straightforward problem.
%
%In general, classifiers will often lean into one of those directions, displaying a tendency towards yielding more positive
%%(\emph{sensitivity})
%or more negative
%%(\emph{specificity})
%labels.
%
%
This choice is highly application-specific. For instance, in the process of diagnosing medical conditions, TPR is often prioritized~\cite{}, favoring the identification of all true cases at the expense of some misleading positive results.
In this case, the cost of missing a positive result is usually far greater than that of a false positive. In other scenarios such as spam email filtering~\cite{}, TNR might be favored, minimizing the number of legitimate emails marked as spam even if some spam emails go undetected.
%The choice between these tendencies reflects the trade-off between correctly identifying positive instances and correctly identifying negative instances and should align with the specific goals and constraints of the classification problem at hand.

Considering this balance across a variety of learning tasks
is a challenging factor to be taken into account, especially in cases where the estimators can be easily adjusted to favor each class.
%
Many estimators, such as those employed in the present study, do not directly output a binary label but instead provide us with a continuous decision value (such as a probability of interaction) that additionally depends on a threshold parameter to establish the final predicted classes. Formally, the predicted labels $\hat Y$ are obtained from a threshold $t$ applied to the decision values $\tilde Y$:
%
\begin{equation}
    \hat Y\el{ij} = \mathbb{I}(\tilde Y\el{ij} > t)
\end{equation}
%
The selection of $t$ directly affects the propensity to positive or negative outputs, so that a single model can yield multiple different results with varying levels of TPR and TNR depending on the chosen thresholds.
A common practice is then to consider TPR and TNR for all $t$, avoiding the influence of threshold selection on the estimator comparisons.
%Otherwise, the score comparisons could be compromised.

% TODO especially important for our test sets with diffenent density than the training sets
Notice that, since a finite set of outputs is considered for evaluation (the test set), a finite set of thresholds will cover all possible classification results of a model. These results can be easily displayed in a two-dimensional plot, using a point for each considered threshold so that its corresponding TPR and TNR are each indicated by an axis. Conventionally, the TPR is plotted in the $y$ axis while the FPR values (representing the TNR, $FPR=1-TNR$) are presented as $x$ coordinates, which results in the traditional \emph{receiver operating characteristic} (ROC) curve, exemplified by \autoref{fig:roc}.
%
An ideal threshold of an ideal estimator would then be close to the top-left corner of the plot, where TPR and TNR are both 1.
On the other hand, consider a completely random classifier outputting uniformly random values $\hat Y\_\text{proba}\el{ij}$ in the 0-1 interval. We would have $\hat Y\el{ij} = \mathbb{I}(\hat Y_\text{proba}\el{ij} > t)$ for a given threshold $t$. This implies that the number of correctly guessed positive labels is $1-t$ (the probability of yielding 1) times the total number of positive labels: $TP = (1-t) (TP+FN)$, which results in $TPR = 1-t$ from the definition. Similarly, $FPR = TPR = 1-t$, so that the ROC curve of a random classifier is a diagonal line from the bottom-left to the top-right corners.

%$FP = (1-t)(TN + FP)$

% TODO interpretation of roc
% TODO its TPR and TNR balance, not TP and TN

To summarize a classifier's performance across all thresholds, the area under the ROC curve (AUC ROC) is often employed, with values ranging from 0.5 to 1, where 1 represents a perfect classifier and 0.5 represents a random classifier. Although theoretically possible, values below 0.5 would signify the opposite label is being consistently predicted, most likely indicating misconfiguration of the estimator. If AUROC < 0.5, the result can be easily converted to a value greater than 0.5 by simply inverting the predicted labels (turning 0s into 1s and vice-versa).

Despite considerably common, the use of ROC curves requires additional considerations when dealing with heavily imbalanced classification datasets (where some classes are greatly overrepresented)~\cite{he2009learning,saito2015precision,fernandez2018learning}.
For instance,% let $p$ be the fraction of positive labels relative to the total number of labels: $p = \frac{TP + FN}{N}$.
consider the case where $NEG \gg POS$. Since the denominator of TNR is far greater than the denominator of TPR, a change in TN (for example, missing one more negative label) will have a much smaller impact on TNR than a change in TP (missing a positive label) will have on TPR.
Specifically, an increase of $k$ in TN will cause an increase in TNR $\frac{NEG}{POS}$ times greater than the increase in TPR caused by the same $k$ increase in TP.

% Positive labels (the minority class) have more weight

Given that ROC equally considers both metrics, classifiers that are more sensitive to positive labels will arguably be favored over those prioritizing negative outputs. By the same logic, ROC will also be more lenient towards false positives (that decrease TNR) rather than false negatives (that decrease TPR)~\cite{}.

To address this issue, it is commonly suggested the usage of precision-recall (PR) curves instead~\cite{ozenne2015precision,he2009learning}, where the precision is plotted as the vertical coordinate while the \emph{recall} (another name for TPR) is represented horizontally.
%
We can see the FP term in \autoref{} as the proxy for the true negatives (FP = N - TN). Notice that FP in the definition of precision (\autoref{}) is not divided by the total number of negative labels, as TN is in TNR. Thus, it is argued that AUPR is less likely than AUROC to prioritize the minority class in imbalanced scenarios~\cite{ozenne2015precision,he2009learning}.

We explore these claims in further detail in the following section, formally defining the ROC and PR curves in terms of ideal label probability distributions.

%In the PR space, considering fixed P and N, a simultaneous increase in TP and TN results in the following improvement in precision:
%%
%\begin{equation}
%    \Delta Pr =
%    \frac{TP + k}{TP + k + FP - k}
%    - \frac{TP}{TP + FP}
%    = \frac{k}{TP + FP}
%    % \frac{k}{TP + FN}
%    % PR area increase: \frac{k^2}{(TP + FP)(TP + FN)}
%    % PR area increase: \frac{k^2}{(TP + FP)(TP + FN)}
%    % PR area increase: \frac{k^2}{(TP + NEG - TN) POS}
%    % ROC area increase: \frac{k^2}{NEG POS}
%\end{equation}
%%
%which is arguably less sensitive to the strategy of increasing FP to obtain larger TP, in comparison with AUROC. %TODO better explanation

% A formal explanation of the differences between ROC and PR curves is developed in the following section.  % TODO: better?
% The following section delves into those considerations with a higher level of detail, formally defining such metrics in the ideal scenario.

% TODO FP represents TN (NEG - TN), so plot TP vs. FP. Precision and recall are similar for those two metrics.

%TODO at least 3 of the 4 basic metrics must be included

% Changes of precision as a function of TN and TP separately
% \begin{equation}
%     \frac{TP}{TP + FP - k}
%     - \frac{TP}{TP + FP}
%     = \frac{TP k}{(TP + FP)(TP + FP - k)}
%     = \frac{TP k}{(TP + FP)^2 - (TP + FP) k}
% \end{equation}
% 
% \begin{equation}
%     \frac{TP + k}{TP + FP + k}
%     - \frac{TP}{TP + FP}
%     = \frac{(TP + k)(TP + FP) - (TP)(TP + FP + k)}{(TP + FP)(TP + FP + k)}
%     = \frac{(TP^2 + TP*FP + kTP + kFP) - (TP^2 + TP*FP + kTP)}{(TP + FP)(TP + FP + k)}
%     = \frac{k FP}{(TP + FP)(TP + FP + k)}
%     = \frac{k (NEG - TN)}{(TP + FP)(TP + FP + k)}
% \end{equation}


% \begin{equation}
%     \sum_t \Delta TPR_t \Delta FPR_t / 2 + TPR_t * FPR_t
%     % = \sum_t \frac{k_t^2}{NEG * POS} / 2 + \frac{TP_t TN_t}{NEG * POS}
%     = \sum_t \frac{k_t^2}{NEG * POS} / 2 + \Delta TNR_t * TPR_t
%     = \sum_t \sum_t \frac{k_t^2}{NEG * POS} / 2
% \end{equation}
% 
% \begin{equation}
%     TPR = tp / pos
%     FPR = fp / (1 - pos)
%     Precision = tp / (tp + fp)
% \end{equation}
% 
% \begin{equation}
%     pos is not the ideal tp + fn, it is simply POS / N
%     you cannot asume c = ppos
% 
%     TPR = (tp/(tp + fp)) * c / pos = prec * c / pos
%     c = pos * TPR (tp + fp) / tp
% 
%     TPR = c - FPR
% 
%     FPR = fp/(fp + tp) * c / (1 - pos)
%     FPR = fp / (fp + tp) * (pos * TPR * (tp + fp)) / ((1 - pos) * tp)
%     FPR = fp * pos * TPR / ((1 - pos) * tp)
%     TPR = ((1 - pos) * tp) * FPR / (pos * fp)
% 
%     Precision = tp / (tp + fp)
%     Precision = 
% \end{equation}
% 
% \begin{equation}
%     TPR = p * c / pos
%     p = pos * TPR / c
%     Precision = p = pos * TPR / c
% \end{equation}

% qualquer estimador √© descrito por uma matriz de confus√£o te√≥rica. Para qualquer estimador, pode se calib√°-lo, isso √©, usar um par√¢metro c que d√° uma porcentagem de vezes que, aleat√≥riamente, mudamos uma predi√ß√£o negativa para positiva ou vice versa. as curvas s√£o em fun√ß√£o desse par√¢metro. para uma determinada m√©trica agora em um conjunto de dados real, provemos que o melhor c √© o correspondente √° proba √† priori do conjunto (desbalanceamento). O estimador ent√£o pode ajust√°-la de acordo com essa proba que ele acha que √© a certa. a quest√£o √© que ele n√£o tem como saber, ent√£o consideramos todas as probas que ele poderia pensar.

% TODO problems with imbalance
% TODO aupr
% TODO problems with imbalance of auroc maybe are not problems with PU learning

%\section{Common approaches to build bipartite models}
%\label{sec:standard adaptations}

\subsubsection{Ideal descriptions of AUROC and AUPRC}
% TODO previous explanations restrained to specific examples and distant analogies. (hans2009 and davis2006 are nice exceptions)

Consider a general estimator outputting a decision value $s \in \mathbb{R}$ for each input instance. The final class to be assigned is still to be defined by a threshold $s^*$ so that $\hat Y\el{ij} = \mathbb{I}(s\el{ij} > s^*)$. For the ideal case of an infinite number of test samples, the possible scoring results of the estimator would be fully determined by the two theoretical distributions of $s$ given the true label $Y\el{ij}$, i.e. the probability density functions $P(s \mid Y\el{ij} = 1)$ and $P(s \mid Y\el{ij} = 0)$. Similar to \textcite{hand2009measuring}, we thus define the probability density functions $f_k$ and their corresponding cumulative distribution functions $F_k$ for each of the two classes $k \in \{0, 1\}$:
%
\begin{gather}
    f_0(s) = P(s \mid Y\el{ij} = 0) \\
    f_1(s) = P(s \mid Y\el{ij} = 1) \\
    F_0(s) = \int_{-\infty}^s f_0(u)\;du \\
    F_1(s) = \int_{-\infty}^s f_1(u)\;du
    \label{eq:ideal_confusion_matrix}
\end{gather}
%
We further define $p=\frac{P}{T}=P(Y_\text{test}\el{ij}=1)$ and $n=\frac{N}{T}=P(Y_\text{test}\el{ij}=0)$, the fractions of positive and negative labels in the test set, respectively. We can then express the expected values of the confusion matrix as functions of a given threshold $s^*$ for the decision value $s$:
%
\begin{gather}
    \text{TP}(s^*) = p (1 - F_1(s^*)) \\
    \text{TN}(s^*) = n F_0(s^*) \\
    \text{FP}(s^*) = n (1 - F_0(s^*)) \\
    \text{FN}(s^*) = p F_1(s^*)
\end{gather}
%
which, in turn, yields
%
\begin{gather}
    \text{TPR}(s^*) = 1 - F_1(s^*) \\
    \text{TNR}(s^*) = F_0(s^*) \\
    \text{FPR}(s^*) = 1 - F_0(s^*) \\
    \text{FNR}(s^*) = F_1(s^*) \\
    \text{Pr}(s^*) = \frac{p (1 - F_1(s^*))}{p (1 - F_1(s^*)) + n (1 - F_0(s^*))}
\end{gather}

The area under the ROC curve can now be expressed as
%
\begin{equation}
    A_\text{ROC}
    = \int_{0}^{1} \text{TPR}(\text{FPR}) \;d\text{FPR}
    = \int_{\infty}^{-\infty} \text{TPR}(s) \frac{d\text{FPR}(s)}{ds} \;ds
    = \int_{-\infty}^{\infty} (1 - F_1(s)) f_0(s) \;ds
    \label{auroc0}
\end{equation}
%
The integration limits arise from the fact that FPR is maximal when all instances are classified as positives
%i.e. the decision threshold is as low as possible ($s^* \to -\infty$),
and minimal when all instances are classified as negatives, which respective corresponds to $s^* \to -\infty$ and $s^* \to \infty$. This formulation of AUROC by \autoref{eq:auroc0} leads to the most common intuition behind the metric.
%
The first factor ($1- F(s^*)$), represents the fraction of positive instances that are ranked higher than the threshold $s^*$, alternatively expressed as $\int_{s^*}^{\infty}f_1(s)\; ds$. $f_0$, as defined by \autoref{eq:ideal_decision_matrix}, represents the probability of finding a negative instance within $s^*$ and $s^* + ds$. Hence, the product of both factors represents the joint probability of having a negative instance between $s^*$ and $s^* + ds$ while also finding a positive instance with $s > s^\ast$. 
%
After integration over all possible thresholds, we conclude that the AUROC score represents the overall probability of randomly selecting a positive instance ranked higher than a randomly selected negative instance~\cite{}.

The baseline score $0.5$ can be derived as follows. A random classifier is defined as an estimator incapable of distinguishing between the true label distributions of each class. That is, a classifier is a random classifier if and only if $f_0(s) = f_1(s) \forall s$. As a consequence,
%
\begin{equation}
    A_\text{ROC}
    = \int_{-\infty}^{\infty} (1 - F_0(s)) f_0(s) \;ds
    = -\int_{-\infty}^{\infty} (1 - F_0) \;d(F_0)
    = \left[\frac{(1-F_0)^2}{2}\right]_{-\infty}^{\infty}
    = \frac{1}{2}
    \label{auroc_random}
\end{equation}
 
\autoref{eq:auroc0} also shows a characteristic of AUROC discussed in the previous section: the AUROC score is independent of the relative prevalence of each class in the test set. There is no influence of $p$ or $n$ and only the dependency on the learned decision value distributions.
%it can be desirable to account for this relative importance.
% However, in imbalanced datasets, misclassifying instances of the majority class would  TODO
%
%By definition of the training and test sets, the negative test instances are independent and identically distributed to the negative instances of the training set, with the same being held for the positive instances. Under this assumption, the probability density functions $f_0$ and $f_1$ are independent of the label imbalance of the test set, being solely determined by the nature of the learning algorithm and the training data. As a result, 

% TODO maybe:
%%  We can make this property more clear by expressing AUROC in terms of TN and TP directly (\autoref{eq:auroc_tptn}).
%%  Variations of TP and TN directly represent the number of correctly classified instances of each class, unlike the variation of the decision values $ds$ that is often a more abstract concept.
%%  %While the variation of the decision values $ds$ is a considerably abstract concept in real-world scenarios,
%%  %Variations of TP and TN are directly related to the number of correctly classified instances of each class.
%%  %
%%  \begin{equation}
%%      A_\text{ROC}
%%          = \frac{1}{n} \int_{0}^{n} \frac{\text{TP}(\text{TN})}{p}\;d\text{TN}
%%      \label{eq:auroc_tptn}
%%  \end{equation}
%%  %
%%  %Notice how TP and TN are respectively divided by $p$ and $n$ in \autoref{eq:auroc_tptn}. % TODO better?
%%  From \autoref{eq:auroc_tptn}, the AUROC can \emph{almost} be interpreted as the mean value of TP when TP is expressed in terms of TN, for all TN values. In that case $\frac{1}{n}$ is the normalization factor, the length of the integration interval. This only fact defying that interpretation is the division of TP by $p$. As a result, 
%%  %while TN is simply the integration variable,
%%  TP is weighted by $\frac{p}$, so that smaller values of $p$ attribute more importance to TP relative to TN at each point of the ROC curve.
%  
%  This property, however, can be seen as a disadvantage in imbalanced datasets.
%  


% NOTE: I don't think so.
%  This property precisely defines the specific implications of using AUROC for imbalanced datasets presented by \autoref{sec:evaluation_protocol}. In such cases, one must have in mind that misclassifications will effectively be inversely weighted by their respective true class prevalences, with misclassifications of the majority class being more tolerated individually than those of the minority class. In other words, AUROC will be more lenient towards false positives than false negatives if positives are the underrepresented class of a problem.
%  %TODO for an estimator with same TP and TN but different p and n, ...
%  %TODO variational calculus, how to hack the auroc
%  %TODO aucroc is proportional to the expected accuracy
%  
%  If the class distribution of the test set is not an inherent property of the problem under study and the main research interest lies in probing the underlying label distributions $f_0$ and $f_1$, this independence from $p$ may be a desired characteristic of AUROC. However, if the label imbalance is in fact a consistent feature of the learning task, taking it into consideration may report a more realistic portrait of the model performance in future studies. If the positive class is always heavily underrepresented in a given learning task, for example, one may intend to prioritize the correct classification of negative instances, since the overall number of correct classifications will be more influenced by them. More generally, the choice to be made is usually between scoring based on the absolute metrics TP and TN, directly related to the number of samples, or the relative metrics TPR and TNR, relying on fractions of negatives and positives.
%  
%  In scenarios of high class imbalance and where the absolute metrics are the main focus,
%  % TODO maybe remove following
%  i.e. constraining the number of false positives is as important as increasing the number of true positives,
%  it is often recommended the usage of precision-recall (PR) curves as a replacement for ROC~\cite{ozenne2015precision,he2009learning,krawczyk2016learning,fernandez2018learning}.
%  %TODO the precision weights fp as blablabl
%  %XXX

The area under the PR curve (AUPRC) is defined as
%
\begin{multline}
    A_\text{PRC}
    = \int_{0}^{1} \text{Pr}(\text{TPR}) \;d\text{TPR}
    = \int_{\infty}^{-\infty} \text{Pr}(s) \frac{d\text{TPR}(s)}{ds} \;ds
    =\\
    = \int_{-\infty}^{\infty} \text{Pr}(s) f_1(s)\;ds
    = \int_{-\infty}^{\infty} \frac{p (1 - F_1(s)) f_1(s) \;ds}{p (1 - F_1(s)) + n (1 - F_0(s))}
    \label{eq:auprc0}
\end{multline}
%
From \autoref{eq:auprc0}, AUPRC can be interpreted as the average precision weighted by the distribution of positive instances. Analogously, it corresponds to collecting the decision values attributed to each positive instance, and then calculating the average precision considering only these values as classification thresholds.
Furthermore, unlike AUROC, the true label cumulative distributions ($F_0$ and $F_1$) appear each weighted by their respective class prevalences ($p$ and $n$).
%Here it may be useful to bring back the discrete scenario of real classification problems, where each sample of the training set defines a threshold $s*$. With the precision weighted by the positive samples' distribution in \autoref{eq:auprc0}, AUPRC would thus be the average precision considering only the threshold values $s*$ defined by a true positive sample.

% TODO maybe:
%% Furthermore, the true label cumulative distributions ($F_0$ and $F_1$) appear each weighted by their respective class prevalences ($p$ and $n$). As a result, expressing the AUPR in terms of TP and FP yields no dependence on $p$ or $n$ apart from dividing by the length of integration interval $\frac{1}{p}$ that characterizes the averaging procedure:
%% %
%% \begin{equation}
%%     A_\text{PRC}
%%     =
%%         \frac{1}{p}
%%         \int_{0}^{p}
%%             \left(1-\frac{\text{FP}}{\text{TP}}\right)^{-1}
%%         \;d\text{TP}
%%     % =
%%     %     \frac{1}{p}
%%     %     \int_{0}^{p}
%%     %         \left(1-\frac{n-\text{TN}}{\text{TP}}\right)^{-1}
%%     %     \;d\text{TP}
%% \end{equation}
%
% TODO why FP instead of TN write in the previous section
% Another distinguishing characteristic of AUPR is its dependence on FP rather than TN.


\subsubsection{AUPR and AUROC in terms of ranked decision values}
\label{sec:metrics from ranks}

%To better describe the differences between using the absolute metrics and the relative metrics, we can once again bring our theoretical formulation closer to real-world applications.
When computing AUROC or AUPR for a given estimator on a test set, the values of the decision function $s$ are usually not directly considered. Instead, the decision values outputted for each test instance are used to rank them from lowest to highest, and from these ranked test labels the curves and respective areas are obtained. The specific values of $s$ are thus indifferent to the scoring process, as long as the ranking is preserved. If the percentile rank of each test instance is denoted by $r \in [0, 1]$ and each test instance is associated with a decision value, there is a one-to-one monotonic correspondence between $r$ and $s$ in the limit of an infinite number of test samples.
% While $s$ can often represent a rather abstract concept, $r$ is directly related to the number of instances below a given threshold.
Formally, we have
% TODO we take the freedom to represent f(r) = f(s(r))
%
%\begin{equation}
%    f(s) = P(s) = pf_1(s) + nf_0(s)
%\end{equation}
%
\begin{gather}
    r(s) = pF_1(s) + nF_0(s)\\
    dr = [pf_1(s) + nf_0(s)] \; ds
\end{gather}

We also take the liberty to represent $g(r) = g(s(r))$, so that
%Therefore, %$\frac{d F_1(r)}{dr} = \frac{f_1(r)}{pf_1(s) + nf_0(s)}$.
\begin{gather}
    F_1(r) = \int_0^r \frac{f_1(r)}{pf_1(r) + nf_0(r)} \; dr\\
    \frac{d F_1(r)}{dr} = \frac{f_1(r)}{pf_1(r) + nf_0(r)}
\end{gather}
%
%so that a change $dr$ represents a change in the number of instances between $s$ and $s+ds$. 
AUPRC as defined in \autoref{eq:auprc0} can now be written as
%
\begin{multline}
    A_\text{PRC}
    = p \int_{0}^{1}
        %\frac{1 - \int \frac{f_1(r)}{f(r)} \;dr}{(p+n) - [pF_1(r) + nF_0(r)]}
        \frac{1 - F_1(r)}{(p+n) - [pF_1(r) + nF_0(r)]}
        \; \frac{f_1(r)}{[pf_1(s) + nf_0(s)]}
    \;dr
    =\\
    = p \int_{0}^{1}
        \frac{[1 - F_1(r)]}{[1 - r]}\frac{d F_1(r)}{dr}
    \;dr
    %=\\
    = -\frac{p}{2} \int_{0}^{1}
        \frac{1}{1 - r}
        \; \frac{d [1 - F_1(r)]^2}{dr}
    \;dr
    =\\
    =
    - \frac{p}{2}
    \left|
        \frac{[1 - F_1(r)]^2}{1 - r}
    \right|_0^1
    + \frac{p}{2}
    \int_{0}^{1}
        \left[\frac{1 - F_1(r)}{1 - r}\right]^2
    dr
    % =
    % \frac{p}{2} \left\{
    %     1 - \lim_{r \to 1} \frac{[1 - F_1(r)]^2}{1 - r}
    %     + \int_{0}^{1}
    %         \left[\frac{1 - F_1(r)}{1 - r}\right]^2
    %     dr
    % \right\}
    =\\
    =
    \frac{p}{2}
    \left\{
        1 + \int_0^1 \left[\frac{1 - F_1(r)}{1 - r}\right]^2 dr
    \right\}
    =
    \frac{p}{2}
    \left\{
        1 + \int_0^1 [\text{Pr}(r)]^2 dr
    \right\}
    \label{eq:aupr ranks}
\end{multline}
%
% where we define
% %
% \begin{gather}
%     c(r) = \frac{p f_1(r)}{f(r)} = \frac{p f_1(r)}{pf_1(r) + nf_0(r)}\\
%     C(r) = \int c(r) \;dr = p F_1(r)
% \end{gather}
%
% \begin{multline}
%     \int_{0}^{1}
%         \frac{1 - \frac{1}{p}C(r)}{1 - r}
%         \; c(r)
%     \;dr
%     = \int_{0}^{1}
%         \frac{1}{1 - r}
%         \; \left(-\frac{p}{2}\right)
%         \; \frac{d [1 - \frac{1}{p}C(r)]^2}{dr}
%     \;dr
%     =\\
%     =
%     - \frac{p}{2}
%     \left|
%         \frac{[1 - \frac{1}{p}C(r)]^2}{1 - r}
%     \right|_0^1
%     + \frac{p}{2}
%     \int_{0}^{1}
%         \left[\frac{1 - \frac{1}{p}C(r)}{1 - r}\right]^2
%     dr
%     % =
%     % \frac{p}{2} \left\{
%     %     1 - \lim_{r \to 1} \frac{[1 - \frac{1}{p}C(r)]^2}{1 - r}
%     %     + \int_{0}^{1}
%     %         \left[\frac{1 - \frac{1}{p}C(r)}{1 - r}\right]^2
%     %     dr
%     % \right\}
%     =\\
%     =
%     \frac{p}{2}
%     \left\{
%         1 + \int_0^1 \left[\frac{1 - F_1(r)}{1 - r}\right]^2 dr
%     \right\}
%     \label{eq:auprc2}
% \end{multline}
%
in which the upper boundary term is determined by using L'H√¥pital's rule and noticing that $F_1(r=1) = 1$ and
%$\frac{d F_1(r)}{dr} = \frac{f_1(r)}{pf_1(r) + n f_0(r)} \le \frac{1}{p}$.
$\frac{d F_1(r)}{dr} \le \frac{1}{p}$.
%
\begin{equation*}
    \lim_{r \to 1} \frac{[1 - F_1(r)]^2}{1 - r}
    %= \lim_{r \to 1} \frac{- 2 [1 - F_1(r)] c(r)}{- p}
    = 2 \cdot \lim_{r \to 1} \left[1 - F_1(r)\right] \frac{d F_1(r)}{dr}
    % = (2)\lim_{r \to 1} (1 - 1) \frac{d F_1(r)}{dr}
    = 0
\end{equation*}
%
\autoref{eq:aupr ranks} reveals that AUPRC is closely related to the average squared precision across all ranks.
%\autoref{eq:auprc2} shows us that AUPRC, when expressed in terms of percentile rankings, has a simple linear dependency on the positive class prevalence $p$ of the test set. Although not completely independent, this simple relationship makes the comparison of AUPRC scores across test sets with different class distributions a straightforward task. % TODO predictable well-behaved Just a matter of rescaling.

%Another consequence of \autoref{eq:auprc2} is a new interpretation of AUPRC as a linear function over the average \emph{squared} precision across all possible thresholds.

% TODO auroc becomes interesting again for PU

%  The same does not hold for the area under the ROC curve. 
%  
%  % TODO:
%  Consider the ranking of a binary classification test set produced by a given classifier (\autoref{}). Green dots represent positive labels while red dots represent negative labels. A threshold is still to be defined, so the only output of the classifier is the order of test instances represented in the figure, indicating which instances the classifier believes are more likely to belong to the positive class (instances to the right are thought to have higher probability to be positive than instances to the left). Splitting the produced ranking in several equally-sized windows, we call $c\el i$ the average fraction of positive labels in the $i$-th window:
%  %
%  \begin{equation}
%      c \el i = \frac{\text{# of positives in window }i}{\text{window length}\times \text{P}}
%  \end{equation}

%%% TODO: c(r) has a maximum value of 1/p, but it shouldn't depend on p?
%%For a sufficiently large number of samples, $c\el i$ approximates a function $c(r)$ describing the density of positive labels as a function of the classifier's percentile ranking $r$. The probability of classifying a sample between $r$ and $r+dr$ as positive is then given by $c(r)dr$, while $\int_{0}^{1}c(r)\;dr = 1$. Notice that $c(r)$ is independent of the label imbalance of the test set, since it is normalized by the fraction of positive labels $P$ and depends only on the underlying probabilities of predicting a decision value given the true label. For instance, consider a classifier characterized by the distributions% as suggested by \textcite{saito2015precisionrecall}:
%%%
%%\begin{gather}
%%    P(\hat Y\el{ij} = 1 | Y\el{ij} = 1) = \beta(1, 4) \\
%%    P(\hat Y\el{ij} = 1 | Y\el{ij} = 0) = \beta(4, 1)
%%\end{gather}
%%%
%%$c(r)$ will be given by
%%%
%%\begin{equation}
%%    c(r) = P(\hat Y\el{ij} = 1 | r) = \frac{\beta(1, 4)}{\beta(4, 1) + \beta(1, 4)}
%%\end{equation}
%%%
%%as illustrated by \autoref{fig:}.
%%
%%% TODO define confusion matrix
%%Plotting $p \cdot c(r)$ as a function of the percentile ranking offers an interesting way of visualizing the confusion matrix generated by each threshold, since the threshold values naturally have a one-to-one monotonic relationship with the values of $r$ (\autoref{fig:}). Under this representation, we can define
%%%
%%\begin{equation}
%%    tp(r) = \frac{TP}{N} = p\left(1 - \int_{0}^{r} c(r)\;dr\right)
%%\end{equation}
%%\begin{equation}
%%    tn(r) = \frac{TN}{N} = r - p\int_{0}^{r} c(r)\;dr
%%\end{equation}
%%\begin{equation}
%%    fp(r) = \frac{FP}{N} = n - r + p\int_{0}^{r} c(r)\;dr
%%\end{equation}
%%\begin{equation}
%%    fn(r) = \frac{FN}{N} = p\int_{0}^{r} c(r)\;dr
%%\end{equation}
%%%
%%which yields
%%%
%%\begin{equation}
%%    tpr(r) = \frac{tp(r)}{p} = 1 - \int c(r)\;dr
%%\end{equation}
%%\begin{equation}
%%    fpr(r) = \frac{fp(r)}{n} = 1 - \frac{r}{n} + \frac{p}{n}\int c(r)\;dr
%%\end{equation}
%%\begin{equation}
%%    pr(r) = \frac{tp(r)}{1-r} = \frac{p}{1-r} \left(1 - \int c(r)\;dr\right)
%%\end{equation}
%%%
%%As such, the area under the ROC curve (AUC ROC) is now given by
%%%
%%\begin{multline}
%%    A_\text{ROC}
%%    = \int_{0}^{1} tpr \;d(fpr)
%%    = -\int_{fpr(1)}^{fpr(0)} tpr \frac{d(fpr)}{dr} \;dr
%%    %= \int_{1}^{0} \left(1 - \int c(r)\;dr \right) \frac{1}{n} \left(-1 + p\,c(r)\right) \;dr
%%    =\\
%%    = \int_{0}^{1} \left(1 - \int c(r)\;dr \right) \frac{1}{n} \left(1 - p\,c(r)\right) \;dr
%%    \label{eq:auroc1}
%%\end{multline}
%%%
%%\autoref{eq:auroc1} leads to the most commonly promoted intuition behind AUROC. Notice that the analogous of the function $c(r)$ for the negative class can be found with 
%%$\frac{1}{n} \left(1 - p\,c(r)\right)$, precisely the last factors of \autoref{eq:auroc1}.
%%%
%%The first factor ($1- \int c(r)\;dr $), on the other hand, represents the fraction of positive instances that are ranked higher than the threshold $r$.
%%%
%%As a consequence, both factors combined represent the probability of finding a negative instance between $r$ and $dr$ while also finding a positive instance with a higher rank than that of the negative instance.
%%%
%%Upon integration over all possible thresholds, we conclude that the AUC ROC score represents the overall probability of randomly selecting a positive instance ranked higher than a randomly selected negative instance.

We can also express the AUROC in terms of the percentile ranks:
%
\begin{multline*}
    A_\text{ROC}
    = \frac{1}{n} \int_{0}^{1}
        (1 - F_1(r)) \left(\frac{n f_0(r)}{nf_0(r) + pf_1(r)}\right) \;dr
    =\\
    =  \frac{p}{n} \int_{0}^{1}
        (1 - F_1(r)) \left(1 - \frac{pf_1(r)}{nf_0(r) + pf_1(r)}\right) \;dr
    =\\
        = \frac{1}{n} \int_{0}^{1} (1 - F_1(r)) \left(1 - p \frac{dF_1(r)}{dr}\right) \;dr
    =\\
    =
        \frac{1}{n} \int_{0}^{1} (1 - F_1(r)) \;dr
        - \frac{p}{n} \int_{0}^{1}
            (1 - F_1(r)) \;d(F_1(r))
    =\\
    =
        \frac{1}{n} \int_{0}^{1} (1 - F_1(r)) \;dr
        + \frac{p}{n} \left[
            \frac{(1 - F_1(r))^2}{2}
        \right]^1_0
    =\\
    =
        \frac{1}{n} \left\{
            \int_{0}^{1} (1 - F_1(r)) \;dr - \frac{p}{2}
        \right\}
    \label{eq:auroc ranks}
\end{multline*}

Equations \ref{eq:aupr ranks} and \ref{eq:auroc ranks} put AUPR and AUROC in a similar format, better delineating the differences between the two metrics. Consider expressing both now in terms of the precision.
%
\begin{gather}
    A_\text{ROC} = \frac{1}{n} \left\{
        \frac{1}{p}\int_{0}^{1} (1 - r)\text{Pr}(r) \;dr - \frac{p}{2}
    \right\}\\
    A_\text{PR} = \frac{1}{2} \left\{
        p + \frac{1}{p}\int_0^1 \text{Pr}(r)^2 dr
    \right\}
\end{gather}
%
%\begin{gather}
%    A_\text{ROC} = \frac{1}{n} \left\{
%        \int_{0}^{1} (1 - F_1(r)) \;dr - \frac{p}{2}
%    \right\}\\
%    A_\text{PR} = \frac{p}{2} \left\{
%        1 + \int_0^1 \left[\frac{1 - F_1(r)}{1 - r}\right]^2 dr
%    \right\}
%\end{gather}
%
%\begin{gather}
%    A_\text{ROC} = \frac{1}{n} \left\{
%        \int_{0}^{1} \text{TPR}(r) \;dr - \frac{p}{2}
%    \right\}\\
%    A_\text{PR} = \frac{p}{2} \left\{
%        1 + \int_0^1 \left[\frac{\text{TPR}(r)}{1 - r}\right]^2 dr
%    \right\}
%\end{gather}

% As a function of F_0:
%
% \begin{gather}
%     A_\text{PR}
%     = \frac{p}{2} \left\{
%         1 + \int_0^1 \left[\frac{\text{TPR}(r)}{1 - r}\right]^2 dr
%     \right\}
%     =\\
%     = \frac{1}{2} \left\{
%         p + \frac{1}{p}\int_0^1 \left[\frac{p - pF_1(r)}{1 - r}\right]^2 dr
%     \right\}
%     = \frac{1}{2} \left\{
%         p + \frac{1}{p}\int_0^1 \left[\frac{p - r + nF_0(r)}{1 - r}\right]^2 dr
%     \right\}
%     =\\
%     = \frac{1}{2} \left\{
%         p + \frac{1}{p}\int_0^1 \left[\frac{1 - r - n(1 - F_0(r))}{1 - r}\right]^2 dr
%     \right\}
%     =\\
%     = \frac{1}{2} \left\{
%         p + \frac{1}{p}\int_0^1 \left[1 - \frac{n(1 - F_0(r))}{1 - r}\right]^2 dr
%     \right\}
% \end{gather}
%
% Using precision, also interesting:
%
%
%Ignoring constant terms and factors, both metrics are centered on integrating the TPR over all possible ranks, each rank representing a classification threshold. The crucial difference is that AUROC equally considers all TPR values, while AUPR weights each TPR value by the inverse of the reversed ranks.
Ignoring constant terms and factors, both metrics are centered on integrating the precision over all possible ranks, each rank representing a classification threshold. The crucial difference is that AUPR equally considers all precision values, while AUROC weights each $\text{Pr}(r)$ value by the the reversed ranks.

%Notice that $\text{Pr}(r^\ast)$ for a given $r^\ast$ sums $f_1(r)$ over all $r > r^\ast$ values, only considering the ranks higher than $r^\ast$. As a result, the $r$ values closer to $1$ will have a larger influence when integrating over the precisions (a sum of sums of $f_1$), since $r\approx 1$ will be considered for a larger number of terms.

The precision metric is normalized by the number of instances it considers, so precision values in different $r$ have comparable magnitude.
%
With this in mind, notice that the precision values calculated for $r\approx 1$ are obtained from a very small number of labeled instances. As a result, each label considered when $r \approx 1$ has a large influence on the $Pr(r)$ value, and precisions will tend to have larger variances when $r\approx 1$.
%
Therefore, the AUPR metric is more influenced by the label value of the highest ranked instances, since it equally considers all precision values.

Additionally, the quadratic exponent of AUPR's integrand emphasizes higher precision values overall, independently of the number of samples on which they were calculated ($1 - r$). This would amplify the effect of high $Pr(r)$ values for $r \approx 1$ resulting from stochastic label variations.
Hence, we suggest that the AUPR metric is more prone to noise in the labels of higly ranked samples, prioritizing models that strictly maintain high precision for a smaller selection of highest ranks.

%Notice that the TPR reduces as $r$ is increased (\autoref{eq:}). Therefore, equally considering all TPR values in the integration interval naturally prioritizes ranks that are closer to 0.
%TODO:
%Another formulation is possible by expressing AUROC in terms of the precision. When $r$ is closer to $0$, $Pr(r)$ is calculated over a larger number of samples, so that . AUROC emphasizes the precision values
%
%On the other hand, AUPR compensates this effect, dividing each TPR value by $1 - r$. 
%At first sight, one could think that this would lead to considering all ranks on a mo
%
%Each TPR(r) thus will have the same weight on the score regardless of $r$.
%  In this case, notice that the precision values calculated for $r\approx 1$ are obtained from a very small number of labeled instances. As a result, the $Pr(r)$ values for $r\approx 1$ will tend to have larger variances.
%  Furthermore, the quadratic exponent of AUPR's integrand emphasizes higher precision values overall, independently of the number os samples on which they were calculated. This would amplify the effect of high $Pr(r)$ values for $r \approx 1$, that could be primarily resulting from stochastic variations.
%  Therefore, we hypothesize that the AUPR metric is more prone to stochastic variations in the labels of higly ranked samples, prioritizing models that strictcly maintain high precision for a smaller selection of higher ranks.
% Even though they receive the same weight as the TPR values for larger $r$.
%
%
%Therefore, the AUPR metric emphasizes the effect of the highest ranked instances. 
%
%Additionally, the square exponent in AUPR's integrand further prioritizes high values of $\frac{\text{TPR}(r)}{1-r}$, likely reinforcing the effect.
%
% In other words, AUPR's penalty for having false negatives is much higher when the rank is also high ($\text{TPR} = 1 - \text{FNR}$).
%
% Keeping the same number of selected instances, FPR = PPOS - TPR
% Therefore, AUPR tends to ensure that the highest ranked instances are indeed true positives,
%On the other side, AUROC is more concerned with precision values calculated for higher ranks, being more likely to allow some highly-ranked false positives.
On the other side, AUROC compensates this effect by weighting each $\text{Pr}(r)$ value by $1 - r$, so that precision values calculated for lower ranks are prioritized. The prioritized precision values ($r\approx 0$) are calculated from a larger number of labels, having lower variances and being more robust to label noise. Each label considered when $r \approx 0$ has a smaller influence on the $Pr(r)$ value relative to the other labels being considered. However, AUROC assigns a larger weight to these precision values, so the influence of smaller ranks also tend to increase.

%is more concerned with precision values calculated for higher ranks, being more likely to allow some highly-ranked false positives.
%This effect is more pronounced if only a few positive annotations are present in the test set, since it is probabilistic easier to achieve higher TPR values for 

% MOVED TO RESULTS
%Overall, using AUPR is more suited when one is interested in selecting a restricted number of top-ranked instances from a pool of predictions, such as in recommendation systems or drug discovery tasks.
%Conversely, AUROC should be favored when the goal is to rank a large batch of interactions. Examples would be modeling genetic interactions in a genome-wide fashion or building interaction databases.
%
%We also argue that AUROC could be preferable for comparing models under the PU assumption, at least in purely theoretical settings. For PU datasets, we naturally expect some negative-labeled instances to be very highly ranked since they could be, in fact, unannotated positives. AUPR would more strictly penalize such predictions, favoring models that consider the labeling mechanism itself rather than only the underlying interaction mechanism. AUPR could thus undermine the model's potential to discover new interactions, likely failing to gauge the generalization capabilities of algorithms in a PU context.
%
%
%%AUPR, for instance, could emphasize a model that simply predicts higher probabilities for more frequently interacting samples
%
%Conversely, the importance of AUPR lies in applied scenarios where selecting false positives could be costly or have significant negative impacts. For instance, when selecting a small number of drug candidates for further testing.

%Even in this scenario, if one has beforehand the number $k$ of candidates to be selected, one could consider $k$ when building new estimators for the task. For instance, using the precision on the best $k$ probabilities could be more informative than the AUPR or AUROC scores, ensuring that the same importance is given to all the $k$ candidates.  % TODO: this is not a good idea, too volatile and failing to generalize

This characteristic of AUROC might be more suitable for model comparison under the Positive-Unlabeled (PU) assumption, even though AUPR is usually reccomended for imbalanced scenarios~\cite{he2009learning,saito2015precision,fernandez2018learning}.
In PU datasets, it is common for negative-labeled instances to rank highly due to the possibility of them being unannotated positives. These highly-ranked negatives would have a larger influence on the AUPR score in comparison to the AUROC score.
In fact, we show in the next section that AUROC is closely related to the Mean Percentile Rank metric, which has been suggested for PU learning contexts of interaction prediction and recommendation systems~\cite{pahikkala2015more,johnsonlogistic}. \autoref{sec:comparing auroc aupr} presents further discussion on the specific usecases of AUPR and AUROC, besides a numerical analysis of their dependence on the percentile ranks.


% TODO indeed the results tend to change less with AUROC across ILR in comparison to AUPR
% TODO which metric is better for PU
% TODO what are the differences in each metric
% if we have the number of prediction, eavluate it


\subsubsection{AUROC is the normalized mean percentile ranks}
\label{sec:auroc mpr}

From \autoref{eq:auroc ranks}, we can express AUROC as
% Defining $C(r) = \int c(r) \; dr$, we can write
%
\begin{multline}
    A_\text{ROC}
        = \frac{1}{n} \left\{
            \int_{0}^{1} (1 - F_1(r)) \;dr - \frac{p}{2}
        \right\}
    =\\
        = \frac{1}{n} \left\{
            1 - \int_{0}^{1} F_1(r) \;dr - \frac{p}{2}
        \right\}
    %=\\
        = \frac{1}{n} \left\{
            1 - \int_{0}^{1} \frac{d(r)}{dr}F_1(r) \;dr - \frac{p}{2}
        \right\}
    =\\
        = \frac{1}{n} \left\{
            1 - \left[rF_1(r)\right]_0^1 + \int_{0}^{1} r \frac{d F_1(r)}{dr} \;dr - \frac{p}{2}
        \right\}
    %=\\
        = \frac{1}{n} \left[
            \int_{0}^{1} r \;d F_1 - \frac{p}{2}
            %\int_{0}^{1} r \frac{d F_1(r)}{dr} \;dr - \frac{p}{2}
            %\int_{0}^{1} r \tilde f_1 \;dr - \frac{p}{2}
        \right]
    \label{eq:auroc mpr}
\end{multline}
%
\autoref{eq:auroc mpr} offers another perspective on AUC ROC. The term $\int_{0}^{1} r\,d F_1(r)$ represents the expected percentile rank of the positive samples:
%
\begin{equation}
    \int_{0}^{1} r\,d F_1(r) = E[r \mid y = 1] = \text{MPR}
    \label{eq:mpr}    
\end{equation}
%
% TODO XXX moment of inertia
% TODO roc is again interesting for PU data, MPR
%
%The term $\int_{0}^{1} r\,c(r) \;dr$ represents the average percentile ranking of positive labels, or the average distance of a positive label from the origin in the plot of $c(r)$. This quantity by itself, the mean percentile ranking (MPR), is a common metric in the realm of collaborative filtering for recommendation systems~\cite{hu2008collaborative,johnsonlogistic} and has also been proposed to bipartite interaction prediction~\cite{ezzat2019computational,hao2019opensource,yu2020fpscdti}.
%or the average distance of a positive label from the origin in the plot of $c(r)$.
This quantity is sometimes referred to as the \emph{mean percentile ranking} (MPR) in the previous literature~\cite{johnsonlogistic}, being proposed in contexts of recommendation systems~\cite{hu2008collaborative,johnsonlogistic} and bipartite interaction prediction~\cite{ezzat2019computational,hao2019opensource,yu2020fpscdti}.
%
%\begin{equation}
%    \text{MPR} = \int_0^1 r\,c(r) \;dr
%\end{equation}
%
%At first glance, MPR seems to offer some advantages over AUROC: besides simpler in formulation, MPR also seemingly does not depend on $p$ or $n$, the relative amounts of postive and negative labels in the test set, making comparisons across different test sets more direct. However, consider the maximum and minimum values of MPR, achieved for the ideal $c(r)$ distributions:
%

Consider now the maximum and minimum values of MPR, achieved, respectively, for the ideal $f_1(r)$ distributions:
%
\begin{gather}
    f_{1, \text{max}}(r) = \frac{1}{p}\mathbb{I}(r > n)\\
    f_{0, \text{max}}(r) = \frac{1}{n}\mathbb{I}(r < n)\\
    f_{1, \text{min}}(r) = \frac{1}{p}\mathbb{I}(r < p)\\
    f_{0, \text{min}}(r) = \frac{1}{n}\mathbb{I}(r > p)
\end{gather}
%
Applying these definitions to determine $\frac{dF_1(r)}{dr}$ and using the results in \autoref{eq:mpr}, we obtain
%
%%\begin{gather}
%%    \frac{d F_1(r)}{dr} = \frac{f_1(r)}{pf_1(r) + nf_0(r)} \le \frac{1}{p}\\
%%\end{gather}
%
%\begin{gather}
%    c_\text{max}(r) = \frac{\mathbb{I}(r > n)}{p}\\
%    c_\text{min}(r) = \frac{\mathbb{I}(r < p)}{p}
%\end{gather}
%
% The extrema of MPR do in fact depend on $n$:
% %
% \begin{equation}
%     \text{MPR}_\text{max}
%         = \int_(1-p)^1 r\;dr
%         = \frac{1 - n^2}{2p}
%         = \frac{1 + n}{2}
% \end{equation}
% %
% \begin{equation}
%     \text{MPR}_\text{min}
%         = \frac{1}{p}\int_0^p r \;dr
%         = \frac{p^2}{2p}
%         = \frac{p}{2}
%         = \frac{1 - n}{2}
% \end{equation}
%
% If we then try to remedy this issue by normalizing MPR to the $[0, 1]$ interval, we obtain
%
\begin{gather}
    \text{MPR}_\text{max}
        = \frac{1}{p} \int_n^1 r\;dr
        = \frac{1 - n^2}{2p}
        = \frac{1 + n}{2}
    \\
    \text{MPR}_\text{min} = \frac{1}{p} \int_0^p r\;dr = \frac{p}{2}
\end{gather}
%
from which is straightforward to show that
%
\begin{equation}
    A_\text{ROC}
        = \frac{
            \text{MPR} - \text{MPR}_\text{min}
        }{
            \text{MPR}_\text{max} - \text{MPR}_\text{min}
        }
\end{equation}
%
Therefore, the AUROC can also be interpreted as the normalized MPR.
To the best of our knowledge, this relationship is not clearly shown in previous explorations.  % TODO

This result corroborates the argument that the AUROC could be preferable for PU learning scenarios, since the closely-related MPR is a known metric specifically recommended for this context~\cite{pahikkala2015more,johnsonlogistic}.
%
Furthermore, the normalization enables the comparison across different tasks, ensuring that the scores on each dataset are always in the same range. Hence, the AUROC should be preferred over the MPR for the majority of cases.
%This is especially valid in our test configurations, % TODO dropout

%The normalization makes the comparison across different datasets
%
%precisely the expression for the AUROC (\autoref{eq:auroc}). To the best of our knowledge, we thus show for the first time that employing MPR as a metric is equivalent to using AUROC normalized to the $[0, 1]$ interval. Since the statistics for model comparison employed in this study consider only the order of the estimators based on their scores and not the score values themselves, MPR results would not differ in any way from AUROC and hence will not be further considered in this work.

%TODO can borrow from physics the intuition of "moment of inertia"~\cite{}. It would represent the 


\subsection{General experimental settings}



\section{Experiments}

\subsection{What are the differences between AUROC and AUPR?}
\label{sec:comparing auroc aupr}

\begin{mdframed}
    %\textbf{How do AUROC and AUPR differ in their assessment of model performance?}
    \textbf{Key findings:}
    \begin{itemize}
        \item AUPR prioritizes a smaller number of highest-ranked interactions, while AUROC considers a larger number of both highest and lowest ranks.
        %\textbf{When to use each metric?}
        \item AUPR should be used when the goal is to select a small number of most-likely interactions. AUROC should be used i) when both likely-positive and likely-negative interactions are important; or ii) when interested in a large fraction of the predictions.
        \item AUROC could be also preferable for PU learning.
    \end{itemize}
\end{mdframed}

This experiment was designed to evaluate the hypotheses raised in \autoref{sec:metrics from ranks}. Our main objective is to elucidate how much importance each metric assigns to each percentile rank. To do so, we perform a Monte Carlo simulation. First, we select $R$ equally spaced rank values between $0$ and $1$ (excluding $0$ and $1$). For each rank, we generate $N$ random binary values to be used as labels. Therefore, each of the $N$ iterations of the simulation will produce a random set of $R$ binary labels, corresponding to each percentile rank. In each iteration, we use the $R$ labels and $R$ percentile ranks to calculate AUROC and AUPR, resulting in $N$ values for each metric. Finally, for each of the $R$ ranks, we calculate the point biserial correlation~\cite{kornbrot2014point} between the $N$ random labels and the $N$ AUROC and AUPR values. The result is displayed in \autoref{fig:correlation ranks}.

% TODO p-values

\begin{figure*}[bth]
    \includegraphics[width=\textwidth]{
        experiments/theoretical_scoring/results/rank_correlation.pdf
    }
    \caption{
        Point biserial correlation between the binary labels at each percentile rank and the AUROC and AUPR scores. The correlation was calculated between the ranks and the scores for $N=10^5$ random sets of $R=10^3$ binary labels. The results show that AUPR is very sensitive to a small group of highest ranks, while AUROC has a more distributed weigthing profile. For AUROC, both higher and lower ranks have a higher impact than ranks around $0.5$, contributing in opposite directions to the final metric. The results confirm the theoretical analyses from \autoref{sec:metrics from ranks}.
    }
    \label{fig:correlation ranks}
\end{figure*}

The plot shows that AUPR is highly sensitive to a small set of highest ranks, while ranks closer to $0$ have little influence on the metric. On the other hand, the correlations with the AUROC score are more distributed across ranks. Also, for AUROC, both higher and lower ranks have a higher impact, but in opposite directions.
This behaviour in agreement with the theoretical analyses from \autoref{sec:metrics from ranks}.
The correlation with AUROC is also symmetrical around the median ($r=0.5$), linearly increasing from arround $-0.05$ at $r=0$ to around $0.05$ in $r=1$. This demonstrates the class symmetry of the AUROC metric described in \autoref{sec:metrics from ranks}, meaning that negative and positive labels could be swapped without affecting the final results.
% TODO show that

Overall, these results demonstrate that using AUPR is more suited when one is interested in selecting a restricted number of top-ranked instances from a pool of predictions, such as in recommendation systems or drug discovery tasks.
Conversely, AUROC should be favored when the goal is to rank a large batch of interactions. Examples would be modeling genetic interactions in a genome-wide fashion or building interaction databases.

If one intends to select a small number of negative instances instead, a possible strategy would be to swap the binary labels and use AUPR.
However, one should prioritize AUROC over AUPR if the goal is to select both negative and positive predictions with the highest confidences. This conclusion results from AUPR disregarding ranks close to $0$, so models selected with AUPR are not guaranteed to yield the most confident true negatives.

We also argue that AUROC could be preferable for comparing models under the PU assumption, at least in purely theoretical settings. For PU datasets, we naturally expect some negative-labeled instances to be very highly ranked since they could be, in fact, unannotated positives. AUPR would more strictly penalize such predictions, favoring models that consider the labeling mechanism itself rather than only the underlying interaction mechanism. AUPR could thus undermine the model's potential to discover new interactions, possibly failing to gauge the generalization capabilities of algorithms in a PU context. The results from \autoref{sec:comparison literature} seem to corroborate this hypothesis, with AUROC ranking estimators more consistently than AUPR across different levels of label noise.


\subsection{Are BGSO models indeed faster than GMO models?}
%\subsection{Empirical time complexity analysis}
\label{sec:empirical_complexity}

\begin{mdframed}
    \textbf{Key findings:}
    \begin{itemize}
        \item The GMO complexity is measured to be $O(n^3\log n)$, while the BGSO complexity is measured to be $O(n^3)$.

        \item The empiral results match the theoretical expectations from \autoref{sec:complexity_analysis}.
        
        \item The difference in complexity between GMO and GSO is less pronounced for the ExtraTrees, but the GSO models still present significantly lower complexity than the GMO models.
    \end{itemize}
\end{mdframed}

This experiment empirically measures the training time complexity of the tree models under study. We artificially generate a series of bipartite datasets by filling three $n$ by $n$ matrices with pseudo-random values, representing the two $X$ matrices and the $Y$ matrix on each interaction. Values were taken uniformly from the interval $[0, 1]$ for the feature matrices and from the interval $[0, 100]$ for the target matrix.
%We thus represent interactions between $n$ drugs and $n$ proteins, each being described by $n$ features.

We then train the GMO and the optimized GSO versions of a single bipartite decision tree (BDT) and a single bipartite ExtraTree (BXT) on each of the generated datasets, measuring their training duration in seconds. The results are shown in Figure \ref{fig:empirical_complexity}. From the least squares linear regression on the log-log plot, we see that the estimated training time complexities closely follow the theoretical expectations developed under Section \ref{sec:complexity_analysis}, with slopes referring to the GSO models (predicted to be $O(n^3)$) approaching 3 while the GMO models (predicted to be $O(n^3\log(n))$) produce slope between 3 and 4.

Statistical testing further shows that the empirical time complexity of the proposed GSO algorithms are indeed significantly lower than that of their GMO counterparts (see the caption for Figure \ref{fig:empirical_complexity}).

% The value slighly above 3 for bdt\_gso also do not defy expectations, since the theoretical analises were based on the assumption of balanced trees and a small increase in tree depth is expected in real scenarios.

The slightly lower slopes for the ExtraTrees in comparison both with the BDTs and with the theoretical complexities are also expected, since the bottleneck calculation for these models in the asymptotic regime is the search for the minimum and maximum values of each feature in each node (lines \ref{}), which can be done much faster than the search for the best split employed by the greedy decision trees (\autoref{alg:find_best_split}), even though both procedures have the same order of asymptotic complexity. As such, much larger datasets would be required to observe the asymptotic behavior of the ExtraTrees.
In spite of that, the empirical complexity of \texttt{bdt\_gso} is still observed to be highly significantly lower than that of \texttt{bxt\_gmo}, validating once more the prediction that \texttt{bdt\_gso} should present faster training times than \texttt{bxt\_gmo} on sufficiently large datasets.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/empirical_complexity/time_vs_n_artificial_data.pdf
        }
        \caption{Training durations in seconds versus the the number of samples and features.}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/empirical_complexity/%
            time_vs_n_artificial_data_loglog.pdf
        }
        \caption{Logarithm of the training durations in seconds versus the logarithm of the number of samples and features.}
    \end{subfigure}
    \caption{
        Empirical time complexity estimation of the proposed bipartite global single-output (BGSO) and the global multi-output (GMO)~\cite{pliakos2018}
        algorithms. Bipartite versions of both extremely randomized trees~\cite{geurts2006extremely} (BXT) and greedy decision trees~\cite{breiman1984} (BDT) were built under the GSO and GMO scheme and trained over artificial datasets of varying numbers of samples (as described in section \ref{sec:complexity_analysis}). Applying least-squares linear regression to the logarithm of the values in (a), as shown in (b), the empirical slopes and respective standard deviations are obtained.
        Independent two-sample t-tests comparing the slope estimates reveal that the time complexity of \texttt{bdt\_gso} is highly significantly lower than \texttt{bdt\_gmo} (p-value $< 10^{-64}$) and even \texttt{bxt\_gmo} (p-value $< 10^{-20}$), and also that \texttt{bxt\_gso} significantly exhibits lower complexity than \texttt{bxt\_gmo} (p-value $< 10^{-22}$). Those values
        corroborate the theoretical estimates from section \ref{sec:complexity_analysis}.
    }
    \label{fig:empirical_complexity}
\end{figure}


% \subsection{Comparison between GSO models}  % TODO
% 
% To assess the impact of global single-output optimizations in bipartite decision tree growing, we compare three slightly different training methods for BXT and BRF models.
% 
% \begin{itemize}
%     \item \textbf{ngso}: Naive global single output implementation (Section \ref{});
%     \item \textbf{ngsous}: Naive global single output implementation with undersampling of the non-interacting pairs to yield a balanced training set (Section \ref{});
%     \item \textbf{gso}: Optimized implementation of global single output trees (Section \ref{}).
% \end{itemize}
% 
% While no significant divergence was measured among the GSO models using the entirety of the training data, undersampling revealed to significantly degrade the predictive performance of both forests in terms of AUPR and MCC (Figure \ref{fig:cdd_gso_models}), even though it is arguably the most common procedure when dealing with this kind of data \cite{}.
% 
% On the other hand, AUROC is significantly improved by the undersampling procedure, which is most likely an artifact of the highly imbalanced nature of the present data, as explained as follows. The models grown on the undersampled datasets are naturally the most likely to assign positive labels to new interactions in general, improving TPR at the expense of also increasing FPR. However, since negative labels greatly outnumbers positive labels in the test sets of our current scenario, an increase in FPR impacts a much larger number of predictions than the same increase in TPR. In spite of that, AUROC equally treats TPR and FPR, so that the impact of a high FPR is underestimated. As such, AUROC results could be deemed as unrepresentative of model performance in this setup.
% 
% 
% % TODO: talk about pairwise CV
% 
% When comparing training times, the common choice for undersampling in previous works is justified, as an expressive reduction of training time is observed for both forests (Table \ref{}) relative to naive GSO training. Nevertheless, it is remarkable that the optimized implementation of GSO forests achieves similar training times in comparison to undersampled GSO without the AUPR and MCC burden of undersampling, keeping the higher scores resulting from employing the entirety of the dataset. For larger and less imbalanced datasets, the optimized implementation of GSO forests is expected to be even more advantageous, in agreement with the theoretical time complexity analysis (Section \ref{}).
% 
% %\begin{table}[h]
% %    \input{
% %        figures/experiments/gso_optimization/%
% %        latex_tables/fit.tex
% %    }
% %\end{table}
% 
% In conclusion, the proposed approach confidently enables the use of the entire training data in a much shorter time frame than naive implementations without the need for data undersampling, which is statistically expected to yield better prediction scores for forest predictors.


\subsection{Which prototype should a GMO forest use?}
%\subsection{Comparison between GMO prediction weights}

\label{sec:prototype_comparison}
%TODO: why we did not use BGSO here?

\begin{mdframed}
    \textbf{Key findings:}
    \begin{itemize}
        \item The \texttt{square} strategy is the best for both BXT and BRF models, except for the LT+TL average precision score, for which the fully-grown trees are the best.
        \item The weighted-neighbors strategies seem to improve generalization.
    \end{itemize}
\end{mdframed}

In this work we propose a different prototyping strategy to determine the output value of each leaf in a GMO decision tree, taking the similarity matrices of our use cases into consideration (\autoref{sec:prototype}).
%
In this experiment we compare such strategies, building BXT and BRF models for every option. The minimum rows per leaf and minimum columns per leaf were both set to 5, ensuring that at least 5 samples of each domain are considered when calculating the prototype values.
%in a weighted-neighbors fashion during evaluation (Section \ref{}).
%
To observe the effect of this early-stopping criterion by itself, we also include forests of fully-grown trees in the comparison. The compared models are described below.

\begin{itemize}
    \item \textbf{\texttt{gmosa}:} proposed by \textcite{pliakos2018global}, the output of each leaf is the average of the labels of the learned samples that reach that leaf (\autoref{eq:prototype1}).
    \item \textbf{\texttt{uniform}:} also proposed by \textcite{pliakos2018global}, the only difference from the \texttt{gmosa} strategy occurs in TL or LT test settings, when the average is taken only among the labels of the known sample of the pair being predicted (\autoref{eq:prototype2}).
    \item \textbf{\texttt{precomputed}:}
    %introduced by us (as also \texttt{square} and \texttt{softmax}),
    the labels in each leaf are weighted by the similarities between the learned samples and the pair being predicted (\autoref{eq:prototype3}).
    \item \textbf{\texttt{square}:} the labels in each leaf are weighted by the squared similarities between the learned samples and the pair being predicted (\autoref{eq:prototype3}).
    \item \textbf{\texttt{softmax}:} the labels in each leaf are weighted by the exponential of the similarities between the learned samples and the pair being predicted (\autoref{eq:prototype3}).
    \item \textbf{\texttt{full}:} the trees are grown until a single interaction remains in each leaf. %, so that the prediction for each pair is the label of the single learned sample in the leaf.
\end{itemize}

%As shown by \autoref{fig:pred_weights_brf} and Table \ref{}, BXT models show an overall superior performance in comparison to BRF models, with each BXT model scoring significantly higher than its BRF counterpart with the same prediction weights. Furthermore, the weighted GMO predictions seem to prevail relative to the leaf-wise prototype GMOSA (Section \ref{}). Specifically, bxt\_square significantly outperforms all other bipartite forests except for bxt\_precomputed, both in terms of AUROC and average precision (AUPR) in the TT sets (Figure \ref{}).

The results for BRF (\autoref{fig:pred_weights_brf}) and BXT (\autoref{fig:pred_weights_bxt}) were similar. In all cases except LT+TL average precision, using the square of the similarities to weight the labels in each leaf resulted in in the best scores. For the LT+TL average precision score, growing the decision tree to its maximal size was the best strategy, followed by \texttt{uniform}, the original proposal by \textcite{pliakos2018global} of averaging only the 
outputs of the learned samples (known from the training set) in each leaf. With the exception of the LT+TL AUROC metric for BXT, the mentioned winning models were statistically distinguished from all the remaining estimators. For LT+TL AUROC, the superiority of \texttt{square} could not be attested when compared to the \texttt{uniform} strategy.

The fully-grown versions of both forest algorithms are shown to be especially advantageous when considering the LT+TL average precision. This suggests that building trees to their maximum depth is the best strategy for learning tasks in which
%
\begin{enumerate}
    \item one of the domains is fixed, with the final goal being to model how new instances will bind to this known set of entities;
    \item the goal is to select a small number of top-ranked interactions (see \autoref{sec:comparing auroc aupr}).
\end{enumerate}
%
%Examples that satisfy the first condition are drug repositioning~\cite{} or dyadic prediction~\cite{}. % TODO check
%
On the other hand, weighted averages seem to improve the forest's generalization ability, as they seem to perform best under the AUROC metric and TT contexts.

We propose that this distinction results from the ability of a fully-grown tree to independently consider the labels of each learned instance when calculating the prototype.
However, the methods using weighted averages ivariably mix the labels of a pool of neighbors in each leaf.
This hypothesis is supported by the fact that the second-best model regarding LT+TL average precision is the \texttt{uniform} strategy, which also uses the labels of individual learned samples to generate the predictions.
%
The hypothesis alone, however, do not explain the superiority of the deeper trees over the \texttt{uniform} weights. In this case, the larger tree depth is likely beneficial through i) an increase in the predictive power of each individual tree, and/or ii) an increase in tree diversity, both of which would improve the ensemble's performance as discussed in \autoref{sec:ensembles}. We let to future work the more specific investigation of these effects.

%The fact the opposite result is found in the TT scenario
On the other hand, \texttt{full} and \texttt{uniform} show notable inferior performance in the TT evaluation settings, suggesting that these strategies have inferior ability to consider completely new interacting pairs.
%the weighting strategy used by the other prototype functions could be an important technique to improve generalization.

%The maximum depth, however, may impact the model's generalization ability, since the same superiority is not shown for the TT average precision score, where both entities of the pair being predicted are unknown to the model. Our squared-similarity weights seemingly display the best generalization in this regard.

% TODO: why fully grown is not that good for AUROC
%The full models are also seemingly affected

% FIXME: gmosa vs gmo uniform on TT: indicates there are duplicated samples in the dataset, so training samples are leaking to test set.
We select the squared weighting strategy and the fully grown trees to be further investigated in the downstream analyses. %, keeping 5 by 5 as the minimum leaf partition size besides.


\begin{figure*}[tbh]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics{
            experiments/prediction_weights/statistical_comparisons/%
            brf/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics{
            experiments/prediction_weights/statistical_comparisons/%
            brf/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}

    \begin{subfigure}{0.49\textwidth}
        \includegraphics{
            experiments/prediction_weights/statistical_comparisons/%
            brf/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics{
            experiments/prediction_weights/statistical_comparisons/%
            brf/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \caption{
        Percentile rankings of prediction scores of bipartite random forests for different prediction weighting strategies, across all 10 datasets.
        The omnibus p-value obtained through a Friedman test is indicated below each plot's title. Significant results in each case lead us to perform pairwise Wilcoxon rank-sum tests, whose non-significant results are indicated by crossbars above each plot, that is, statistically undistinguished boxes are connected by the crossbars. The pairwise test results were corrected by the Benjamini-Hochberg procedure among each subfigure, considering that all possible estimator pairs were evaluated even if their comparison is not indicated in the plot. See \autoref{sec:empirical_complexity} for further descriptions of each estimator.
    }
    \label{fig:pred_weights_brf}
\end{figure*}


\begin{figure*}[tbh]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/prediction_weights/statistical_comparisons/%
            bxt/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/prediction_weights/statistical_comparisons/%
            bxt/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}

    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/prediction_weights/statistical_comparisons/%
            bxt/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/prediction_weights/statistical_comparisons/%
            bxt/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \caption{
        Percentile rankings of prediction scores of bipartite extremely randomized trees for different prediction weighting strategies, across all 10 datasets.
        The omnibus p-value obtained through a Friedman test is indicated below each plot's title. Significant results in each case lead us to perform pairwise Wilcoxon rank-sum tests, whose non-significant results are indicated by crossbars above each plot, that is, statistically undistinguished boxes are connected by the crossbars. The pairwise test results were corrected by the Benjamini-Hochberg procedure among each subfigure, considering that all possible estimator pairs were evaluated even if their comparison is not indicated in the plot. See \autoref{sec:empirical_complexity} for further descriptions of each estimator.
    }
    \label{fig:pred_weights_bxt}
\end{figure*}



\subsection{Which adaptation strategy is the best for decision forests?}
\label{sec:adaptation_comparison}

\begin{mdframed}
    \textbf{Key findings:}
    \begin{itemize}
        \item SLMO is the best strategy for BRF models on LT+TL sets.
        \item GMO is the best strategy for completely new dyads.
        \item Undersampling of negative annotations is beneficial for GSO BXT in terms of AUROC, but should be avoided if the goal is to select the highest ranked interactions.
    \end{itemize}
\end{mdframed}

We now compare each of the described approaches for adapting decision forests to bipartite data, including data-centered adaptations (\autoref{sec:standard adaptations}) and the natively bipartite forests (\autoref{sec:bipartite_trees} and \autoref{sec:bgso_trees}). We briefly describe below the suffixes in the model names of this section, indicating the employed bipartite adaptations.

% TODO: contribution: ensembles of gmo (not gmosa) were nevel built!

\begin{itemize}
    \item \textbf{\texttt{lmo}:} implements the standard local multi-output approach (SLMO; \autoref{sec:slmo}) by training four separate multioutput models, two for each domain. First explored by \textcite{schrynemackers2015classifying}.
    \item \textbf{\texttt{lso}:} also implements the SLMO approach (\autoref{sec:slmo}), but each multioutput model is instead a composition of several local single-output (LSO) models, i.e. one model is trained for each row or column of the interaction matrix. This setting is similar to the early proposal by \textcite{bleakley2009supervised}, but employing decision forests as the base algorithm.
    \item \textbf{\texttt{gmosa}:} a global multi-output forest with single-label averaging (GMOSA; \autoref{sec:gmo}), as explored by \textcite{pliakos2019network} under the name eBICT.
    \item \textbf{\texttt{gmo}:} a global multi-output (GMO; \autoref{sec:gmo}) forest with minimum leaf dimensions of 5 by 5, implementing our proposed squared-similarities weighting for the prototype function (\autoref{sec:prototype}). Apart from the new prototype, this model is based on the original GMO trees proposed by \textcite{pliakos2018global}. Despite their original results suggesting advantage over GMOSA, to the best of our knowledge, this is the first time the GMO trees are employed in building decision forests.  % TODO explain GMOSA
    \item \textbf{\texttt{gso}:} a bipartite global single-output (BGSO; \autoref{sec:bgso}) forest, as initially explored by \textcite{schrynemackers2015classifying} but now implementing our proposed algorithm with improved computational complexity.
    \item \textbf{\texttt{sgso\_us}:} implements the standard global single-output (SGSO; \autoref{sec:sgso}) adaptation, additionally employing undersampling of the non-interacting pairs to yield a balanced training set (\autoref{sec:sgso}).  % TODO explain that is frequently used
\end{itemize}

All the global models were built with 100 trees. For SLMO, each of the four forests used 50 trees, while for LSO 50 trees were used for each row or column of the interaction matrix.

In the LT+TL AP evaluation setting, the pattern observed in \autoref{sec:prototype_comparison} again emerges: the label averaging strategy employed by GMO performs considerably worse in comparison to forests that separately consider each known instance. In particular, the SLMO and SLSO adaptations yield the clear best BRF models in terms of LT+TL AP, and these adaptations interpret each instance as a separate output to be predicted. Among the local adaptations, SLMO significantly ouperforms SLSO. SLSO treats each training instance as a completely independent task, building a separate forest for each row and column of the interaction matrix. As such, the previous result shows that this complete independence is not desirable for the learning problems under study, and exploring label correlations between instances of the same domain is beneficial (as demonstrated by SLMO). This result could partially be a consequence of the very sparse nature of our problems: if the interaction information of each instance is limited, it becomes advantageus to aggregate information from other instances with correlated interactomes. We then speculate that the advantage of SLMO over SLSO could become less prominent once the number of known interactions per instance increases and more data are available for training.

Still considering LT+TL AP but focusing on BXT models instead, we notice that SLMO loses the advantage to the fully-grown bipartite trees GMO and GSO. A possible explanation comes from the fact that local approaches yield shallower trees, and much more randomized trees could be required to achieve comparable performance. In more detail, first notice that the individual performance of each tree in a BXT forest is lower than that of a tree in a BRF. Thus, a BXT ensemble requires a larger number of trees to reach satisfactory performance. Additionally, if the trees are shallow, they tend to be less representative of the training data (each tree node brings a little more information on the dataset).
Even more trees then should be required to compensate the randomness of each individual. Finally, building a forest locally as in SLMO or SLSO results that each tree is trained on a much smaller number of samples in comparison to considering each dyad as a separate instance. Therefore, local approaches generate smaller and less representative trees. We then suggest that the advantage of SLMO could also manifest for BXT estimators if the number of trees in the ensemble were to be increased.

For both BXT and BRF in all the TT settings, the GMO model significantly outperforms the other estimators. GMO is also the best model in the LT+TL AUROC setting when comparing BXTs and the second best for BRFs. This reinforces the findings of \autoref{sec:prototype_comaprison} suggesting that larger leaves and label weighting seems to improve generalization to unseen instances.
%
We also highlight SLMO as a prominent strategy for BRFs under TT AUROC and TT AP, being the second best model in both cases.

The SGSO US strategy performed consistently worse than the others under the AP metric. Conversely, it was significantly the second best BXT model in both LT+TL and TT AUROC settings, while surpassing BRF GMOSA also under both AUROC test sets.
This shows that unsersampling is a viable technique when when considering the overall ranking of interactions. However, it is detrimental when the goal is to select the most likely interactions. That is, SGSO US allows a larger number of false positives in the highest ranked positions.

This behaviour is expected. Notice that SGSO US models are trained in a balanced dataset, while the other models are trained in the original dataset where negative annotations are much more frequent. With more training examples, the other models tend to be considerably better in correctly classifying negative annotations. This suggests that AUPR tends to favor models of high specificity, that are strictly avoid predicting false positives.
%The explanation would be that the undersampling procedure naturally causes the model to equally prioritize the positive and negative labels, which matches the AUROC's behaviour of equally treating TPR and FPR.
%More specifically, these models tend to produce a larger number of false positives since they are unaware of the large label imabalance of our test sets. While this substantially affects AP, the AUROC metric is agnostic to the label imbalance (\autoref{sec:prediction_metrics}), being much more forgiving with the number of false positives given the large total number of negative labels in the test sets.

% TODO sgso us is popular but not good

We select LMO, GMO, GSO, and GMOSA to be further analysed in the next section.

% TODO: relationship with previous work

% The GMO approach is also highlighted by the results of BXT ensembles, being undisputed for the LT+TL AUROC and TT scores. Interestingly, for the LT+TL average precision score, its mean percentile ranking was the second to last, being only better than SGSOUS. These results corroborate the idea discussed in \autoref{sec:prototype_comparison} that reaching the maximum tree depth is the best strategy for LT+TL, 

% TODO training times

% To avoid differences in random sampling when using the naive GSO adapter versus the natively bipartite GSO tree, no bootstraping was applied to any forest, providing all trees with the whole training samples space. To still ensure randomization in random forest estimators, the maximum features parameter was set to $0.5$, meaning that each tree in a random forest was trained on a random subset of half the features from each sample domain. Due to implementation details, this means the naive GSO forests will sample features slightly differently: they will pick half the features from the whole feature space combined, while the natively bipartite GSO forests will ensure half the features from each sample domain is selected. This is not expected to have a significant impact on the results, given that the total number of features is especially high in the present scenario, where similarity matrices are being employed. 

%All forests were composed of 100 tree estimators and were fully grown, with the exception of the GMO models, whose leaf sizes were limited to a minimum of 5 by 5 samples (at least 5 samples from each domain) in order to take advantage of neighborhood weighting (which was set to the squared similarities, see Section \ref{}).

%In all of the evaluated scenarios, a BXT model was ranked the best. In both TT-MCC and TL-AP, the BRF models and bxt\_gmo were significantly surpassed by the remaining BXT forests. In TL-MCC, bxt\_lso significantly outperformed all the other models. Given that this test-set provides the greatest intersection with the training set, due to the overall higher number of column samples in the datasets we used, we suggest that the LSO model could be better at taking advantage of already seen information from a sample domain, since a forest is grown separately for each row and column. From another perspective, this effect could be regarded as a form of overfitting. This hypothesis is further supported by the fact that both GMO models, which were the only forests subject to a minimum leaf size constraint and thus less prone, in theory, to overfitting, were the most frequently outperformed models in the TL-MCC and TL-AP.

%Contrastingly, both GMO models were consistently the best performing models in all AUROC settings, which is commonly assumed to be a less indicative metric in highly imbalanced classification contexts such as the present one \cite{}. This could be explained by a bias towards the majority class, possibly caused by the averaging of the larger leaves employed in this methods. %FIXME: actually, a bias towards minority class

\begin{figure*}[tbh]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/bipartite_adaptations/statistical_comparisons/%
            brf/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/bipartite_adaptations/statistical_comparisons/%
            brf/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}

    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/bipartite_adaptations/statistical_comparisons/%
            brf/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/bipartite_adaptations/statistical_comparisons/%
            brf/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \caption{
        Percentile rankings of prediction scores of random forest models under different adaptation strategies to bipartite interaction data, across all 10 datasets.
        The omnibus p-value obtained through a Friedman test is indicated below each plot's title. Significant results in each case lead us to perform pairwise Wilcoxon rank-sum tests, whose non-significant results are indicated by crossbars above each plot, that is, statistically undistinguished boxes are connected by the crossbars. The pairwise test results were corrected by the Benjamini-Hochberg procedure among each subfigure, considering that all possible estimator pairs were evaluated even if their comparison is not indicated in the plot. See \autoref{sec:adaptation_comparison} for further descriptions of each estimator.
    }
    \label{fig:adaptations_brf}
\end{figure*}


\begin{figure*}[tbh]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/bipartite_adaptations/statistical_comparisons/%
            bxt/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/bipartite_adaptations/statistical_comparisons/%
            bxt/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}

    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/bipartite_adaptations/statistical_comparisons/%
            bxt/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/bipartite_adaptations/statistical_comparisons/%
            bxt/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \caption{
        Percentile rankings of prediction scores of extremely randomized tree ensembles under different adaptation strategies to bipartite interaction data, across all 10 datasets.
        The omnibus p-value obtained through a Friedman test is indicated below each plot's title. Significant results in each case lead us to perform pairwise Wilcoxon rank-sum tests, whose non-significant results are indicated by crossbars above each plot, that is, statistically undistinguished boxes are connected by the crossbars. The pairwise test results were corrected by the Benjamini-Hochberg procedure among each subfigure, considering that all possible estimator pairs were evaluated even if their comparison is not indicated in the plot. See \autoref{sec:adaptation_comparison} for further descriptions of each estimator.
    }
    \label{fig:adaptations_bxt}
\end{figure*}


\subsection{Can label imputation aid bipartite forests?}
\label{sec:y_reconstruction}

\begin{mdframed}
    \textbf{Key findings:}
    \begin{itemize}
        \item Imputing positive annotations with NRLMF improves the performance of bipartite forests in the majority of cases.
        \item For LT+TL AP, the best BRF is BRF LMO and the best BXT is BXT GMOSA NRLMF.
        % \item For LT+TL AUROC, the best BRF is BRF LMO NRLMF
        \item GMO NRLMF is the overall best model for TT.
    \end{itemize}
\end{mdframed}

It was previously suggested to employ logistic matrix factorization to create a dense representation of the interaction matrix. Using this representation as the training data for a BXT forest could improve the performance on DTI datasets \cite{pliakos2020drugtarget}.
To test this hypothesis, we compare the bipartite forests cross-validation scores with and without the interaction matrix reconstruction step. As done by \cite{pliakos2020drugtarget}, the reconstruction step was performed using neighborhood-regularized logistic matrix factorization (NRLMF)\cite{liu2016neighborhood}.

%\subsubsection{Hyperparameter selection}
%\subsubsection{Experimental settings}
We performed a randomized search to select hyperparameters for the NRLMF algorithm. 100 different combinations of hyperparameters were evaluated
in terms of their resulting mean squared error in a nested bipartite 5-fold diagnonal cross-validation.
The best combination of parameters according to the inner CV loop was then used to reconstruct the interaction matrix of each outer CV fold. Only then the resulting matrices were used as the training data for the bipartite forests. Note that a single forest was built per outer CV fold, so that the NRLMF hyperparameter search was performed independently from the downstream forest performance. The hyperparameters \texttt{lambda\_rows}, \texttt{lambda\_cols}, \texttt{alpha\_cols}, \texttt{alpha\_cols}, and \texttt{learning\_rate} were all independently sampled from a log-uniform distribution bounded by $\frac{1}{4}$ and $2$. The number of latent vector components was set to be equal for both axes, and chosen between 50 and 100. The number of neighbors was randomly selected as 3, 5 or 10 in each iteration, and the maximum number of optimization steps was always set to 100. The parameter $c$, as in the original paper, was set to $5$. This parameter specifies the emphasis on positive annotations, so each positive annotation contributes $5$ times more to the loss function in our settings, as if 5 copies of each positive interaction were present in the training set.
%
See \autoref{sec:adaptation_comparison} for descriptions on the bipartite forests being compared.

The results for BRF and BXT are shown by figures \ref{fig:brf_y_reconstruction} and \ref{fig:bxt_y_reconstruction}, respectively. The \texttt{nrlmf} suffix to a model indicates that it was built on the output of NRLMF.

The interaction matrix reconstruction step is shown to be especially beneficial in terms of AUROC scores, improving this score for almost all forest algorithms investigated. The only exceptions were \texttt{bxt\_gmo} and \texttt{brf\_gmo} in LT+TL ROC AUC, where the improvement is still observed but not statistically significant.
%
%As discussed in \autoref{sec:comparing auroc aupr}, ROC AUC is characterized by more leniency towards highly-ranked false positives, which explains the advantage posed by NRLMF in this setting: NRLMF explicitly defines a parameter to prioritize outputting positive interactions (\autoref{sec}). % TODO
%
%In accordance with previous investigations~\cite{liu2016neighborhood,liu2017lpinrlmf,liu2020predicting,pliakos2020drugtarget},
%this parameter was set to 5, meaning that the loss function employed in the gradient descent procedure of NRLMF is weighted 5 times more for positive interactions than for negative interactions, as if 5 copies of each positive interaction were present in the training set.

Even without the help from NRLMF, the \texttt{bxt\_gmo} and \texttt{brf\_gmo} models employing our squared similarities output weighting (sections \autoref{sec:prototype} and \autoref{sec:prototype_comparison}) are placed second in their corresponding TT ROC AUC results, both significantly outperforming all NRLMF-combined models except for their own versions, \texttt{bxt\_gmo\_\_nrlmf} and \texttt{brf\_gmo\_\_nrlmf}. % (as aforementioned).
\texttt{brf\_gmo} was also the top performing BRF model in terms of TT average precision, significantly surpassing all but \texttt{brf\_gmo\_\_nrlmf} and \texttt{brf\_lmo\_\_nrlmf}.
Similarly, \texttt{bxt\_gmo} was the top performing BXT model under the same metric, significantly surpassing all estimators but \texttt{brf\_gmo\_\_nrlmf}, \texttt{brf\_lmo\_\_nrlmf} and BXT GMOSA NRLMF.

Regarding TT AP, NRLMF still provides a significant improvement for all models except \texttt{brf\_gmo}, \texttt{bxt\_gmo} and \texttt{bxt\_lmo}. While both BRF GMO and BXT GMO resulted in the highest average ranks for TT AP, the top position could not be statistically resolved between the BRFs using GMO, GMO NRLMF, and LMO NRLMF, nor between the BXTs using GMO, GMO NRLMF, LMO NRLMF, and GMOSA NRLMF.

In the LT+TL AP setting for BXT ensembles, NRLMF is shown to significantly degrade the performance of \texttt{bxt\_gmo}. Therefore, the combination of the squared neighbors prototype and NRLMF likely results in more high-rank false positives under LT+TL, especially since such degradation is not observed under LT+TL AUROC.  % TODO name prototype
%
%We attribute this effect to the aforementioned increase in false positives caused by NRLMF, especially since such degradation is not observed under LT+TL AUROC. Notwithstanding, we note that this degradation of LT+TL AP is exclusive to the GMO adaptation, as all other BXT models are significantly improved by NRLMF in this setting. Moreover, we highlight the differences between the LT+TL AUROC and LT+TL AP results considering BXT GMO and BXT GMO NRLMF: while these models are the best ranked in the former setting, they are the clear worst in the latter. We thus argue that both the GMO strategy and the NRLMF reconstruction step, each in and of itself, have the effect of increasing the tendency towards false positives in the predictions. Their summed effects could thus impair predictive performance especially under LT+TL AP evaluation, whereas in other settings they could  missing positives or AUROC evaluation in general, the combina...  % TODO finish this discussion

%This suggests that the combination of GMO's label averaging method 

% TODO: (last section) gmo more fp

% Interestingly, the scores for \texttt{bxt\_gmo}, \texttt{brf\_gmo}, and \texttt{brf\_lmo} are instead significantly reduced by combination with the matrix factorization technique in LT+TL average precision, contrary to the overall tendency.
%

For BRF models under LT+TL AP, the advantage of employing NRLMF is not clear. GSO was the only adaptation to significantly benefit from NRLMF in this setting, while GMO and LMO were significantly impaired by it. In fact, the BRF LMO model without label matrix reconstruction significantly outperformed all the other random forests for this evaluation setting.

% Not clear from the data:
%Additionally, the GSO and GMOSA models are seemingly favored by the LT+TL setting in comparison to TT, reinforcing the results of \autoref{sec:adaptation_comparison} suggesting that deeper trees are advantageous for LT+TL, while the larger leaves of GMO models seem to improve generalization for completely unknown pairs (TT average precision). Notably, the \texttt{bxt\_gmosa\_\_nrlmf} model, proposed by \textcite{pliakos2020drugtarget} under the name of BICTR, was the top scoring model among BXT for all but the TT ROC AUC test settings.

% TODO add hue, compare within pairs with vs without nrlmf
% TODO
% fullly grown doesnt mean n_leaf == 1
% we expect more trees to make nrlmf irrelevant
% nrlmf probably makes more false positives
% less relevant for gmo and degrades performance in lttl ap only for rf
% complexity of other adaptations
% why nrlmf is detrimental to BRF lt+tl ap?
% why nrlmf is detrimental to GMO?
% why nrlmf is not detrimental to bxt lmo?

%So why does NRLMF seem detrimental to \texttt{brf\_gmo}, \texttt{bxt\_gmo} and \text{brf\_lmo} in terms of LT+TL average precision? For any tree, if one of the interacting entities is known from the training data (let's call it $x_\text{known}$), the leaf at which the interaction from the TL or LT sets arrives is guaranteed to hold a partition of $Y_\text{train}$ with a column corresponding to $x_\text{known}$. The GMO and LMO models are the only ones to emphasize the effect of this column: LMO will output the average of only the labels in this column (see \autoref{sec:slmo}), while GMO will weight it considerably higher than the other columns when computing the leaf's prototype (\autoref{sec:prototype}). The usage of NRLMF however, causes the negative labels in such column of $Y_\text{leaf}$ to be influenced by neighbors from the training set. While this should improve generalization, we speculate that the LT+TL setting could benefit from this "partial overfitting" that occurs when NRLMF is not employed. We thus hypothesize this effect is less pronounced in BXT due to the naturally lower tendency to overfit of ExtraTrees, resulting from the randomized selection of thresholds (\autoref{sec:bipartite_forests}). We also suppose that this effect in LT+TL ROC AUC is overruled by the sensitivity improvement brought by NRLMF, as discussed above, hence it not being apparent from the results.

%Another hypothesis can be formulated based on the fact that the dense reconstructed matrix will cause trees to grow to a much deeper extent.
%%as homogeneity of leaves is harder to obtain.
%As such, this could favor overfitting of BRF models to the output of NRLMF, carrying the generalization error of the NRLMF reconstruction to the results. At the same time, NRLMF being a better model than BRF on TT sets would generate, as observed, the inverse effect, with NRLMF improving performance in this case. % TODO discuss this in model comparison section
%% this would cause BRF to be equal or worse than NRLMF alone
%Again, BXT's randomized split search procedure likely makes it more resilient to this influence, building uncorrelated individual estimators even if the trees are large and thus more generally benefitting from NRLMF.

% TODO
%On BXT, Based on these hypothesis, we predict that the impact of NRLMF will be less prominent on a greater number of trees

% Additionally, imputting the negative labels allows the trees to grow deeper

%\texttt{brf\_gmo} and \texttt{lmo} are the models most likely to overfit the training data, given that 

%We speculate that the addition of the NRLMF training step does introduce more information into the training set, of which downstream trees can take advantage, at the expense of elevating type I errors and reducing estimator variance. The first effect, increasing FPR, could explain the positive effect of NRLMF on ROC AUC, as aforementioned...

% The second effect would be to decrease the correlation of each tree with the original interaction matrix.

% nrlmf and gmo
% TODO: future testing with less positive importance and larger trees
% some of the estimators are definitely not calibrated (explain)

In summary, interaction matrix reconstruction by NRLMF seems to consistently improve BRF and BXT results in terms of AUROC and also the results of BXT regarding LT+TL AP. On TT AP, while less evidence is found, the results still seem to point in the same direction of a beneficial NRLMF transformation step. On the other hand, the comparisons of BRF under LT+TL average precision evaluation indicate the opposite conclusion, disfavoring usage of NRLMF especially for \texttt{brf\_lmo} and \texttt{brf\_gmo}. We thus discourage the application of NRLMF with BRF in scenarios where i) one of the entities of the dyad is always known to the model, such as in drug repositioning; and ii) the main interest is to select a small set of most-likely interactions.

% TODO: remake the following explanation after displaying the rank boxplots

% A first explanation could be that the NRLMF hyperparameter search was not exhaustive enough, and that a better combination of hyperparameters could have been found. However, we later show in section \ref{} that the NRLMF model alone displays competitive performance, disfavoring such hypothesis of underfitting.
% 
% We also notice that \cite{Liu_2017} performs bipartite cross-validation in unusual fashion, by replacing with zeros the positive labels of pairs assigned to the test set but still using them to train the model. Albeit test labels are masked, each model thus still receives all available samples during training, and we hypothesize that unsupervised information from the test set could possibly still be exploited during training. For instance, estimating the sample density of the feature space could provide an importance score, a weight factor, for each sample, in order to favor correct predictions of denser clusters and undermine outliers. Wether and how this or similar mechanisms are explored by the NRLMF algorithm is out of the scope of this work, but we discourage the use of such CV strategy and point it as a possible reason for the higher NRLMF scores observed by previous authors \cite{Pliakos_2020;Liu_2017}.

% (False, they also used nested CV) The authors of \cite{Pliakos_2020} also do not clearly specify the could have employed the reconstruction step before splitting the data into the cross-validation folds, in which case the observed performance improvement could be attributed to indirect data leakage, where the labels used to train the downstream forest estimator were generated from neighbor samples that are possibly in the test sets.

\begin{figure*}[tbh]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/y_reconstruction/statistical_comparisons/%
            brf/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/y_reconstruction/statistical_comparisons/%
            brf/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}

    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/y_reconstruction/statistical_comparisons/%
            brf/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/y_reconstruction/statistical_comparisons/%
            brf/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \caption{
        Percentile rankings of prediction scores of extremely randomized tree ensembles under different adaptation strategies to bipartite interaction data, across all 10 datasets.
        The omnibus p-value obtained through a Friedman test is indicated below each plot's title. Significant results in each case lead us to perform pairwise Wilcoxon rank-sum tests, whose non-significant results are indicated by crossbars above each plot, that is, statistically undistinguished boxes are connected by the crossbars. The pairwise test results were corrected by the Benjamini-Hochberg procedure among each subfigure, considering that all possible estimator pairs were evaluated even if their comparison is not indicated in the plot. See \autoref{sec:adaptation_comparison} for further descriptions of each estimator.
    }
    \label{fig:brf_y_reconstruction}
\end{figure*}


\begin{figure*}[tbh]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/y_reconstruction/statistical_comparisons/%
            bxt/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/y_reconstruction/statistical_comparisons/%
            bxt/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}

    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/y_reconstruction/statistical_comparisons/%
            bxt/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/y_reconstruction/statistical_comparisons/%
            bxt/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \caption{
        Percentile rankings of prediction scores of extremely randomized tree ensembles under different adaptation strategies to bipartite interaction data, across all 10 datasets.
        The omnibus p-value obtained through a Friedman test is indicated below each plot's title. Significant results in each case lead us to perform pairwise Wilcoxon rank-sum tests, whose non-significant results are indicated by crossbars above each plot, that is, statistically undistinguished boxes are connected by the crossbars. The pairwise test results were corrected by the Benjamini-Hochberg procedure among each subfigure, considering that all possible estimator pairs were evaluated even if their comparison is not indicated in the plot. See \autoref{sec:adaptation_comparison} for further descriptions of each estimator.
    }
    \label{fig:bxt_y_reconstruction}
\end{figure*}



\subsection{What is the best way of building semi-supervised forests?}
\label{sec:ss_comparison}

\begin{mdframed}
    \textbf{Key findings:}
    \begin{itemize}
        \item The MD unsupervised impurity is the best option for LT+TL.
        \item AD is the most promising strategy for TT.
        \item The best strategy for determining the supervision balance seems to be either size or fixed, but further investigation is needed.
    \end{itemize}
\end{mdframed}

In this experiment, we compare the performances of semi-supervised bipartite forests \autoref{sec:ss trees}. All forests in this section are based on the BGSO BXT, as described in \autoref{sec:adaptation_comparison}. We evaluate all combinations of the three strategies for calculating the unsupervised impurity and the four strategies for determining the supervision balance at each node.

\paragraph*{Strategies for determining the unsupervised impurity:}
\begin{itemize}
    \item \textbf{\texttt{mse}:} it refers to forests using the mean squared error as the unsupervised impurity (\autoref{eq:mse impurity}).
    \item \textbf{\texttt{md}:} corresponds forests employing the mean distance unsupervised impurity (\autoref{eq:mean distance impurity}).
    \item \textbf{\texttt{ad}:} also uses the mean squared error (\autoref{eq:mse impurity}), however, the semi-supervised impurity is only calulated twice on each tree node. The search for the best feature and split point in each instance domain uses the supervised impurity only. Then, the semi-supervised impurity is only used to evaluate the best split in each domain. Finally, the two resulting values for the semi-supervised impurity are used to select between the horizontal and vertical split.
\end{itemize}

\paragraph*{Strategies for determining the supervision balance ($\sigma$):}
\begin{itemize}
    \item \textbf{\texttt{fixed}:} the supervision balance ($\sigma$) is fixed at $0.5$ for mse and md and at $0$ for ad.
    \item \textbf{\texttt{density}:} $\sigma$ is determined by the density of positive annotations in each tree node (\autoref{eq:sigma density}). 
    \item \textbf{\texttt{size}:} $\sigma$ is determined by the total number of interactions current in the tree node (\autoref{eq:sigma size}).
    \item \textbf{\texttt{random}:} $\sigma$ is set to a random value between $0$ and $1$ drawn at each tree node (\autoref{eq:sigma random}).
\end{itemize}

The comparison results are displayed by \autoref{fig:ss_comparison}.
%
Regarding LT+TL AUROC, the 0\% ILR and 50\% ILR do not result in statistically significant differences in performance. For ILR=70\%, md random and md size significantly outperform the remaining models, while under IRL=90\%, md random, md size, and md density are shown to surpass the others. These results indicate that the MD unsupervised impurity is the most suited to the LT+TL AUROC evaluation setting. Furthermore, we see that the random strategy to select the supervision amount %TODO
is among the best in both ILR=70\% and ILR=90\%, suggesting that increasing diversity among the trees could be the main mechanism behind the improvement of the semi-supervised models, rather than necessarily guessing the best $\sigma$ at each tree node. Since md size is also among the best, it is not clear if the advantage of md size over the other models is due to a better choice of $\sigma$ or to a more stochastic nature of the $\sigma$s it selects.
%
%As a future investigation, we suggest using the size-based $\sigma$ selection with a completely random unsupervised impurity, .
%
As a future investigation, we suggest using completely random values for the unsupervised impurity, to assess the possibility of tree-diversity being the main factor behind the observed improvements.

As for TT AUROC, only ILR=90\% yielded significant comparisons. In this case, the mse unsupervised impurity with size, random, and density $\sigma$ selection were the best strategies, surpassing the remaining. MSE thus seems the better option in terms of AUROC for scenarios with very scarse information and completely unknown instances.

Under LT+TL AP, md fixed significantly outperformed the other models for ILR=50\% and ILR=70\%. For ILR=0\%, md fixed is also the first place, but could not be statistically resolved from the second place ad size. AD size was also the second best model for ILR=0\%, 50\%, 70\%, statistically outperforming all but md fixed in ILR=70\%.

With respect to TT AP, the AD strategies prevail, being the best four models for ILR=50\% and ILR=70\%, and being among the five best models for ILR=0\%. The presence of AD random among the best models again suggests that tree-diversity is an important factor for performance improvement.
%
It seems that AD models perform overall better according to AP in comparison to AUROC. This is consistent with the discussion in \autoref{sec:comparing auroc aupr}: AP tends to prioritize models more tighly related to the observed labels, assuming less risk of false positives. AD is the strategy that is less influenced by the unsupervised impurity function.

In conclusion, the best unsupervised impurity under LT+TL seems to be the mean distance, and AD seems to be the best for AP TT. The best strategy for determining $\sigma$ seems to be either size or fixed, but more experiments are needed to confirm this finding.
% TODO more discussion
% TODO size vs fixed

% md size, md fixed, ad size, ad fixed, mse density

\begin{figure*}[tb]
    \centering
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            no_drop/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            drop50/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            drop70/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            drop90/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}

    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            no_drop/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            drop50/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            drop70/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            drop90/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}

    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            no_drop/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            drop50/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            drop70/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            drop90/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}

    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            no_drop/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            drop50/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            drop70/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/semisupervised_forests/statistical_comparisons/%
            drop90/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}

    \caption{
        Percentile rankings of prediction scores of extremely randomized tree ensembles under different adaptation strategies to bipartite interaction data, across all 10 datasets.
    }
    \label{fig:ss_comparison}
\end{figure*}


\subsection{Which forests are the best?}
\label{sec:best_forests}

\begin{mdframed}
    \textbf{Key findings:}
    \begin{itemize}
        \item Semi-supervised impurities are beneficial in terms of AUROC, especially when more annotations are missing.  % And not exactly for AP
        \item Label imputation with NRLMF is better than semi-supervised impurities.
        \item BXT are superior to BRF in all settings.
    \end{itemize}
\end{mdframed}

% TODO bxt better than brf?
In this experiment, we compare the performances of various models from the previous sections. For more information on each model see the corresponding section in the list bellow.

\begin{itemize}
    \item \textbf{BXT-GSO, BRF-LMO:}
    Bipartite forests without label imputation.
    See \autoref{sec:adaptation_comparison} for more information.
    \item \textbf{MD SIZE, MD FIXED, AD SIZE, AD FIXED:}
    Bipartite forests employing semi-supervised impurities.
    See \autoref{sec:ss_comparison}.
    %\item \textbf{BRF GMO NRLMF, BXT GMO NRLMF, BXT GSO NRLMF, BXT GMOSA NRLMF:} 
    \item \textbf{Models with the NRLMF suffix:} 
    Bipartite forests using NRLMF to impute positive annotations.
    See \autoref{sec:y_reconstruction}.
\end{itemize}
%For information on models BXT GSO and BRF LMO, see \autoref{sec:adaptation_comparison}. For information on MD SIZE, MD FIXED, AD SIZE and AD FIXED, see \autoref{sec:ss_comparison}. For information on 

%Based on the results of \autoref{sec:y_reconstruction}, we select the following models for further comparison:

% TODO: nrlmf very efficient for missing data
% gmo is faster than full trees
% ss less efficient
% bxt better than brf
% gmo king
% is gmo ss as well?
% using nrlmf makes trees deeper -> diversity / ovefitting
% time to train is complicated with NRLMF

The comparisons are presented by \autoref{fig:best_forests}. The results reveal a clear superiority of forests employing the NRLMF as a label imputation strategy, in comparison to those using semi-supervised impurities. The four models employing NRLMF were the four highest ranked estimators in almost all evaluation settings, the two exceptions being LT+TL AP with ILR=0\% and ILR=50\%. In the first exception (LT+TL AP ILR=0\%), BRF GMO NRLMF and BXT GMO NRLMF are the two worst-performing models, whereas BXT GMOSA NRLMF and BXT GSO NRLMF are the first and second best, respectively. In the second exception (LT+TL AP ILR=50\%), the two best models are the same, and BRF GMO NRLMF is still one the worst performers. However, BXT GMO NRLMF jumps to the third best position. 

% TODO gmo better on less info: less info, more imputation better

Furthermore, under the other metrics (LT+TL AUROC, TT AUROC, and TT AP), BXT GMO NRLMF was the highest ranked estimator in almost all cases, the only exception being TT AP IRL=0\%, where it was only behind BXT GMOSA NRLMF. Notwithstanding, the comparison between BXT GMO NRLMF and BXT GMOSA NRLMF was still not statistically significant in this setting. Similarly, no statistical difference is found between these models in the LT+TL AUROC 0\% setting, where they also occupy the first positions. The leadership of BXT GMO NRLMF is also not significant in comparison to BXT GSO NRLMF under TT AP 50\% and TT AP 70\%. For all remaining cases where BXT GMO NRLMF was the best model, it was statistically significantly better than all other estimators (TT AP 90, LT+TL AUROC 50, LT+TL AUROC 70, LT+TL AUROC 90, and all TT AUROC).

% TODO: metrics section: define questions: there is difference of performance between metrics? between ILR?
% TODO: metrics section: how is AP and AUROC affected by PU?
% TODO: unsupervised impurity according to the prototype of GMO with similarity weighting? (Iu predicts the prototype)
% TODO: leaf weighting with BGSO
% TODO: SS with other adaptations

Another observation is that some semi-supervised impurities significantly improve predictive performance relative to the original BXT GSO, especially when more annotations are missing. This conclusion is based on the fact that, in all settings with ILR != 0 except LT+TL AP, the original BXT GSO is among the three worst performers. For all AUROC settings with ILR!=0, it was the lowest ranked model. On the other hand, md fixed is noted to significantly surpass BXT GSO in 10 of the 16 evaluation settings, the exceptions being LT+TL AUROC 0\%, TT AUROC 0\%, TT AUROC 90\%, TT AP 0\%, TT AP 70\% and TT AP 90\%. Therefore, although not as effective as the NRLMF reconstruction technique, using semi-supervised trees seems indeed beneficial when label information is scarce.

% TODO: future work: GMO + SS impurity

When comapring the BXT against the BRF models, we notice that BXT GMO NRLMF significantly outperformed BRF GMO NRLMF in all settings. The other random forest, BFR LMO, was also significantly outperformed by BXT GMO NRLMF in all cases but LT+TL AP 0\%, where the opposite was observed. Similar results hold for the other BXT models as well: they significantly surpassed BRF LMO in the vast majority of test configurations, with the only exception being TT AUROC 0\%, in which the comparison between BXT GMOSA NRLMF and BRF LMO was not significant. These results suggest that BXT models could offer significant advantages over BRF models in the context of DTI prediction. This conclusion is especially relevant given that the BXT training algorithm is considerably faster than the procedure for building BRFs, as discussed in \autoref{sec:complexity_analysis}.

% TODO: do they never compared BXT vs BRF?


\begin{figure*}[tbh]
    \centering
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            no_drop/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            drop50/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            drop70/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            drop90/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}

    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            no_drop/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            drop50/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            drop70/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            drop90/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}

    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            no_drop/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            drop50/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            drop70/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            drop90/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}

    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            no_drop/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            drop50/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            drop70/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/best_forests_with_dropout/statistical_comparisons/%
            drop90/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \caption{
        Percentile rankings of prediction scores of extremely randomized tree ensembles under different adaptation strategies to bipartite interaction data, across all 10 datasets.
    }
    \label{fig:best_forests}
\end{figure*}


% \begin{table}[h]
%     \input{figures/experiments/y_reconstruction/latex_tables/TT.tex}
% \end{table}


%\subsection{Comparison with previous algorithms}
%\subsection{Can bipartite forests compete with the state-of-the-art?}
\subsection{Can bipartite forests compete with other proposals?}
\label{sec:comparison literature}

\begin{mdframed}
    \textbf{Key findings:}
    \begin{itemize}
        \item Forests with NRLMF surpass NRLMF alone.
        \item BXT GMO NRLMF is the best overall performer, with the exception of LT+TL AP. 
        \item For LT+TL AP, BXT GSO NRLMF and BXT GMOSA NRLMF are the best models overall.
        % TODO \item who is more affected by missing labels? gso and gmo
    \end{itemize}
\end{mdframed}

In this section we compare the two most promising methods we developed (BXT GMO NRLMF and BXT GSO NRLMF) with several prominent models from the literature. 
%
The algorithms being considered in this section are listed below, and their scoring results are shown by \autoref{fig:comparison literature}.

\begin{itemize}
    \item \textbf{BXT GSO NRLMF, BXT GMOSA NRLMF, BXT-GMO-NRLMF:} Ensembles of randomized trees using the NRLMF model to impute missing annotations. The GSO model implements our optimized method for growing the trees. GMOSA uses the global multi output strategy developed by \cite{pliakos2018global}. Both GSO and GMOSA grows the trees to their maximum depth. GMO uses the weighted-neighbors prototype we developed in \autoref{sec:prototype}, and enforces at least 5 samples of each domain in each leaf. See \autoref{sec:prototype_comparison} and \autoref{sec:y_reconstruction} for more information.
    % \item \textbf{BXT GMO NRLMF:} Bipartite ensemble of randomized trees using the NRLMF model to impute missing annotations. 
    % \item \textbf{BXT GSO NRLMF:}
    % \item \textbf{BXT GMOSA NRLMF:}

    \item \textbf{NRLMF:} Neighborhood-Regularized Logistic Matrix Factorization, as proposed by \cite{liu2016neighborhood} and described in \autoref{sec:neighborhood regularization}. See \autoref{sec:y_reconstruction} for the hyperparameters we utilized.

     % TODO other section
    \item \textbf{Kron-RLS:} Kronecker Regularized Least Squares, as proposed by \cite{vanlaarhoven2011gaussian} and described in \autoref{sec:kron rls}.
    Each of the two input kernel matrices was taken as a linear combination of the similarity matrix and the gaussian interaction profiles~\cite{vanlaarhoven2011gaussian} (\autoref{eq:}).
    The weight of the similarity kernel in this combination (the $\alpha$ parameter), was selected between the values \{0.0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0\}.
    The selection was performed in each fold by an internal 5 by 5 bipartite diagonal CV procedure (\autoref{sec:cross_validation}).

    \item \textbf{LMO RLS:} SLMO adaptation (\autoref{sec:slmo}) of Regularized Least Squares, as proposed by \cite{vanlaarhoven2011gaussian} as RLS-avg. Both the primary and secondary models were multi-output kernel Ridge regressors~\cite{}. 
    As Kron-RLS, the input features were linear combinations of similarity matrices and gaussian interactions profiles. The $\alpha$ parameter was selected in the same way as in Kron-RLS.

    % TODO: section for weighted nearest neighbors
    \item \textbf{BLMNII RLS:} Bipartite local models~\cite{yamanishi2008prediction,bleakley2009supervised} with neighbor-based interaction-profile inferring~\cite{mei2013drug}. They use the SLMO strategy (\autoref{sec:slmo}), employing a weighted-neighbors technique (\autoref{sec:dwnn}) for the primary estimators and kernel Ridge regression~\cite{} models as secondary estimators.
    As Kron-RLS, the input features were linear combinations of similarity matrices and gaussian interactions profiles~\cite{vanlaarhoven2011gaussian}. The $\alpha$ parameter was selected in the same way as in Kron-RLS.

    \item \textbf{BLMNII SVM:} The same as BLMNII RLS, but using support vector machines~\cite{} as secondary estimators instead of regularized least squares.

    \item \textbf{DTHybrid:} Method proposed by \cite{dthybrid} that combines the similarity kernels with network features of each domain calculated from the interaction matrix. A weighted-neighbors approach is then used to predict new interactions.  % TODO I think it is not originally proposed for new instances

    \item \textbf{MLP:} Multi-layer perceptron~\cite{} model adapted with the SGSO strategy (\autoref{sec:sgso}). We performed random undersampling of negative annotations so that the model was trained on equal number of negative and positive labels. 
    The architecture was selected at each fold between four options: 5 hidden layers of 100 neurons; 10 hidden layers of 50 neurons; 5 hidden layers with [200, 100, 100, 100, 50] neurons; and 6 hidden layers of [1024, 512, 256, 128, 64, 32] neurons. The architecture selection in each fold was performed by an internal 5 by 5 bipartite diagonal CV procedure (\autoref{sec:cross_validation}).
    The activation function was always the rectified linear unit, and ADAM~\cite{adam} was the chosen optimizer. Other parameters were kept as the defaults from the scikit-learn library~\cite{sklearn}.
\end{itemize}

BXT GMO NRLMF was the best ranked model in 11 out of the 16 test settings analysed. The exceptions were the four LT+TL AP configurations and TT AP 0\%. In TT AP 0\%, BXT GMO NRLMF was only behind lmo rls, and among the three models that could not be statistically distinguished from lmo rls (the others being kron rls bxt and gmosa nrlmf). In LT+TL AUROC 0\%, the comparison between BXT GMO NRLMF and the second best model, bxt gmosa nrlmf, was also not statistically significant. In the remaining 10 cases where BXT GMO NRLMF prevails, it was significantly better than all other learning algorithms analysed.

Regarding LT+TL AP, in ILR=50 and ILR=70 bxt gso nrlmf and bxt gmosa nrlmf significantly outperform the other models. In ILR=0, bxt gmosa nrlmf was the best model and bxt gso nrlmf was the second best, but neither could be statistically distinguished from the third best model, kron rls. In ILR=90, bxt gso nrlmf was the best model and bxt gmosa nrlmf was the second best, but neither could be statistically distinguished from the third best model, kron rls. Under ILR=90, bxt gso nrlmf and bxt gmo nrlmf are shown to significantly outperform the others.

We notice that the highest ranked bipartite forests in each evaluation setting are always able to significantly surpass NRLMF alone. This demonstrates that the forests are able to capture different patterns than the NRLMF model, even though they are trained on NRLMF's outputs. If this result were to not hold, one could argue that the forests might be merely approximating the predictive function learned by NRLMF, rather than expanding on the information extracted in the matrix factorization step.

We also note that the competitiveness of the kron rls model for TT AP 0\% and LT+TL AP 0\% is remarkable, since kron rls has notably low training times in comparison to the other models.  %TODO figure
However, the algorithm seems to not perform as well when a larger number of positive annotations is missing. This can be seen when comparing those same test metrics under higher ILR: TT AP 70, TT AP 90, LT+TL AP 70 LT+TL AP 90.
Furthermore, Kron-RLS seems to be prioritized by AP in comparison to AUROC. This is especially suggested when observing the four TT AUROC, in which Kron-RLS performs poorly in comparison to the other cases.
According to the discussion in \autoref{sec:comparing auroc aupr}, this could mean that Kron-RLS is mostly effective when considering a restricted number of top predictions.
%This would suggest that only a small number of interactions in the dataset can be modeled by a linear relationship, and that 

Conversely, the performance of BXT GMO NRLMF for the LT+TL AP setting seems to improve relative to the other models as the number of missing annotations increases. A possible explanation comes from BXT GMO having higher generalization capabilities, as demonstrated in the TT AUROC setting. Notice that the LT+TL setting could possibly still benefit from some form of overfitting, since it considers instances that are present in the training set (\autoref{sec:cross_validation}). However, being able to generalize becomes more important as the number of missing annotations increases, since the model has to rely on less information to make predictions. And that could be why we see the BXT GMO NRLMF model being more valued in the LT+TL AP setting for higher ILR.
Additionally, we argue that the AP metric is less sensitive than AUROC to this ability of discovering missing positives \autoref{sec:comparing auroc aupr}, which explains why AUROC does not show the same pattern as AP in the LT+TL setting.

% one could use c estimation to choose the best model based on the amout of missing annotations

% linear models underfitted. previous good results in TL an LT are likely partial data leakage due to network feature extraction

% even in TT, data leakage could occur if cv is performed by masking positives

% partial conclusions:
% no need for matrix factorization step
% calibration is needed
% gso is competing in the top, even if theorically faster
% extratrees are better than rf for these datasets


\begin{figure*}[tb]
    \centering
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            no_drop/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            drop50/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            drop70/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            drop90/all_datasets/boxplots/LT+TL_roc_auc.pdf
        }
    \end{subfigure}

    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            no_drop/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            drop50/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            drop70/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            drop90/all_datasets/boxplots/LT+TL_average_precision.pdf
        }
    \end{subfigure}

    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            no_drop/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            drop50/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            drop70/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            drop90/all_datasets/boxplots/TT_roc_auc.pdf
        }
    \end{subfigure}

    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            no_drop/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            drop50/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            drop70/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{
            experiments/literature_models/statistical_comparisons/%
            drop90/all_datasets/boxplots/TT_average_precision.pdf
        }
    \end{subfigure}
    \caption{
        Percentile rankings of prediction scores of extremely randomized tree ensembles under different adaptation strategies to bipartite interaction data, across all 10 datasets.
    }
    \label{fig:comparison literature}
\end{figure*}


%\subsection{Drug-Target affinity prediction}
%
%In this section, we evaluate bipartite forests performance in a bipartite regression dataset, comparing them to state of the art deep neural networks.
%
%\begin{itemize}
%    \item deep\_dta\_raw: Uses convolutional layers to encode raw amino acid sequences and SMILES strings of drug molecules. DeepDTA \cite{Ozturk_2018}
%    \item transformer\_raw: DNN employing transformer modules to embedd the raw amino acid sequence of target proteins and SMILES string of drug molecules. Parameters were based on MolTrans \cite{}
%\end{itemize}
%
%The bxt\_gmosa model \cite{Pliakos_2020} significantly outperforms both neural networks and brf\_gso in all scenarios. bxt\_gso also outperforms the neural networks and brf\_gso in the TT setting, and score significantly higher than brf\_gso and transformer\_raw in the remaining configurations. Most impressively, the forest models take considerably less time to train than the neural networks given the experimental setup, as shown by Figure \ref{fig:davis_mse}, with the bxt\_gso still displaying highly competitive performance despite providing sensible gains in time complexity in comparison to the GMO forests and a naive GSO implementation (see sections \ref{sec:complexity_analysis} and \ref{sec:empirical_complexity}).
%
%This result points bipartite ExtraTree ensembles as state-of-the-art regression models in drug-target affinity prediction tasks, with highly competitive improvements in training efficiency.
%
%% TODO: comment on GPU possible improvements
%
%%\begin{figure*}
%%    \centering
%%    \begin{subfigure}{0.24\textwidth}
%%        \includegraphics[width=\textwidth]{
%%            experiments/neural_nets/statistical_comparisons/%
%%            davis__boxplot__LT_neg_mean_squared_error.pdf
%%        }
%%        \caption{Negative mean squared error on the LT test set.}
%%    \end{subfigure}
%%    \begin{subfigure}{0.24\textwidth}
%%        \includegraphics[width=\textwidth]{
%%            experiments/neural_nets/statistical_comparisons/%
%%            davis__boxplot__TL_neg_mean_squared_error.pdf
%%        }
%%        \caption{Negative mean squared error on the TL test set.}
%%    \end{subfigure}
%%    \begin{subfigure}{0.24\textwidth}
%%        \includegraphics[width=\textwidth]{
%%            experiments/neural_nets/statistical_comparisons/%
%%            davis__boxplot__TT_neg_mean_squared_error.pdf
%%        }
%%        \caption{Negative mean squared error on the TT test set.}
%%    \end{subfigure}
%%    \begin{subfigure}{0.24\textwidth}
%%        \includegraphics[width=\textwidth]{
%%            experiments/neural_nets/statistical_comparisons/%
%%            davis__boxplot__fit_time.pdf
%%        }
%%        \caption{Training duration in seconds.}
%%    \end{subfigure}
%%
%%    \begin{subfigure}{0.24\textwidth}
%%        \includegraphics[width=\textwidth]{
%%            experiments/neural_nets/statistical_comparisons/%
%%            davis__boxplot__LT_r2.pdf
%%        }
%%        \caption{$R^2$ score on the LT test set.}
%%    \end{subfigure}
%%    \begin{subfigure}{0.24\textwidth}
%%        \includegraphics[width=\textwidth]{
%%            experiments/neural_nets/statistical_comparisons/%
%%            davis__boxplot__TL_r2.pdf
%%        }
%%        \caption{$R^2$ score on the TL test set.}
%%    \end{subfigure}
%%    \begin{subfigure}{0.24\textwidth}
%%        \includegraphics[width=\textwidth]{
%%            experiments/neural_nets/statistical_comparisons/%
%%            davis__boxplot__TT_r2.pdf
%%        }
%%        \caption{$R^2$ score on the TT test set.}
%%    \end{subfigure}
%%    \begin{subfigure}{0.24\textwidth}
%%        \includegraphics[width=\textwidth]{
%%            experiments/neural_nets/statistical_comparisons/%
%%            davis__boxplot__score_time.pdf
%%        }
%%        \caption{Prediction duration in seconds.}
%%    \end{subfigure}
%%
%%    \caption{Model performance on the DAVIS dataset.}
%%    \label{fig:davis_mse}
%%\end{figure*}


%TODO
% \subsection{Next steps}
% traditional to bipartite approximation
% 
% n-partite data