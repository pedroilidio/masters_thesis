%% USPSC-Cap3-Conclusao.tex
% ---
% Conclus√£o
% ---
\chapter{Conclusion}

In this chapter, we summarize the main findings of this work and discuss future research directions.

\section{Main findings}

We now list our final answers to the research questions proposed in \autoref{sec:objectives}.

\begin{enumerate}
    \item \textbf{Can bipartite trees be faster?}

    Yes, we developed significantly faster-growing bipartite trees by exploiting the bipartite nature of the problem.

    We demonstrate both theoretically (\autoref{sec:complexity_analysis}) and empirically (\autoref{sec:empirical_complexity}) that our proposed bipartite tree algorithm achieves a $\log n$ improvement in training time complexity relative to its predecessors, with comparable predictive performance (\autoref{sec:comparison literature}).
    %maintaining comparable predictive performance.
    %, $n$ being the number of samples in each domain.
    %\item \textbf{How do different bipartite adaptations compare}

    \item \textbf{Are semi-supervised techniques beneficial for bipartite forests?}

    % Yes, we show that some semi-supervised impurities improve the predictive performance of a bipartite global single output forest.
    Yes. Overall, we show that both semi-supervised impurities and label imputation by matrix factorization improve the predictions of bipartite forests.% in the majority of cases.

    For most scenarios, using our proposed semi-supervised impurity improves the predictive performance relative to the corresponding supervised model (\autoref{sec:best_forests}).
    %However, it is not clear if this improvement arises the diversity it introduces to %the ensemble. a general and further investigation is needed.
    % However, the reason for this improvement could arise mainly from the diversity it introduces to the ensemble. a general and further investigation is needed.
    However, further investigation is needed to evaluate the benefits of other impurity functions.
    Additional experiments would also be interesting to investigate the effect of the methods that define the supervision balance parameter.
    %as well as to understand the effect of the strategies for defining the supervision balance parameter.  % TODO
    %to elucidate the reasons for this improvement, as it could arise mainly from the diversity it introduces to the ensemble.
    
    In any case, the most significant improvements were achieved by reconstructing the interaction matrix.
    As proposed by \cite{pliakos2020drugtarget}, we employ neighborhood-regularized matrix factorization~\cite{liu2016neighborhood} (NRLMF) to impute positive annotations prior to training the forests, and show that this approach consistently improves the scores in almost all settings (\autoref{sec:y_reconstruction} and \autoref{sec:best_forests}).
    %we employ neighborhood-regularized matrix factorization~\cite{liu2016neighborhood},As proposed by \cite{pliakos2020drugtarget},
    % to impute positive annotations prior to training the forests, and show that this approach consistently improves the scores in almost all settings (\autoref{sec:y_reconstruction} and \autoref{sec:best_forests}).

    \item \textbf{How do AUROC and AUPR differ in their assessment of model performance?}

    AUPR prioritizes a smaller number of highest-ranked interactions, while AUROC considers a larger number of both highest and lowest ranks.

    We show that AUPR is very sensitive to which interactions from the test set the estimator selects as the most likely to occur. Also, AUPR mostly disregards the interactions selected as unlikely by the estimator. On the other hand, AUROC equally considers both the highest and lowest ranked interactions, and is less influenced by the extremal ranks in comparison to AUPR (\autoref{sec:comparing auroc aupr}).

    %In accordance, our analyses suggest that AUROC is more consistent across different fractions of missing positive annotations (\autoref{sec:literature_algorithms}). 
    %Therefore, AUROC should be favored in most cases for model comparison under %positive-unlabeled settings. The AUPR can be 
    Therefore, we argue that AUROC should be generally favored for model comparison under positive-unlabeled settings. One should favor AUPR instead when the main goal is to select a small number of most likely interactions.

    %\item \textbf{Are bipartite forests competitive with proficient models in the field?}
    \item \textbf{How do bipartite forests compare with proficient models in the field?}
    %\item \textbf{How do models from the literature compare under a consistent evaluation framework?}

    %Yes, bipartite forests showed the best results in our experimental comparisons.
    The bipartite forests showed the best results in our experimental comparisons.

    Notably, the BXT GMO NRLMF model stood out in most test settings analysed, especially when predicting interactions between unknown instances (\autoref{sec:comparison literature}). This model employs NRLMF label imputation and our proposed prototype function, demonstrating the effectiveness of these techniques in improving generalization.

    %best overall scores were achieved by the
    %global multi-output


    %\item \textbf{How the missing positive annotations impact the performance of bipartite models?}

    %\item \textbf{Are application-specific deep learning models always superior to similarity based methods?}
    %No. We show that bipartite forests consistently outperform deep learning models in a case study.

    %We compared the models performance on the DAVIS dataset, representing a regressive bipartite task of drug-target affinity prediction. 
    %Bipartite forests achieve significantly better scores than the state-of-the-art models DeepDTA~\cite{ozturk2018deepdta} and MolTrans~\cite{huang2020moltrans}. Notwithstanding, training times were significantly lower for the forests (\autoref{sec:case_study}).
\end{enumerate}

\section{Main contributions}

ESCREVER %TODO

% 
% \begin{enumerate}
%     \item \textbf{Faster bipartite trees}
%     We propose the bipartite global single output approach for training bipartite trees.
% 
%     \item \textbf{Semi-supervised bipartite forests}
%     \item \textbf{New semi-supervised impurity}
%     for kernels
%     \item \textbf{New prototype function}
%     \item \textbf{Forests with new prototypes}
%     \item \textbf{Two new datasets formatted to bipartite learning}
%     \item \textbf{AUPR and AUROC comparison}
%     \item \textbf{Model comparison under a consistent evaluation framework}
%     \item \textbf{Accessible implementation}
% \end{enumerate}


\section{Future work}

The present study opened several paths for future research. We list some of the most promising ones below.

\begin{enumerate}
    \item \textbf{Adding noise to the impurity}

    It is possible that using completely random values for the unsupervised impurity could still result in improved performance for the semi-supervised forests. This is because the diversity introduced by the noisy impurity to the individual trees could be enough, in theory, to improve the performance of the ensemble.

    Hence, we propose adding noise to the supervised impurity and compare the results with supervised and semisupervised trees. The experiment could bring insights into the importance of different impurities in comparison with adding diversity to the trees.


    \item \textbf{There is a lot of unsupervised impurities to be explored}

    The present work explored two options of unsupervised impurity functions, that are utilized for training semi-supervised decision trees. However, unsupervised decision trees are a vast topic to explore. Many other approaches could be brought to the bipartite setting, with potential benefits for the predictive performance and scalability of the forests.
    For instance, several works use a single feature column to determine an unsupervised impurity~\cite{}. Their reasoning is that the most significant change in impurity will result from the feature being used to select the split points. This feature is previously ordered by the algorithm. Therefore, other feature columns can be disregarded.

    \item \textbf{Semi-supervised impurities for all forest models}

    Only the bipartite global single-output ExtraTrees were trained with semi-supervised impurities. However, other models could benefit from the same approach, maybe even to a greater extent. Future investigations could explore the use of semi-supervised impurities with Random Forests and global multi-output forests. Another promising strategy would be to combine such impurities with label imputation by matrix factorization, since imputation by itself proved to be effective.

    \item \textbf{Other label imputation approaches}

    As previous studies, we used neighborhood-regularized matrix factorization (NRLMF) to impute positive annotations before training bipartite forests. The results were promising, but many other methods for label imputation could be explored. 

    The neighborhood regularization terms are not strictly necessary if the labels are used to train a second model. Their main importance is in inferring labels for new instances, which could be done solely be the downstream model in this case.
    Therefore, it is possible that simpler methods achieve similar results in less time (logistic matrix factorization~\cite{johnsonlogistic}, for example).

    Additionally, there have been further developments of NRLMF that could be explored. NRLMF-$\beta$~\cite{ban2019nrlmf} adds beta-distribution-rescoring to the outputs of NRLMF. Dual network integrated logistic matrix factorization~\cite{zhang2019dnimf,li2019dnilmflda} uses a concept similar to NRLMF, but merging the similarity matrices with the interaction matrix before factorization.

    Other than matrix factorization, potentially any estimator could be utilized, even the same forest algorithms used for the final inference. This would be a form of self-learning, where the model is trained with the labels it has inferred~\cite{}. In a similar fashion, co-training could be used, where models are trained in separate folds and then used to infer labels for the other models~\cite{}.

    %Cascade forests~\cite{} take this process one step further, by training a sequence of forests where each one is trained with the labels inferred by the previous one. Recently, more sophisticated cascades employ 

    %DTHybrid

    %Self-learning, simple random walk, other MF (neighborhood regularization is not strictly needed).

    %\item \textbf{Compression of the dataset.} 2D Histograms?
    %\item \textbf{BGSO in combination with weighted-neighbors prototypes.}

    \item \textbf{Other forest algorithms}

    There is a variety of tree-based algorithms that could be adapted to the bipartite setting but were not explored in this work. Some of them are: Gradient Boosting Machines~\cite{}, Rotation Forests~\cite{}, Fuzzy Forests~\cite{}, Oblique Random Forests~\cite{} and Deep forests~\cite{}.

    \item \textbf{Other fields}

    The present study concentrated on scenarios in which both instance domains have respective feature matrices. However, bipartite forests could also be extended to contexts where only one of the domains has side-features (dyadic prediction, multi-label learning, weak-label learning) or even to scenarios where both domains are featureless (collaborative filtering, recommendation systems).
    %
    In these cases, features can be extracted from the interaction matrix itself, for instance, as gaussian interaction profiles~\cite{vanlaarhoven2011gaussian}.

    %A bipartite forest would then be an efficient way to tackle the problem.
    For multi-output scenarios, bipartite forests 
    %For example, in multi-label learning, bipartite trees
    can learn to partition the label matrix both horizontally and vertically, learning patterns that are specific for certain groups of outputs. This property is explored by \cite{zaminth} for single trees and hierarchical multi-label learning.

    Weak-label learning could also be approached in a similar way. The difference from traditional multi-label learning is that negative annotations are not validated, similarly to the PU learning~\cite{bekker2020learning} setting of the present work. As such, the semi-supervised techniques we developed could be especially advantageous for weak-label learning.

    %Multi-label and weak-label learning, recommendation systems (talk about vertical feature extraction).
    % \item \textbf{Specific applications.} miRNA and lncRNA, etc
    \item \textbf{Explainable artificial intelligence}

    Decision Forests have remarkable interpretability properties that were not explored in the present work. Future studies could exploit explanation techniques~\cite{mdi, mdi+, treeshap} to provide new insights into the nature of each learning task.
    For instance, in mircroRNA-gene interaction prediction, it would be interesting to pinpoint specific positions or sequence motifs that are relevant for the interaction.

    % TODO Kernel trees: feature matrix is restricted to node features (X_\text{node} is square).
    % TODO trees that modify Y as it grows
\end{enumerate}


COMPLEMENTAR  %TODO
